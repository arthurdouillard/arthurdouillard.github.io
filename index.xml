<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arthur Douillard on Arthur Douillard</title>
    <link>/</link>
    <description>Recent content in Arthur Douillard on Arthur Douillard</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Continuum</title>
      <link>/project/continuum/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0200</pubDate>
      
      <guid>/project/continuum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inclearn</title>
      <link>/project/inclearn/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0200</pubDate>
      
      <guid>/project/inclearn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Small-Task Incremental Learning</title>
      <link>/publication/podnet/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0100</pubDate>
      
      <guid>/publication/podnet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.13513&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/2004.13513&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Deep Neural Networks incrementally forever</title>
      <link>/post/incremental-learning/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0100</pubDate>
      
      <guid>/post/incremental-learning/</guid>
      <description>

&lt;p&gt;The hallmark of human intelligence is the capacity to learn. A toddler has comparable
aptitudes to reason about space, quantities, or causality than other ape species (&lt;a href=&#34;https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;). The difference of our cousins and us is the ability to learn from others.&lt;/p&gt;

&lt;p&gt;The recent deep learning hype aims to reach the Artificial General Intelligence (AGI):
an AI that would express (supra-)human-like intelligence. Unfortunately current deep learning models are flawed in many ways: one of them is that they are unable to learn
continuously as human does through years of schooling, and so on.&lt;/p&gt;

&lt;h2 id=&#34;why-do-we-want-our-models-to-learn-continuously&#34;&gt;Why do we want our models to learn continuously?&lt;/h2&gt;

&lt;p&gt;Regardless of the far away goal of AGI, there are several practicals reasons why
we want our model to learn continuously. Before describing a few of them, I&amp;rsquo;ll mention
two constraints:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Our model cannot review all previous knowledge each time it needs to
learn new facts. &lt;em&gt;As a child in 9th grade, you don&amp;rsquo;t review all the syllabus of 8th
grade as it&amp;rsquo;s supposed to have been already memorized.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Our model needs to learn continuously without forgetting any previously learned knowledge.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A real applications of these two constraints is robotics: a robot in the wild should
learn continuously its environment. Furthermore due to hardware limitation, it
may neither store all previous data nor spend too much computational resource.&lt;/p&gt;

&lt;p&gt;Another application is what I do at &lt;a href=&#34;https://www.heuritech.com/&#34; target=&#34;_blank&#34;&gt;Heuritech&lt;/a&gt;: we
detect fashion trends. However every day across the globe a new trend may appear.
It is impracticable to review our large trends database each time we need to learn
a new one.&lt;/p&gt;

&lt;p&gt;Now that the necessity of learning continuously has been explained, let us differentiate three scenarios (&lt;a href=&#34;https://arxiv.org/abs/1705.03550&#34; target=&#34;_blank&#34;&gt;Lomonaco and Maltoni, 2017&lt;/a&gt;):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning new data of known classes (&lt;em&gt;online learning&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Learning new classes (&lt;em&gt;class-incremental learning&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;The union of the two previous scenarios&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article I will focus only on the second scenario. Note however that the
methods used are fairly similar between scenario.&lt;/p&gt;

&lt;p&gt;More practically this article will cover models that learn incrementally new classes.
The model will see only new classes&amp;rsquo; data, as we aim to remember well old classes.
After each task, the model is trained on a all seen classes using a separate test set:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/incremental_base.jpg&#34; alt=&#34;*Figure 1: Several steps of incremental learning.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 1: Several steps of incremental learning.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As seen in the image above, each step produces a new accuracy score. Following
(&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34; target=&#34;_blank&#34;&gt;Rebuffi et al, 2017&lt;/a&gt;) the final score is the
average of all previous task accuracy score. It&amp;rsquo;s called the &lt;strong&gt;average incremental accuracy&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;naive-solution-transfer-learning&#34;&gt;Naive solution: transfer learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Transfer learning&lt;/strong&gt; allows to transfer the knowledge gained on one task (e.g
&lt;em&gt;ImageNet and its 1000 classes&lt;/em&gt;) to another task (e.g &lt;em&gt;classify cats &amp;amp; dogs&lt;/em&gt;) (&lt;a href=&#34;https://arxiv.org/abs/1403.6382&#34; target=&#34;_blank&#34;&gt;Razavian et al, 2014&lt;/a&gt;). Usually the backbone
(a ConvNet in Computer Vision, like ResNet) is kept while a new classifier is
plugged in on top of it. During transfer, we train the new classifier &amp;amp;
&lt;strong&gt;fine-tune&lt;/strong&gt; the backbone.&lt;/p&gt;

&lt;p&gt;Finetuning the backbone is essential to reach good performance on the destination
task. However we don&amp;rsquo;t have access anymore to the original task data. Therefore our
model is now optimized only for the new task. While at the training end, we will have good performance on this new task, the old task will suffer a significant drop of
performance.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1364661399012942&#34; target=&#34;_blank&#34;&gt;French, 1995&lt;/a&gt;)
described this phenomenon as a &lt;strong&gt;catastrophic forgetting&lt;/strong&gt;. To solve it, we must find an optimal trade-off between
&lt;strong&gt;rigidity&lt;/strong&gt; (being good on old tasks) and &lt;strong&gt;plasticity&lt;/strong&gt; (being good on new tasks).&lt;/p&gt;

&lt;h2 id=&#34;three-broad-strategies&#34;&gt;Three broad strategies&lt;/h2&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1802.07569&#34; target=&#34;_blank&#34;&gt;Parisi et al, 2018&lt;/a&gt;) defines 3 broad strategies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;External Memory&lt;/strong&gt; storing a small amount of previous tasks data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;-based methods avoiding forgetting on previous tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Plasticity&lt;/strong&gt; extending the capacity&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;1-external-memory&#34;&gt;1. External Memory&lt;/h4&gt;

&lt;p&gt;As said previously we cannot keep all our previous data for several reasons. We
can however relax this constraint by limiting access to previous data to a bounded
amount.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rehearsal learning&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34; target=&#34;_blank&#34;&gt;Rebuffi et al, 2017&lt;/a&gt;)&amp;rsquo;s iCaRL
assumes we dispose of a limited amount of space to store previous data. Our
external memory could have a capacity of 2,000 images. After learning new
classes, a few amount of those classes data could be kept in it while
the rest would be discarded.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/incremental_memory.jpg&#34; alt=&#34;*Figure 2: Several steps of incremental learning with a memory storing a subset of previous data.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 2: Several steps of incremental learning with a memory storing a subset of previous data.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Pseudo-Rehearsal learning&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/1705.08690&#34; target=&#34;_blank&#34;&gt;Shin et al, 2017&lt;/a&gt;; &lt;a href=&#34;https://arxiv.org/abs/1711.10563&#34; target=&#34;_blank&#34;&gt;Kemker and Kanan, 2018&lt;/a&gt;)
assume instead that we cannot keep previous data, like images, but that we can
store the class distribution statistics. With this, a generative model can generate
on-the-fly old classes data. This approach is however very reliant on the quality
of the generative model; generated data are still subpar
to real data (&lt;a href=&#34;https://arxiv.org/abs/1905.10887&#34; target=&#34;_blank&#34;&gt;Ravuri and Vinyals, 2019&lt;/a&gt;).
Furthermore it is still crucial to also avoid a forgetting in the generator.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/incremental_gan.jpg&#34; alt=&#34;*Figure 3: Several steps of incremental learning with a generator generating previous data.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 3: Several steps of incremental learning with a generator generating previous data.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Generally (pseudo-)rehearsal-based methods outperforms methods only using new classes
data. It&amp;rsquo;s then fair to compare their performance separately.&lt;/p&gt;

&lt;h3 id=&#34;2-constraints-based-methods&#34;&gt;2. Constraints-based methods&lt;/h3&gt;

&lt;p&gt;Intuitively, forcing the current model $M^t$ to be similar to its previous version $M^{t-1}$
will avoid forgetting. There is a large array of methods aiming to do so. However
they all have to balance a &lt;strong&gt;rigidity&lt;/strong&gt; (encouraging similarity between
$M^t$ and $M^{t-1}$) and &lt;strong&gt;plasticity&lt;/strong&gt; (letting enough slack to $M^t$ to learn
new classes).&lt;/p&gt;

&lt;p&gt;We can separate those methods in three broads categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Those enforcing a similarity of the activations&lt;/li&gt;
&lt;li&gt;Those enforcing a similarity of the weights&lt;/li&gt;
&lt;li&gt;And those enforcing a similarity of the gradients&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-1-constraining-the-activations&#34;&gt;2.1. Constraining the activations&lt;/h4&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1606.09282&#34; target=&#34;_blank&#34;&gt;Li and Hoiem, 2016&lt;/a&gt;)&amp;rsquo;s LwF introduced knowledge
distillation from (&lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34; target=&#34;_blank&#34;&gt;Hinton et al, 2015&lt;/a&gt;): given
a same image, $f^t$&amp;rsquo;s base probabilities should be similar to $f^{t-1}$&amp;rsquo;s probabilities:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/knowledge_distillation.jpg&#34; alt=&#34;*Figure 4: Base probabilities are distilled from the previous model to the new one.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 4: Base probabilities are distilled from the previous model to the new one.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The distillation loss can simply be a binary cross-entropy between old and new
probabilities.&lt;/p&gt;

&lt;p&gt;Model output probabilities is just one kind of activation among others.
(&lt;a href=&#34;http://dahua.me/publications/dhl19_increclass.pdf&#34; target=&#34;_blank&#34;&gt;Hou et al, 2019&lt;/a&gt;)&amp;rsquo;s UCIR used a
similarity-based between the extracted features $h^{t-1}$ and $h^t$ of the old
and new model:&lt;/p&gt;

&lt;p&gt;$$L_\text{Less-Forget} = 1-\langle \frac{h^t}{\Vert h^t \Vert_2}, \frac{h^{t-1}}{\Vert h^{t-1} \Vert_2}\rangle$$&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/less_forget.jpg&#34; alt=&#34;*Figure 5: New model embeddings must be similar from the old one.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 5: New model embeddings must be similar from the old one.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To sum up, encouraging the new model to mimic the activations of its previous
version reduces the forgetting of old classes. A different but similar approach
is reduce the difference between the new and old model weights:&lt;/p&gt;

&lt;h4 id=&#34;2-2-constraining-the-weights&#34;&gt;2.2. Constraining the weights&lt;/h4&gt;

&lt;p&gt;A naive method would be to minimize a distance between the new and old weights
likewise $L = (\mathbf{W}^t - \mathbf{W}^{t-1})^2$. However, as remarked by
(&lt;a href=&#34;https://arxiv.org/abs/1612.00796&#34; target=&#34;_blank&#34;&gt;Kirkpatrick et al, 2016&lt;/a&gt;)&amp;rsquo;s EWC, the resulting new
weights would be under-performing for both old and new classes. Then, the authors
suggested to modulate the regularization according to neurons importance.&lt;/p&gt;

&lt;p&gt;Important neurons for task $T-1$ must not change in the new model. On the other
hand, unimportant neurons can be more freely modified, to learn efficiently the new
task $T$:&lt;/p&gt;

&lt;p&gt;$$L = I (W^{t-1} - W^t)^2$$&lt;/p&gt;

&lt;p&gt;With $W^{t-1}$ and $W^{t}$ the weights of respectively the old and new model, and
$I$ a neurons importance matrix defined from $W^{t-1}$.&lt;/p&gt;

&lt;p&gt;In EWC, the neurons importance are defined with the Fisher information, but variants
exist. Following research (&lt;a href=&#34;https://arxiv.org/abs/1703.04200&#34; target=&#34;_blank&#34;&gt;Zenke et al, 2017&lt;/a&gt;;
&lt;a href=&#34;https://arxiv.org/abs/1801.10112&#34; target=&#34;_blank&#34;&gt;Chaudhry et al, 2018&lt;/a&gt;) builds on the same idea
with refinement of the neurons importance definition.&lt;/p&gt;

&lt;h4 id=&#34;2-3-constraining-the-gradients&#34;&gt;2.3. Constraining the gradients&lt;/h4&gt;

&lt;p&gt;Finally a third category of constraints exist: constraining the gradients. Introduced
by (&lt;a href=&#34;https://arxiv.org/abs/1706.08840&#34; target=&#34;_blank&#34;&gt;Lopez-Paz and Ranzato, 2017&lt;/a&gt;)&amp;rsquo;s GEM, the key idea
is that the the new model&amp;rsquo;s loss should be lower or equal to the old model&amp;rsquo;s loss
on old samples stored in a memory (&lt;em&gt;cf rehearsal learning&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;$$L(f^t, M) \le L(f^{t-1}, M)$$&lt;/p&gt;

&lt;p&gt;The authors rephrase this constraint as an angle constraint on the gradients:&lt;/p&gt;

&lt;p&gt;$$\langle \frac{\partial L(f^t, M)}{\partial f^t}, \frac{\partial L(f^{t-1}, M)}{\partial f^{t-1}} \rangle \ge 0$$&lt;/p&gt;

&lt;p&gt;Put it more simply, we want the gradients of the new model to &amp;ldquo;&lt;em&gt;go in the same
direction&lt;/em&gt;&amp;rdquo; as they would have with the previous model.&lt;/p&gt;

&lt;p&gt;If this constraint is respected, it&amp;rsquo;s likely that the new model won&amp;rsquo;t forget old
classes. Otherwise the incoming gradients $g$ must be &amp;ldquo;&lt;em&gt;fixed&lt;/em&gt;&amp;ldquo;: they are reprojected
to their closest valid alternative $\tilde{g}$ by minimizing this quadratic program:&lt;/p&gt;

&lt;p&gt;$$\text{minimize}_{\tilde{g}}\, \Vert g^t - \tilde{g} \Vert_2^2$$&lt;/p&gt;

&lt;p&gt;$$\text{subject to}\, \langle g^{t-1}, \tilde{g} \rangle \ge 0$$&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/gem.jpg&#34; alt=&#34;*Figure 6: Gradients must keep going in the same direction, otherwise their direction is fixed.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 6: Gradients must keep going in the same direction, otherwise their direction is fixed.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As you may guess, solving this program for each violating gradients, before
updating the model weights is very costly in time. (&lt;a href=&#34;https://arxiv.org/abs/1812.00420&#34; target=&#34;_blank&#34;&gt;Chaudhry et al, 2018&lt;/a&gt;
; &lt;a href=&#34;https://arxiv.org/abs/1903.08671&#34; target=&#34;_blank&#34;&gt;Aljundi et al, 2019&lt;/a&gt;) improve the algorithm
speed by different manners, including sampling a representative subset of the gradients
constraints.&lt;/p&gt;

&lt;h3 id=&#34;3-plasticity&#34;&gt;3. Plasticity&lt;/h3&gt;

&lt;p&gt;Other algorithms modify the network structure to reduce &lt;em&gt;catastrophic forgetting&lt;/em&gt;.
The first strategy is to add new neurons to the current model.
(&lt;a href=&#34;https://arxiv.org/abs/1708.01547&#34; target=&#34;_blank&#34;&gt;Yoon et al, 2017&lt;/a&gt;)&amp;rsquo;s DEN first trains on the
new task. If its loss is not good enough, new neurons are added at several
layers and they will be dedicated to learn on the new task. Furthermore the authors
choose to freeze some of the already-existing neurons. Those neurons, that are
particularly important for the old tasks, must not change in order to reduce forgetting.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/den.jpg&#34; alt=&#34;*Figure 7: DEN adds new neurons for the new tasks, and selectively fine-tunes existing neurons.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 7: DEN adds new neurons for the new tasks, and selectively fine-tunes existing neurons.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;While expanding the network capacity makes sense in an incremental setting where
our model learns indefinitely, it&amp;rsquo;s worth noting that existing deep learning models
are over-parametrized. The initial capacity can be enough to learn many tasks, at
the condition that it&amp;rsquo;s used appropriately. As (&lt;a href=&#34;https://arxiv.org/abs/1803.03635&#34; target=&#34;_blank&#34;&gt;Frankle and Carbin, 2019&lt;/a&gt;)&amp;rsquo;s
Lottery Ticket Hypothesis formalized, large networks are made of very efficient sub-networks.&lt;/p&gt;

&lt;p&gt;Each sub-network can be dedicated to only one task:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/subnetwork.jpg&#34; alt=&#34;*Figure 8: Among a large single network, several subnetworks can be uncovered, each specialized for a task.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 8: Among a large single network, several subnetworks can be uncovered, each specialized for a task.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Several methods exist to uncover those sub-networks: (&lt;a href=&#34;https://arxiv.org/abs/1701.08734&#34; target=&#34;_blank&#34;&gt;Fernando et al, 2017&lt;/a&gt;)&amp;rsquo;s
PathNet uses evolutionary algorithm, (&lt;a href=&#34;https://arxiv.org/abs/1903.04476&#34; target=&#34;_blank&#34;&gt;Golkar et al, 2019&lt;/a&gt;)
sparsify the whole network with a L1 regularization, and (&lt;a href=&#34;https://arxiv.org/abs/1910.06562&#34; target=&#34;_blank&#34;&gt;Hung et al, 2019&lt;/a&gt;)&amp;rsquo;s
CPG learns binary masks activating and deactivating connections to produce sub-networks.&lt;/p&gt;

&lt;p&gt;It is worth noting that methods based on sub-networks assume to know on which task
they are evaluated on. This setting, called &lt;strong&gt;multi-heads&lt;/strong&gt; is challenging but fundamentally
easier than &lt;strong&gt;single-head&lt;/strong&gt; evaluation where models are evaluated on all tasks
in the same time.&lt;/p&gt;

&lt;h2 id=&#34;dealing-with-class-imbalance&#34;&gt;Dealing with class imbalance&lt;/h2&gt;

&lt;p&gt;We saw previously three strategy to avoid forgetting (rehearsal, constraints,
and plasticity). Those methods can be used together. Rehearsal is often used in addition
of constraints.&lt;/p&gt;

&lt;p&gt;Moreover another challenge of incremental learning is the large class imbalance
between new and old classes. For example, on some benchmarks, new classes could
be made of 500 images each, while old classes would only have 20 images each stored
in memory.&lt;/p&gt;

&lt;p&gt;This class imbalance further encourages, wrongly, the model to be over-confident
for new classes while being under-confident for old classes. Catastrophic forgetting
is furthermore exacerbated.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1807.09536&#34; target=&#34;_blank&#34;&gt;Castro et al, 2018&lt;/a&gt;) train for each
task their model under this class imbalance, but fine-tune it after with under-sampling:
old &amp;amp; new classes are sampled to have as much images.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34; target=&#34;_blank&#34;&gt;Wu et al, 2019&lt;/a&gt;) consider to use re-calibration
(&lt;a href=&#34;http://proceedings.mlr.press/v70/guo17a.html&#34; target=&#34;_blank&#34;&gt;Guo et al, 2017&lt;/a&gt;): a small linear
model is learned on validation to &amp;ldquo;&lt;em&gt;fix&lt;/em&gt;&amp;rdquo; the over-confidence on new classes. It
is only applied for new classes logits. (&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34;&gt;Belouadah and Popescu, 2019&lt;/a&gt;) proposed
concurrently a similar solution fixing the new classes logits, but using instead
class statistics.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34;&gt;Hou et al, 2019&lt;/a&gt;)
remarked that weights &amp;amp; biases of the classifier layer have larger magnitude for
new classes than older classes. To reduce this effect, they replace the usual classifier
by a cosine classifier where weights and features are L2 normalized. Moreover they
freeze the classifier weights associated to old classes.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article we saw what is incremental learning: learning model with classes
coming incrementally; what is its challenge: avoiding forgetting the previous classes to
the benefice only of new classes; and broad strategies to solve this domain.&lt;/p&gt;

&lt;p&gt;This domain is far from being solved. The upper bound is a model trained in a single
step on all data. Current solutions are considerably worse than this.&lt;/p&gt;

&lt;p&gt;On a personal note, my team and I have submitted an article for a conference on this
subject. If it&amp;rsquo;s accepted, I&amp;rsquo;ll make a blog article on it. Furthermore I have made
a library to train incremental model: &lt;a href=&#34;https://github.com/arthurdouillard/incremental_learning.pytorch&#34; target=&#34;_blank&#34;&gt;inclearn&lt;/a&gt;.
The library wasn&amp;rsquo;t updated since a few months as I&amp;rsquo;m currently cleaning my code. Be sure
to check it out later.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How To Be Confident In Your Neural Network Confidence</title>
      <link>/post/miscalibration/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0200</pubDate>
      
      <guid>/post/miscalibration/</guid>
      <description>

&lt;p&gt;Those notes are based on the research paper
&amp;ldquo;&lt;strong&gt;On Calibration of Modern Neural Networks&lt;/strong&gt;&amp;rdquo; by &lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;(Guo et al, 2017.)&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;how-to-be-confident-in-your-neural-network-confidence&#34;&gt;How To Be Confident In Your Neural Network Confidence?&lt;/h1&gt;

&lt;p&gt;Very large and deep models, as ResNet, are far more accurate than their older counterparts, as LeNet, on computer vision datasets such as CIFAR100. &lt;strong&gt;However while
they are better at classifying images, we are less confident in their own confidence!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Most neural networks for classification uses as last activation a softmax: it
produces a distribution of probabilities for each target (cat, dog, boat, etc.).
These probabilities sum to one. We may expect that if for a given image, our
model associate a score of 0.8 to the target ‘boat’, our model is confident at
80% that this is the right target.&lt;/p&gt;

&lt;p&gt;Over 100 images that were detected as boat, we can expect approximately that 80
images are indeed real boats, while the 20 remaining were false positives.&lt;/p&gt;

&lt;p&gt;It was true for shallow model as LeNet but as newer models gained in accuracy
&lt;strong&gt;their confidences became decorrelated from the “real confidence”&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This does not work anymore for deep neural networks:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration.png&#34; alt=&#34;*Figure 1: Miscalibration in modern neural network [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 1: Miscalibration in modern neural network [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As you can see, older networks as LeNet had a low accuracy (55%) but their
confidence was actually in line with the accuracy! Modern networks as ResNet have
a higher accuracy (69%) but as showed in figure 1, they are over-confident.&lt;/p&gt;

&lt;p&gt;This discrepancy between the model confidence and the actual accuracy is called
&lt;strong&gt;miscalibration&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;why-it-is-important&#34;&gt;Why It Is Important&lt;/h2&gt;

&lt;p&gt;Outside of toy datasets used in the academy, it can be useful to know how much
confident our model is.&lt;/p&gt;

&lt;p&gt;Imagine we have a model predicting frauds. We want to flag some transaction as
suspicious based on the model confidence that it is a fraud.
We could definitely compute an optimal threshold on the validation set, and then
every confidence above this threshold would be flagged as a fraud. However
this computed threshold could be 0.2 or 0.9 but would probably make much sense to a human.&lt;/p&gt;

&lt;p&gt;A model without miscalibration would help the users to interpret better the
predictions.&lt;/p&gt;

&lt;h2 id=&#34;why-it-happens&#34;&gt;Why It Happens&lt;/h2&gt;

&lt;p&gt;The authors explores empirically what are the causes of this miscalibration in
modern networks.&lt;/p&gt;

&lt;p&gt;They measure the miscalibration with the &lt;strong&gt;E&lt;/strong&gt;xpected &lt;strong&gt;C&lt;/strong&gt;alibration &lt;strong&gt;E&lt;/strong&gt;rror (ECE):
the average difference between the confidence and the accuracy. This metric should
be minimized.&lt;/p&gt;

&lt;h3 id=&#34;higher-capacity-cross-entropy&#34;&gt;Higher Capacity &amp;amp; Cross-Entropy&lt;/h3&gt;

&lt;p&gt;The most interpretable cause of the miscalibration is the increase of capacity
and the cross-entropy loss.&lt;/p&gt;

&lt;p&gt;Model capacity can be seen as a measurement of how much a model can memorize.
With an infinite capacity, the model could simply learn by heart the whole
training dataset. A trade-off has to be made between a low and high capacity.
If it is too low the model wouldn’t be able to learn essential features of your
data. If it is too high, the model will learn too much and overfit instead of
generalize. Indeed comprehension is compression: by leaving few enough capacity
the model has to pick up the most representative features (pretty much in the
same way PCA works) and will then generalize better (but too few capacity &amp;amp; no
learning will happen!).&lt;/p&gt;

&lt;p&gt;The new architectures such as ResNet have way more capacity than the older
LeNet (25M parameters for the former and 20k for the latter). This high
capacity led to better accuracy: the training set can almost be learned by heart.&lt;/p&gt;

&lt;p&gt;In addition the models optimizes the cross-entropy loss that force them to be
right AND to be very confident. The higher capacity helped to lower the
cross-entropy loss and thus encourages deep neural networks to be over-confident.
As you’ve seen on figure 1, the new models are now over-confident.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_capacity.png&#34; alt=&#34;*Figure 2: More capacity (in depth or width) increases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 2: More capacity (in depth or width) increases the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;the-mysterious-batch-normalization&#34;&gt;The Mysterious Batch Normalization&lt;/h3&gt;

&lt;p&gt;Batch Normalization normalizes the tensors in a network. It greatly improves the
training convergence &amp;amp; the final performance. Why exactly it works that well
is still a bit undefined (&lt;a href=&#34;/posts/normalization&#34;&gt;see more&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The authors remark empirically that using Batch Normalization increased the miscalibration
but could not find an exact reason why.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_bn.png&#34; alt=&#34;*Figure 3: Batch Normalization increases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 3: Batch Normalization increases the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Could the help given by this method in training facilitate the over-confidence?&lt;/p&gt;

&lt;h3 id=&#34;regularization&#34;&gt;Regularization&lt;/h3&gt;

&lt;p&gt;The weight decay is an additional loss that penalizes the L2 norm of the weights.
The larger the weights, the bigger the norm and thus the loss. By constraining the weights
magnitude, it avoid the model finding extreme weight values that could make it overfit.&lt;/p&gt;

&lt;p&gt;The authors found that increasing the regularization decreases the model accuracy
as expected. However it also decreased the miscalibration! The answer is then again
because regularization avoid overfitting &amp;amp; thus over-confidence.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_reg.png&#34; alt=&#34;*Figure 4: More regularization decreases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 4: More regularization decreases the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;how-to-fix-miscalibration&#34;&gt;How To Fix Miscalibration&lt;/h2&gt;

&lt;p&gt;This article&amp;rsquo;s title, &amp;ldquo;&lt;em&gt;How To Be Confident In Your Neural Network Confidence&lt;/em&gt;&amp;rdquo;,
led you to believe that you would discover how to reduce miscalibration.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;re not going to reduce the capacity, remove Batch Normalization, and increase
the regularization: you&amp;rsquo;ll hurt too much your precious accuracy.&lt;/p&gt;

&lt;p&gt;Fortunately there are post-processing solutions. The authors describe several
but the most effective one is also the simplest: &lt;strong&gt;Temperature Scaling&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Instead of computing the softmax like this:&lt;/p&gt;

&lt;p&gt;$$\text{softmax}(x)_i = \frac{e^{y_i}}{\Sigma_j^N e^{y_j}}$$&lt;/p&gt;

&lt;p&gt;All the logits (values just before the final activation, here softmax) are divided
by the same value called temperature:&lt;/p&gt;

&lt;p&gt;$$\text{softmax}(x)_i = \frac{e^{\frac{y_i}{T}}}{\Sigma_j^N e^{\frac{y_j}{T}}}$$&lt;/p&gt;

&lt;p&gt;Similar to (&lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34; target=&#34;_blank&#34;&gt;Hinton et al, 2015.&lt;/a&gt;), this temperature
&lt;em&gt;softens the probabilities&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Extreme probabilities (high confidence) are more decreased than smaller probabilities
(low confidence). The authors find the optimal temperature by minimizing the
Expected Calibration Error on the validation set.&lt;/p&gt;

&lt;p&gt;The miscalibration is almost entirely corrected:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_tempscaling.png&#34; alt=&#34;*Figure 5: Temperature Scaling fixes the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 5: Temperature Scaling fixes the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Another cool feature of Temperature Scaling: because all logits are divided by the
same value, and that softmax is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Monotonic_function&#34; target=&#34;_blank&#34;&gt;monotone function&lt;/a&gt;,
the accuracy remains unchanged!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Normalization in Deep Learning</title>
      <link>/post/normalization/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/normalization/</guid>
      <description>

&lt;p&gt;Deep Neural Networks (DNNs) are notorious for requiring less feature engineering than
Machine Learning algorithms. For example convolutional networks learn by themselves
the right convolution kernels to apply on an image. No need of carefully
handcrafted kernels.&lt;/p&gt;

&lt;p&gt;However a common point to all kinds of neural networks is the &lt;strong&gt;need of normalization&lt;/strong&gt;.
Normalizing is often done on the input, but it can also take place inside the
network. In this article I&amp;rsquo;ll try to describe what the literature is saying about
this.&lt;/p&gt;

&lt;p&gt;This article is not exhaustive but it tries to cover the major algorithms. If
you feel I missed something important, tell me!&lt;/p&gt;

&lt;h3 id=&#34;normalizing-the-input&#34;&gt;Normalizing the input&lt;/h3&gt;

&lt;p&gt;It is &lt;em&gt;extremely&lt;/em&gt; common to normalize the input
&lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf&#34; target=&#34;_blank&#34;&gt;(lecun-98b)&lt;/a&gt;, especially
for computer vision tasks. Three normalization schemes are often seen:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Normalizing the pixel values between 0 and 1:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Normalizing the pixel values between -1 and 1 (as &lt;a href=&#34;https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L47-L50&#34; target=&#34;_blank&#34;&gt;Tensorflow does&lt;/a&gt;):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;127.5&lt;/span&gt;
img &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Normalizing according to the dataset mean &amp;amp; standard deviation (as &lt;a href=&#34;https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L52-L55&#34; target=&#34;_blank&#34;&gt;Torch does&lt;/a&gt;):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255.&lt;/span&gt;
mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.485&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.456&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.406&lt;/span&gt;] &lt;span style=&#34;color:#75715e&#34;&gt;# Here it&amp;#39;s ImageNet statistics&lt;/span&gt;
std &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.229&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.225&lt;/span&gt;]

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;): &lt;span style=&#34;color:#75715e&#34;&gt;# Considering an ordering NCHW (batch, channel, height, width)&lt;/span&gt;
    img[i, :, :] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; mean[i]
    img[i, :, :] &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; std[i]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Why is it recommended? Let&amp;rsquo;s take a neuron, where:&lt;/p&gt;

&lt;p&gt;$$y = w \cdot x$$&lt;/p&gt;

&lt;p&gt;The partial derivative of $y$ for $w$ that we use during backpropagation is:&lt;/p&gt;

&lt;p&gt;$$\frac{\partial y}{\partial w} = X^T$$&lt;/p&gt;

&lt;p&gt;The scale of the data has an effect on the magnitude of the gradient for
the weights. If the gradient is big, you should reduce the learning rate.
However you usually have different gradient magnitudes in a same batch. Normalizing
the image to smaller pixel values is a cheap price to pay while making easier to
tune an optimal learning rate for input images.&lt;/p&gt;

&lt;h3 id=&#34;1-batch-normalization&#34;&gt;1. Batch Normalization&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve seen previously how to normalize the input, now let&amp;rsquo;s see a normalization
inside the network.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Ioffe &amp;amp; Szegedy, 2015&lt;/a&gt;) declared that DNN
training was suffering from the &lt;em&gt;internal covariate shift&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The authors describe it as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;[&amp;hellip;] the distribution of each layer’s inputs changes during training, as the
parameters of the previous layers change.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Their answer to this problem was to apply to the pre-activation a Batch
Normalization (BN):&lt;/p&gt;

&lt;p&gt;$$BN(x) = \gamma \frac{x - \mu_B}{\sigma_B} + \beta$$&lt;/p&gt;

&lt;p&gt;$\mu_B$ and $\sigma_B$ are the mean and the standard deviation of the batch.
$\gamma$ and $\beta$ are learned parameters.&lt;/p&gt;

&lt;p&gt;The batch statistics are computed for a whole channel:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/batch_norm.png&#34; alt=&#34;*Statistics are computed for a whole batch, channel per channel.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Statistics are computed for a whole batch, channel per channel.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;$\gamma$ and $\beta$ are essential because they enable the BN to represent
the identity transform if needed. If it couldn&amp;rsquo;t, the resulting BN&amp;rsquo;s transformation
(with a mean of 0 and a variance of 1) fed to a sigmoid non-linearity would
be constrained to its linear regime.&lt;/p&gt;

&lt;p&gt;While during training the mean and standard deviation are computed on the batch,
during test time BN uses the whole dataset statistics using a moving average/std.&lt;/p&gt;

&lt;p&gt;Batch Normalization has showed a considerable training acceleration to existing
architectures and is now an almost de facto layer. It has however for weakness
to use the batch statistics at training time: With small batches or with a dataset
non &lt;a href=&#34;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&#34; target=&#34;_blank&#34;&gt;i.i.d&lt;/a&gt;
it shows weak performance. In addition to that, the difference between training
and test time of the mean and the std can be important, this can lead to a difference of performance between the two modes.&lt;/p&gt;

&lt;h3 id=&#34;1-1-batch-renormalization&#34;&gt;1.1. Batch ReNormalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.03275&#34; target=&#34;_blank&#34;&gt;(Ioffe, 2017)&lt;/a&gt;&amp;rsquo;s Batch Renormalization (BR)
introduces an improvement over Batch Normalization.&lt;/p&gt;

&lt;p&gt;BN uses the statistics ($\mu_B$ &amp;amp; $\sigma_B$) of the batch. BR introduces
two new parameters $r$ &amp;amp; $d$ aiming to constrain the mean and std of BN,
reducing the extreme difference when the batch size is small.&lt;/p&gt;

&lt;p&gt;Ideally the normalization should be done with the instance&amp;rsquo;s statistic:&lt;/p&gt;

&lt;p&gt;$$\hat{x} = \frac{x - \mu}{\sigma}$$&lt;/p&gt;

&lt;p&gt;By choosing $r = \frac{\sigma_B}{\sigma}$ and $d = \frac{\mu_B - \mu}{\sigma}$:&lt;/p&gt;

&lt;p&gt;$$\hat{x} = \frac{x - \mu}{\sigma} = \frac{x - \mu_B}{\sigma_B} \cdot r + d$$&lt;/p&gt;

&lt;p&gt;The authors advise to constrain the maximum absolute values of $r$ and $d$.
At first to 1 and 0, behaving like BN, then to relax gradually those bounds.&lt;/p&gt;

&lt;h3 id=&#34;1-2-internal-covariate-shift&#34;&gt;1.2. Internal Covariate Shift?&lt;/h3&gt;

&lt;p&gt;Ioffe &amp;amp; Szegedy argued that the changing distribution of the pre-activation hurt
the training. While Batch Norm is widely used in SotA research, there is still
controversy (&lt;a href=&#34;https://youtu.be/Qi1Yry33TQE?t=17m4s&#34; target=&#34;_blank&#34;&gt;Ali Rahami&amp;rsquo;s Test of Time&lt;/a&gt;)
about what this algorithm is solving.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.11604&#34; target=&#34;_blank&#34;&gt;(Santurkar et al, 2018)&lt;/a&gt; refuted the Internal
Covariate Shift influence. To do so, they compared three models, one baseline,
one with BN, and one with random noise added &lt;em&gt;after&lt;/em&gt; the normalization.&lt;/p&gt;

&lt;p&gt;Because of the random noise, the activation&amp;rsquo;s input is not &lt;em&gt;normalized&lt;/em&gt; anymore
and its distribution change at every time test.&lt;/p&gt;

&lt;p&gt;As you can see on the following figure, they found that the random shift of distribution
didn&amp;rsquo;t produce extremely different results:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/cmp_icf.png&#34; alt=&#34;*Comparison between standard net, net with BN, and net with noisy BN.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Comparison between standard net, net with BN, and net with noisy BN.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;On the other hand they found that the Batch Normalization improved the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Lipschitz_continuity&#34; target=&#34;_blank&#34;&gt;Lipschitzness&lt;/a&gt; of the loss
function. In simpler term, the loss is smoother, and thus its gradient as well.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/smoothed_loss.png&#34; alt=&#34;*Figure 3: Loss with and without Batch Normalization.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 3: Loss with and without Batch Normalization.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;According to the authors:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Improved Lipschitzness of the gradients gives us confidence that when we take
a larger step in a direction of a computed gradient, this gradient direction
remains a fairly accurate estimate of the actual gradient direction after
taking that step.  It thus enables any (gradient–based) training algorithm to
take larger steps without the danger of running into a sudden change of the
loss landscape such as flat region (corresponding to vanishing gradient) or
sharp local minimum (causing exploding gradients).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The authors also found that replacing BN by a $l_1$, $l_2$, or $l_{\infty}$
lead to similar results.&lt;/p&gt;

&lt;h3 id=&#34;2-computing-the-mean-and-variance-differently&#34;&gt;2. Computing the mean and variance differently&lt;/h3&gt;

&lt;p&gt;Algorithms similar to Batch Norm have been developed where the mean &amp;amp; variance
are computed differently.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/normalization.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    
    &lt;a href=&#34;https://arxiv.org/abs/1803.08494&#34;&gt; 
    source
    &lt;/a&gt; 
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;2-1-layer-normalization&#34;&gt;2.1. Layer Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34; target=&#34;_blank&#34;&gt;(Ba et al, 2016)&lt;/a&gt;&amp;rsquo;s layer norm (LN) normalizes
each image of a batch independently using all the channels. The goal is have constant
performance with a large batch or a single image. &lt;strong&gt;It&amp;rsquo;s used in recurrent neural
networks&lt;/strong&gt; where the number of time steps can differ between tasks.&lt;/p&gt;

&lt;p&gt;While all time steps share the same weights, each should have its own statistic.
BN needs previously computed batch statistics, which would be impossible if there
are more time steps at test time than training time. LN is time steps independent
by simply computing the statistics on the incoming input.&lt;/p&gt;

&lt;h3 id=&#34;2-2-instance-normalization&#34;&gt;2.2. Instance Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.08022&#34; target=&#34;_blank&#34;&gt;(Ulyanov et al, 2016)&lt;/a&gt;&amp;rsquo;s instance norm (IN)
normalizes each channel of each batch&amp;rsquo;s image independently. &lt;strong&gt;The goal is to
normalize the constrast of the content image&lt;/strong&gt;. According to the authors, only the
style image contrast should matter.&lt;/p&gt;

&lt;h3 id=&#34;2-3-group-normalization&#34;&gt;2.3. Group Normalization&lt;/h3&gt;

&lt;p&gt;According to &lt;a href=&#34;https://arxiv.org/abs/1803.08494&#34; target=&#34;_blank&#34;&gt;(Wu and He, 2018)&lt;/a&gt;, convolution
filters tend to group in related tasks (frequency, shapes, illumination, textures).&lt;/p&gt;

&lt;p&gt;They normalize each image in a batch independently so the model is batch size
independent. Moreover they normalize the channels per group arbitrarily defined
(usually 32 channels per group). All filters of a same group should specialize
in the same task.&lt;/p&gt;

&lt;h3 id=&#34;3-normalization-on-the-network&#34;&gt;3. Normalization on the network&lt;/h3&gt;

&lt;p&gt;Previously shown methods normalized the inputs, there are methods were the normalization
happen in the network rather than on the data.&lt;/p&gt;

&lt;h3 id=&#34;3-1-weight-normalization&#34;&gt;3.1. Weight Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07868&#34; target=&#34;_blank&#34;&gt;(Salimans and Kingma, 2016)&lt;/a&gt; found that
decoupling the length of the weight vectors from their direction accelerated the
training.&lt;/p&gt;

&lt;p&gt;A fully connected layer does the following operation:&lt;/p&gt;

&lt;p&gt;$$y = \phi(W \cdot x + b)$$&lt;/p&gt;

&lt;p&gt;In weight normalization, the weight vectors is expressed the following way:&lt;/p&gt;

&lt;p&gt;$$W = \frac{g}{\Vert V \Vert}V$$&lt;/p&gt;

&lt;p&gt;$g$ and $V$ being respectively a learnable scalar and a learnable matrix.&lt;/p&gt;

&lt;h3 id=&#34;3-2-cosine-normalization&#34;&gt;3.2. Cosine Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.05870&#34; target=&#34;_blank&#34;&gt;(Luo et al, 2017)&lt;/a&gt; normalizes both the weights
and the input by replacing the classic dot product by a cosine similarity:&lt;/p&gt;

&lt;p&gt;$$y = \phi(\frac{W \cdot X}{\Vert W \Vert \Vert X \Vert})$$&lt;/p&gt;

&lt;h3 id=&#34;4-conclusion&#34;&gt;4. Conclusion&lt;/h3&gt;

&lt;p&gt;Batch normalization (BN) is still the most represented method among new
architectures despite its defect: the dependence on the batch size. Batch
renormalization (BR) fixes this problem by adding two new parameters to
approximate instance statistics instead of batch statistics.&lt;/p&gt;

&lt;p&gt;Layer norm (LN), instance norm (IN), and group norm (GN), are similar to
BN. Their difference lie in the way statistics are computed.&lt;/p&gt;

&lt;p&gt;LN was conceived for RNNs, IN for style transfer, and GN for CNNs.&lt;/p&gt;

&lt;p&gt;Finally weigh norm and cosine norm normalize the network&amp;rsquo;s weight instead of simply
the input data.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TP Deep Learning RDFIA</title>
      <link>/rdfia/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/rdfia/</guid>
      <description>

&lt;h1 id=&#34;tp-deep-learning-rdfia&#34;&gt;TP Deep Learning RDFIA&lt;/h1&gt;

&lt;p&gt;RDFIA / Master DAC &amp;amp; IMA / Sorbonne&lt;/p&gt;

&lt;p&gt;Le cours est organisé par le professeur Matthieu Cord. Vos assistants de TPs auquels
vous devrez envoyer vos travaux sont Yifu Chen (yifu.chen@lip6.fr) et moi-même Arthur
Douillard (arthur.douillard@lip6.fr).&lt;/p&gt;

&lt;p&gt;Pour simplifier notre tâche vous êtes priés de nous adresser les mails avec pour objet
&lt;code&gt;[RDFIA][TP-&amp;lt;numero&amp;gt;]&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;rappels&#34;&gt;Rappels&lt;/h2&gt;

&lt;p&gt;Les TPs seront en Python3 et plusieurs bibliothèques seront utilisées. Voici
quelques liens pour rappel, ou pour vous familiariser en avance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learnxinyminutes.com/docs/python/&#34; target=&#34;_blank&#34;&gt;Rappel de Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.scipy.org/doc/numpy/user/quickstart.html&#34; target=&#34;_blank&#34;&gt;Rappel de Numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34; target=&#34;_blank&#34;&gt;Introduction de Scikit-Learn&lt;/a&gt;. L&amp;rsquo;api est très similaire quelque soit l&amp;rsquo;algorithme (init / fit / predict)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34; target=&#34;_blank&#34;&gt;Introduction de Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Les cours seront ajoutés au fur et à mesure.&lt;/p&gt;

&lt;h2 id=&#34;tp-1-2-sift-bag-of-words&#34;&gt;TP 1 - 2 : SIFT / Bag of words&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;18 &amp;amp; 25 Septembre 2019&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp1-2.pdf&#34;&gt;TP1-2.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code et data: &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/tp1-2.zip&#34; target=&#34;_blank&#34;&gt;TP1-2.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pour ceux n&amp;rsquo;ayant pas réussi à calculer tous les descriptors SIFT du dataset fourni: voici un zip
les contenant: &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/sift.zip&#34; target=&#34;_blank&#34;&gt;sift.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Voici le genre de résultat que vous aurez pu obtenir:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/rdfia_sift_1.png&#34; alt=&#34;Image sift résultat&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/rdfia_sift_2.png&#34; alt=&#34;Image sift résultat&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;tp-3-svm&#34;&gt;TP 3 : SVM&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;2 Octobre 2019&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp3.pdf&#34;&gt;TP3.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Bow &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/15_scenes_Xy.npz&#34; target=&#34;_blank&#34;&gt;15_scenes_Xy.npz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-4-5-introduction-aux-réseaux-de-neurones&#34;&gt;TP 4-5 : Introduction aux réseaux de neurones&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;9 &amp;amp; 16 Octobre 2019&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp4-5.pdf&#34;&gt;TP4-5.pdf&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Code: &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/tp4-5.zip&#34; target=&#34;_blank&#34;&gt;TP4-5.zip&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Math formules: &lt;a href=&#34;/files/rdfia_resources/tp4-5_math.pdf&#34;&gt;TP4-5_formula.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-6-7-réseaux-convolutionnels-pour-l-image&#34;&gt;TP 6-7 : Réseaux convolutionnels pour l&amp;rsquo;image&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;23 &amp;amp; 30 Octobre 2019&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp6-7.pdf&#34;&gt;TP6-7.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/tp6-7.zip&#34; target=&#34;_blank&#34;&gt;TP6-7.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-8-transfer-learning-par-extraction-de-features-dans-un-cnn&#34;&gt;TP 8: Transfer Learning par extraction de features dans un CNN&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;27 Novembre 2019&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp8.pdf&#34;&gt;TP8.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/tp8.zip&#34; target=&#34;_blank&#34;&gt;TP8.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-9-visualisation-des-réseaux-de-neurones&#34;&gt;TP 9: Visualisation des réseaux de neurones&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;4 Décembre 2019&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp9.pdf&#34;&gt;TP9.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/tp9.zip&#34; target=&#34;_blank&#34;&gt;TP9.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-10-11-generative-adversarial-networks&#34;&gt;TP 10-11: Generative Adversarial Networks&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;11 Décembre 2019&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp10-11.pdf&#34;&gt;TP10-11.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Code: &lt;a href=&#34;https://webia.lip6.fr/~douillard/rdfia/tp10-11.zip&#34; target=&#34;_blank&#34;&gt;TP10-11.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;mises-à-jour&#34;&gt;Mises à jour:&lt;/h4&gt;

&lt;p&gt;2019-09-25, 14:16: Ajout d&amp;rsquo;un zip sift + deux images résultats&lt;/p&gt;

&lt;p&gt;2019-10-02, 11:15: Ajout du TP 3 + BoW data.&lt;/p&gt;

&lt;p&gt;2019-10-09, 11:07: Ajout du TP 4-5.&lt;/p&gt;

&lt;p&gt;2019-10-16, 12:52: Ajout des formules forward / backward.&lt;/p&gt;

&lt;p&gt;2019-10-23, 12:42: Ajout du TP 6-7.&lt;/p&gt;

&lt;p&gt;2019-10-26, 17:15: Modification de la date du TP 7 + report de la date de rendu.&lt;/p&gt;

&lt;p&gt;2019-11-27, 13:27: Ajout du TP 8.&lt;/p&gt;

&lt;p&gt;2019-12-04, 12:50: Ajout du TP 9, correction de typo + env var pour le TP 8.&lt;/p&gt;

&lt;p&gt;2019-12-11, 13:15: Ajout du TP 10-11.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detecting cars from aerial imagery for the NATO Innovation Challenge</title>
      <link>/post/nato-challenge/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/nato-challenge/</guid>
      <description>

&lt;p&gt;&lt;strong&gt;Imagine you’re in a landlocked country, and an infection has spread. The
government has fallen, and rebels are roaming the country. If you’re the armed
forces in this scenario, how do you make decisions in this environment? How can
you fully understand the situation at hand?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/nato.png&#34; alt=&#34;nato&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A few weeks ago, NATO organized an innovation challenge that posed these
questions. We decided to take on the challenge with
the goal of finding innovative solutions in the areas of data filtering/fusing,
visualization, and predictive analytics.&lt;/p&gt;

&lt;p&gt;For those who don’t know, NATO is an intergovernmental military alliance between
29 North American and European countries. It constitutes a system of collective
defense whereby its independent member states agree to mutual defense in response
to an attack by any external party.&lt;/p&gt;

&lt;p&gt;NATO did not provide any data for the challenge, so we had to find it ourselves.
Ultimately, the solution we came up with used a variety of different techniques
including computer vision on aerial imagery, natural language processing on
press &amp;amp; social media, geo data processing, and — of course — fancy graphs.&lt;/p&gt;

&lt;p&gt;In this post, we will focus on the most technical part: object detection for
aerial imagery, walking through what kind of data we used, which architecture
was employed, and how the solution works, and finally our results. If you’re
interested in a higher-level look at the project, that’s over
&lt;a href=&#34;https://blog.dataiku.com/data-science-and-disease-outbreak-assistance-nato-act-innovation-challenge&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This challenge was done while I was an intern at &lt;a href=&#34;https://www.dataiku.com/&#34; target=&#34;_blank&#34;&gt;Dataiku&lt;/a&gt;.
My team was composed of an commander, a salesman, and myself as the lead/sole scientist.&lt;/p&gt;

&lt;h3 id=&#34;1-the-dataset&#34;&gt;1. The dataset&lt;/h3&gt;

&lt;p&gt;For the object detection portion of the project, we used the
&lt;a href=&#34;https://gdo152.llnl.gov/cowc/&#34; target=&#34;_blank&#34;&gt;Cars Overhead With Context&lt;/a&gt; (COWC) dataset,
which is provided by the Lawrence Livermore National Laboratory. It features
aerial imagery taken in six distinct locations:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Toronto, Canada&lt;/li&gt;
&lt;li&gt;Selwyn, New Zealand&lt;/li&gt;
&lt;li&gt;Potsdam and Vaihingen*, Germany&lt;/li&gt;
&lt;li&gt;Columbus (Ohio)* and Utah, USA&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;* &lt;em&gt;We ultimately did not use the Columbus and Vaihingen data because the
imagery was in grayscale.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This dataset offers large imagery (up to 4 square kilometers) with good
resolution (15cm per pixel) with the center localization of every car. As
suggested in &lt;a href=&#34;https://medium.com/the-downlinq/car-localization-and-counting-with-overhead-imagery-an-interactive-exploration-9d5a029a596b&#34; target=&#34;_blank&#34;&gt;this Medium post&lt;/a&gt;, we assumed that cars have a mean size of
3 meters. We created boxes centered around each car center to achieve our
ultimate goal of predicting box (i.e., car) locations in unseen images.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/cowc_example1.png&#34; alt=&#34;*Figure 1: An example image from the COWC dataset*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 1: An example image from the COWC dataset&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;2-the-architecture&#34;&gt;2. The architecture&lt;/h3&gt;

&lt;p&gt;To detect cars in these large aerial images, we used the RetinaNet &lt;a href=&#34;https://arxiv.org/abs/1708.02002&#34; target=&#34;_blank&#34;&gt;(Lin et al, 2017)&lt;/a&gt; architecture. Published in 2017 by Facebook
FAIR, this paper won the Best Student Paper of ICCV 2017.&lt;/p&gt;

&lt;p&gt;Object detection architectures are split in two categories: single-stage and
two-stage.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Two-stage architectures&lt;/strong&gt; first categorize potential objects in two classes:
foreground or background. Then all foreground’s potential objects are classified
in more fine-grained classes: cats, dogs, cars, etc. This two-stage method is
slow but also, and of course, produces the best accuracy. The most famous
two-stage architecture is &lt;a href=&#34;/post/faster-rcnn&#34;&gt;Faster-RCNN&lt;/a&gt; &lt;a href=&#34;https://arxiv.org/abs/1506.01497&#34; target=&#34;_blank&#34;&gt;(Ren et al, 2015)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On the other hand, &lt;strong&gt;single-stage architectures&lt;/strong&gt; don’t have this pre-selection
step of potential foreground objects. They are usually less accurate, but
they are also faster. RetinaNet’s single-stage architecture is an exception:
it reaches two-stage performance while having single-stage speed!&lt;/p&gt;

&lt;p&gt;On the figure 2 below, you can see a comparison of object detection
architectures.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/cmp_obj_detect.png&#34; alt=&#34;*Figure 2: Performance of object detection algorithms*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 2: Performance of object detection algorithms&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;RetinaNet is made of four components. We’ll try to describe how the data is
transformed through every step.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/retinanet.png&#34; alt=&#34;*Figure 3: The RetinaNet architecture*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 3: The RetinaNet architecture&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;2-1-convolutional-network&#34;&gt;2.1. Convolutional Network&lt;/h4&gt;

&lt;p&gt;First of all there is a &lt;strong&gt;ResNet-50&lt;/strong&gt; &lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;(He et al., 2015)&lt;/a&gt;.
As every convolutional neural network (CNN),
it takes an image as input and processes it through convolution kernels.
Each kernel’s output is a feature map — the first feature maps capture high-level
features (such as a line or a color). The further we go down in the network,
the smaller the feature maps become because of the pooling layers. While they
are smaller, they also represent more fined-grained information (such as an eye,
a dog ear, etc.). The input image has three channels (red, blue, green), but
every subsequent feature map has dozens of channels! Each of them represents
a different kind of feature that it captured.&lt;/p&gt;

&lt;p&gt;A common classifier takes the ResNet’s last feature maps (of shape &lt;code&gt;(7, 7, 2048)&lt;/code&gt;),
applies an average pooling on each channel (resulting in &lt;code&gt;(1, 1, 2048)&lt;/code&gt;), and feeds
it to a fully connected layer with a softmax.&lt;/p&gt;

&lt;h4 id=&#34;2-2-feature-pyramid-network&#34;&gt;2.2. Feature Pyramid Network&lt;/h4&gt;

&lt;p&gt;Instead of adding a classifier after ResNet, RetinaNet adds a
&lt;strong&gt;Feature Pyramid Network&lt;/strong&gt; (FPN) &lt;a href=&#34;https://arxiv.org/abs/1612.03144&#34; target=&#34;_blank&#34;&gt;(Lin et al., 2016)&lt;/a&gt;.
By picking feature maps at different layers from the ResNet, it provides
rich and multi-scale features.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/fpn.png&#34; alt=&#34;*Figure 4: The lateral connection between the backbone and the FPN*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 4: The lateral connection between the backbone and the FPN&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;However, ResNet’s first feature maps may be too crude to extract any useful
information. As you can see in figure 4, the smaller and more precise feature
maps are combined with the bigger feature maps. We first upsample the smaller
ones and then sum it with the bigger ones. Several upsampling methods exist;
here, the upsampling is done with the nearest neighbor method.&lt;/p&gt;

&lt;p&gt;Each level of the FPN encodes a different kind of information at a different
scale. Thus, each of them should participate in the object detection task.
The FPN takes as input the output of the third (512 channels), fourth
(1024 channels), and fifth (2048 channels) blocks of ResNet. The third is half
the size of the fourth, and the fourth is half of the fifth.&lt;/p&gt;

&lt;p&gt;We apply &lt;a href=&#34;/post/3-small-but-powerful-cnn&#34;&gt;pointwise convolution&lt;/a&gt;
(convolution with a 1x1 kernel) to uniformize the number of channels of each
level to 256. Then we upsampled the smaller levels by a factor of two to match
the dimension of the bigger levels.&lt;/p&gt;

&lt;h4 id=&#34;2-3-anchors&#34;&gt;2.3. Anchors&lt;/h4&gt;

&lt;p&gt;At each FPN level, &lt;strong&gt;anchors&lt;/strong&gt; are moved around the FPN’s feature maps.
An anchor is a rectangle with different sizes and ratios, like this:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/anchors.svg&#34; alt=&#34;*Figure 5: A sample of anchors of different sizes and ratios*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 5: A sample of anchors of different sizes and ratios&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;These anchors are the base position of the potential objects. Five sizes and
three ratios exist, thus there are 15 unique anchors. These anchors are also
scaled according to the dimension of the FPN levels. These unique anchors are
duplicated on all the possible positions in the feature maps. It results in $K$
total anchors.&lt;/p&gt;

&lt;p&gt;Let’s put aside those anchors for the moment.&lt;/p&gt;

&lt;h4 id=&#34;2-4-regression-classification&#34;&gt;2.4. Regression &amp;amp; classification&lt;/h4&gt;

&lt;p&gt;Each FPN’s level is fed to two &lt;strong&gt;Fully Convolutional Networks&lt;/strong&gt; (FCN), which are
neural networks made only of convolutions and pooling. To fully exploit the fact
that every FPN’s level holds different kind of information, the two FCNs are
shared among all levels! Convolution layers are independent of the input size;
only their kernel size matter. Thus while each FPN’s feature maps have different
sizes, they can be all fed to the same FCNs.&lt;/p&gt;

&lt;p&gt;The first FCN is the &lt;strong&gt;regression branch&lt;/strong&gt;. It predicts $K x 4$
(&lt;code&gt;x1&lt;/code&gt;, &lt;code&gt;y1&lt;/code&gt;, &lt;code&gt;x2&lt;/code&gt;, &lt;code&gt;y2&lt;/code&gt; for each anchor) values. Those values are &lt;strong&gt;deltas&lt;/strong&gt; that
slightly modify the original anchors so they fit the potential objects
better. All the potential objects will now have coordinates of the type:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;(x1 + dx1, y1 + dy1, x2 + dx2, y2 + dy2)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;With &lt;code&gt;x?&lt;/code&gt; and &lt;code&gt;y?&lt;/code&gt;, the fixed coordinates of the anchors, and &lt;code&gt;dx?&lt;/code&gt;, &lt;code&gt;dy?&lt;/code&gt;,
the deltas produced by the regression branch.&lt;/p&gt;

&lt;p&gt;We now have the final coordinates for all objects — that is, all potential
objects. They are not yet classified as background or car, truck, etc.&lt;/p&gt;

&lt;p&gt;The second FCN is the &lt;strong&gt;classification branch&lt;/strong&gt;. It is a multi-label problem
where the classifier predicts $K x N$ ($N$ being the number of classes) potential
objects with sigmoid.&lt;/p&gt;

&lt;h4 id=&#34;2-5-removing-duplicates&#34;&gt;2.5. Removing duplicates&lt;/h4&gt;

&lt;p&gt;At this point we have $K x 4$ coordinates and $K x N$ class scores. We now have
a problem: it is common to detect, for the same class, several boxes for a
same object!&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/nms_before.png&#34; alt=&#34;*Figure 6: Several boxes have been detected for a single car.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 6: Several boxes have been detected for a single car.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Therefore, for each class (even if it’s not the highest scoring class)
we apply a &lt;strong&gt;Non-max suppression&lt;/strong&gt;. Tensorflow provides a &lt;a href=&#34;https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression&#34; target=&#34;_blank&#34;&gt;function&lt;/a&gt; to do it:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;tf&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;non_max_suppression(boxes, scores, max_output_size, iou_threshold)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The main gist of this method is that it will remove overlapping boxes
(such as in Figure 6) to keep only one. It also using the &lt;code&gt;scores&lt;/code&gt; to keep the
most probable box.&lt;/p&gt;

&lt;p&gt;A general comment on the input parameter of the Tensorflow method above:
The &lt;code&gt;max_output_size&lt;/code&gt; corresponds to the maximum number of boxes we want at the
end — let’s say 300. The &lt;code&gt;iou_threshold&lt;/code&gt; is a float between 0 and 1, describing
the maximum ratio of overlapping that is accepted.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/nms_after.png&#34; alt=&#34;*Figure 7: Figure 6 after the non-max-suppression has been applied.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 7: Figure 6 after the non-max-suppression has been applied.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h4 id=&#34;2-6-keeping-the-most-probable-class&#34;&gt;2.6. Keeping the most probable class&lt;/h4&gt;

&lt;p&gt;Duplicate boxes for the same class at the same place are now removed. For each
of the remaining boxes, we are keeping only the highest-scoring class
(car, truck, etc.). If none of the classes have a score above a fixed threshold
(we used $0.4$), it’s considered to be part of the background.&lt;/p&gt;

&lt;h4 id=&#34;2-7-the-focal-loss&#34;&gt;2.7. The Focal Loss&lt;/h4&gt;

&lt;p&gt;All this may sound complicated, but it’s nothing new — it’s not enough to
have good accuracy. The real improvement from RetinaNet is its loss: the
&lt;strong&gt;Focal Loss&lt;/strong&gt;. Single-stage architectures that don’t have potential objects
pre-selection are overwhelmed with the high frequency of background objects.
The Focal Loss deals with it by according a low weight to well-classified
examples, usually the background.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/focal_loss1.png&#34; alt=&#34;*Figure 8: We define Pt, the confidence to be right*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 8: We define Pt, the confidence to be right&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In Figure 8, we define $p_t$, the confidence to be right in a binary
classification.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/focal_loss2.png&#34; alt=&#34;*Figure 9: The Focal Loss*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 9: The Focal Loss&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;In Figure 9, we module the cross entropy loss $-\log(p_t)$ by a factor
$(1 — p_t)^\gamma$. Here, $\gamma$ is a modulating factor oscillating between
0 and 5. The well-classified examples have a high $p_t$ , and thus a low factor.
Therefore, the loss for well-classified examples is low and forces the
model learn on harder examples. You can see in Figure 10 how much the loss is
affected.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/focal_loss3.png&#34; alt=&#34;*Figure 10: The focal loss under various modulating factors*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 10: The focal loss under various modulating factors&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;3-implementation&#34;&gt;3. Implementation&lt;/h3&gt;

&lt;p&gt;We used the excellent Keras &lt;a href=&#34;https://github.com/fizyr/keras-retinanet&#34; target=&#34;_blank&#34;&gt;implementation&lt;/a&gt;
of RetinaNet by Fizyr. We also wrote a new generator, taking Pandas’ DataFrames
instead of CSV files.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;DfGenerator&lt;/span&gt;(CSVGenerator):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, df, class_mapping, cols, base_dir&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Custom generator intended to work with in-memory Pandas&amp;#39; dataframe.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        Arguments:
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            df: Pandas DataFrame containing paths, labels, and bounding boxes.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            class_mapping: Dict mapping label_str to id_int.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;            cols: Dict Mapping &amp;#39;col_{filename/label/x1/y1/x2/y2} to corresponding df col.
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        &amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;base_dir &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base_dir
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cols &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cols
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; class_mapping
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;labels &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {v: k &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; k, v &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;classes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;items()}

        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image_data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_read_data(df)
        self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image_names &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image_data&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;keys())

        Generator&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__(self, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_read_classes&lt;/span&gt;(self, df):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {row[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]: row[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _, row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iterrows()}

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __len__(self):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; len(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;image_names)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;_read_data&lt;/span&gt;(self, df):
        data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {}
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; _, row &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iterrows():
            img_file, class_name &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; row[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cols[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;col_filename&amp;#39;&lt;/span&gt;]], row[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cols[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;col_label&amp;#39;&lt;/span&gt;]]
            x1, y1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; row[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cols[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;col_x1&amp;#39;&lt;/span&gt;]], row[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cols[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;col_y1&amp;#39;&lt;/span&gt;]]
            x2, y2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; row[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cols[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;col_x2&amp;#39;&lt;/span&gt;]], row[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cols[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;col_y2&amp;#39;&lt;/span&gt;]]

            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; img_file &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data:
                data[img_file] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []

            &lt;span style=&#34;color:#75715e&#34;&gt;# Image without annotations&lt;/span&gt;
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; isinstance(class_name, str) &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isnan(class_name):
                &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;

            data[img_file]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append({
                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x1&amp;#39;&lt;/span&gt;: int(x1), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x2&amp;#39;&lt;/span&gt;: int(x2),
                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;y1&amp;#39;&lt;/span&gt;: int(y1), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;y2&amp;#39;&lt;/span&gt;: int(y2),
                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;class&amp;#39;&lt;/span&gt;: class_name
            })

&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; data&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As you can see, &lt;strong&gt;images without annotations are kept in the training phase&lt;/strong&gt;.
They still help the training of our algorithm, as it forces the algorithm to
not see cars everywhere (even where there aren’t any).&lt;/p&gt;

&lt;p&gt;We used a pre-trained RetinaNet on &lt;a href=&#34;http://cocodataset.org/&#34; target=&#34;_blank&#34;&gt;COCO&lt;/a&gt; and then
fine-tuned it for the COWC dataset. Only the two FCNs are retrained for this
new task, while the ResNet backbone and the FPN are frozen.&lt;/p&gt;

&lt;p&gt;You can see in the code block below how to load the RetinaNet and compile it.
Note that it is important to add &lt;code&gt;skip_mismatch=True&lt;/code&gt; when loading the weights!
The weights were created on COCO with 80 classes, but in our case we only have
1 class, thus the number of anchors is not the same.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_retinanet&lt;/span&gt;(weights, n_classes, freeze&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True):
    modifier &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; freeze_model &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; freeze &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; None

    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; resnet50_retinanet(num_classes&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;num_classes, modifier&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;modifier)
    model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load_weights(weights, by_name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True, skip_mismatch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; model


&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compile&lt;/span&gt;(model):
    model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;compile(
        loss&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;regression&amp;#39;&lt;/span&gt;    : keras_retinanet&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;losses&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;smooth_l1(),
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;classification&amp;#39;&lt;/span&gt;: keras_retinanet&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;losses&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;focal()
        },
        optimizer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;optimizers&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;adam(lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;configs[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;lr&amp;#39;&lt;/span&gt;], clipnorm&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.001&lt;/span&gt;)
    )


&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train&lt;/span&gt;(model, train_gen, val_gen, callbacks, n_epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;train_gen and val_gen are instances of DfGenerator.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_generator(
        train_gen,
        steps_per_epoch&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;len(train_gen),
        validation_data&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;val_gen,
        validation_steps&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;len(val_gen),
        callbacks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;callbacks,
        epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;n_epochs,
        verbose&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
)&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There is something we still need to deal with, which is the &lt;strong&gt;massive weight
of each image&lt;/strong&gt;. Images from the COWC dataset are up to 4 square kilometers,
or 13k pixel wide and high. Those big images weigh 300mb. It is impracticable
to feed such large images to our RetinaNet. Therefore, we cut the images in
patches of 1000x1000 pixels (or 150x150 meters).&lt;/p&gt;

&lt;p&gt;However, it would be stupid to miss cars because they’d been cut between two
patches. So to avoid this problem, we made a sliding window of 1000x1000 pixels
that moves by steps of 800 pixels. That way, there is a 200-pixel-wide overlap
between two adjacent patches.&lt;/p&gt;

&lt;p&gt;This leads to another problem: we may detect cars twice. To remove duplicates,
we applied non-max suppression when binding together the small patches. Indeed,
that means we have a non-max suppression twice: after the RetinaNet and when
binding together the small patches. For the second non-max suppression, we used
a Numpy version of the algorithm. You can either use a &lt;a href=&#34;https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/&#34; target=&#34;_blank&#34;&gt;fast &amp;amp; vectorized version&lt;/a&gt;
by PyImageSearch, or the following naive version:&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;jaccard&lt;/span&gt;(a, b):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Compute the jaccard score between box a and box b.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    side1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, min(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; max(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
    side2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, min(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; max(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
    inter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; side1 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; side2

    area_a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (a[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; a[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (a[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; a[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
    area_b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (b[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; b[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (b[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; b[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])

    union &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; area_a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; area_b &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; inter

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; inter &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; union


&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;naive_nms&lt;/span&gt;(boxes, scores, threshold&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.4&lt;/span&gt;):
    scores_idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; scores&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argsort()[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#75715e&#34;&gt;# Keep highest scores first&lt;/span&gt;
    boxes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; boxes[scores_idx]

    indices_to_skip &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; set()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(boxes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(boxes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]):
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; j &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; indices_to_skip:
                &lt;span style=&#34;color:#66d9ef&#34;&gt;continue&lt;/span&gt;

            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; jaccard(boxes[i], boxes[j]) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; threshold:
                indices_to_skip&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;add(j)

    mask &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(boxes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bool)
    mask[np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;array(list(indices_to_skip))] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; boxes[mask]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;When dealing with aerial imagery, we can use a lot of data augmentation.
First of all, we can flip the horizontal axis and the vertical axis. We can also
rotate the image by any angle. If the imagery’s scale is not uniform (the
distance drone-to-ground may not be constant), it is also useful to randomly
scale down and up the pictures.&lt;/p&gt;

&lt;h3 id=&#34;4-results&#34;&gt;4. Results&lt;/h3&gt;

&lt;p&gt;You can see on Figures 11 and 12 below how our RetinaNet behaves on this
unseen image of Salt Lake City.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/nato_result_large.png&#34; alt=&#34;*Figure 11: 13,000 detected cars in a 4 square kilometer area of Salt Lake City*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 11: 13,000 detected cars in a 4 square kilometer area of Salt Lake City&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/nato_result_zoom.png&#34; alt=&#34;*Figure 12: A zoom in of Figure 11*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 12: A zoom in of Figure 11&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;5-are-we-good&#34;&gt;5. Are we good?&lt;/h3&gt;

&lt;p&gt;How can we evaluate our performance?&lt;/p&gt;

&lt;p&gt;Accuracy is not enough; we need to see how many &lt;strong&gt;false positives&lt;/strong&gt; and &lt;strong&gt;false
negatives&lt;/strong&gt; we get. If we detect cars everywhere, we’d have a lot of false
positive, but if we miss most of the cars, that’s a lot of false negative.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;recall&lt;/strong&gt; measures the former while the &lt;strong&gt;precision&lt;/strong&gt; measures the latter. Finally,
the &lt;strong&gt;f1-score&lt;/strong&gt; is a combination of those two metrics.&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;compute_metrics&lt;/span&gt;(true_pos, false_pos, false_neg):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;Compute the precision, recall, and f1 score.&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    precision &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; true_pos &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (true_pos &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; false_pos)
    recall &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; true_pos &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (true_pos &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; false_neg)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; precision &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;or&lt;/span&gt; recall &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; precision, recall, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

    f1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; precision &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; recall)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; precision, recall, f1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;However, we are not expecting our RetinaNet to detect the cars at the exact
right pixels. Therefore, we are computing the &lt;strong&gt;Jaccard Index&lt;/strong&gt; of the detected
cars and the ground-truth cars, and if it is more than a chosen threshold, we
consider that the car was rightfully detected. Note that the Jaccard index is
often also (blandly) called &lt;strong&gt;Intersection-over-Union&lt;/strong&gt; (IoU):&lt;/p&gt;

&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;jaccard&lt;/span&gt;(box_a, box_b):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&amp;#34;box_X is a tuple of the shape (x1, y1, x2, y2).&amp;#34;&amp;#34;&amp;#34;&lt;/span&gt;
    side1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, min(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; max(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]))
    side2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; max(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, min(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; max(a[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;], b[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
    inter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; side1 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; side2

    area_a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (a[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; a[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (a[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; a[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
    area_b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (b[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; b[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (b[&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; b[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
    union &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; area_a &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; area_b &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; inter

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; inter &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; union


&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_valid&lt;/span&gt;(box_pred, box_true, threshold&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; jaccard(box_red, box_true) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; threshold&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/nato_result_color.png&#34; alt=&#34;*Figure 13: True Positive (green), False Positive (yellow), and False Negative (red)*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 13: True Positive (green), False Positive (yellow), and False Negative (red)&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;You can see a sample on Figure 13 where true positives, false positives, and
false negatives have been plotted.&lt;/p&gt;

&lt;p&gt;Note that among the four false positives, two of them are garbage bins, one is
a duplicate, and one is actually&amp;hellip; a car! Indeed, as in every dataset, there
may be some errors in the ground-truth annotations.&lt;/p&gt;

&lt;p&gt;On Figure 12, the f1-score is $0.91$. Usually in more urban environments the
f1-score is around $0.95$. The main mistake our model makes is considering
ventilation shafts on tops of buildings to be cars. To the model’s defense,
without knowledge of building, it’s quite hard to see that.&lt;/p&gt;

&lt;h3 id=&#34;6-conclusion&#34;&gt;6. Conclusion&lt;/h3&gt;

&lt;p&gt;For the NATO challenge, we didn’t only use car detection from aerial imagery,
but it was the main technical part of the project.&lt;/p&gt;

&lt;p&gt;Oh&amp;hellip; Did we forget to tell you the challenge results?&lt;/p&gt;

&lt;p&gt;Three prizes were awarded: The NATO prize (with a trip to Norfolk), the
France prize (with $25k), and the Germany prize (with a trip to Berlin).&lt;/p&gt;

&lt;p&gt;We won both the NATO and France prize!&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/nato_award.png&#34; alt=&#34;*Figure 14: General Maurice, Supreme Commander Mercier, and our team*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 14: General Maurice, Supreme Commander Mercier, and our team&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Thanks to &lt;a href=&#34;https://medium.com/@HichamEB?source=post_page&#34; target=&#34;_blank&#34;&gt;Hicham El Boukkouri&lt;/a&gt; &amp;amp;
&lt;a href=&#34;https://medium.com/@dsleo?source=post_page&#34; target=&#34;_blank&#34;&gt;Léo Dreyfus-Schmidt&lt;/a&gt; for their review
of this blog post.&lt;/p&gt;

&lt;p&gt;Note that this post was at first published on &lt;a href=&#34;https://medium.com/data-from-the-trenches/object-detection-with-deep-learning-on-aerial-imagery-2465078db8a9&#34; target=&#34;_blank&#34;&gt;Dataiku&amp;rsquo;s technical blog on Medium&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PhD Grind</title>
      <link>/books/phd-grind/</link>
      <pubDate>Tue, 15 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/phd-grind/</guid>
      <description>&lt;p&gt;PhD Grind (&lt;a href=&#34;http://pgbovine.net/PhD-memoir/pguo-PhD-grind.pdf&#34; target=&#34;_blank&#34;&gt;pdf url&lt;/a&gt;) is not
strictly speaking a book, but rather a pdf memoir. It is the story of a PhD, during
its six long years, in the USA in Computer Science (Model checking &amp;amp; HCI).&lt;/p&gt;

&lt;p&gt;I found this book fantastic. As a first-year PhD, it really talked to me. I was interested
to discover the PhD system in the USA, which sounds quite different from what I know
in France.&lt;/p&gt;

&lt;p&gt;I also very much enjoyed the unfiltered speech of a student who had doubts, failures,
and success (spoiler: there is a happy ending.)&lt;/p&gt;

&lt;p&gt;If I find motivation, and time, I would like to write such memoir about my own
thesis.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3 Small but Powerful Convolutional Neural Networks</title>
      <link>/post/3-small-but-powerful-cnn/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/3-small-but-powerful-cnn/</guid>
      <description>

&lt;p&gt;Most CNN architectures have been developed to attain the best accuracy on
ImageNet. Computing power is not limited for this competition, why bother?&lt;/p&gt;

&lt;p&gt;However you may want to run your model on an old laptop, maybe without GPU, or
even on your mobile phone. Let’s see three CNN architectures that are efficient
while sacrificing little accuracy performance.&lt;/p&gt;

&lt;h3 id=&#34;1-mobilenet&#34;&gt;1. MobileNet&lt;/h3&gt;

&lt;p&gt;Arxiv link: &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34; target=&#34;_blank&#34;&gt;(Howard et al, 2017)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MobileNet uses depthwise separable convolutions. This convolution block was at
first introduced by Xception &lt;a href=&#34;https://arxiv.org/abs/1610.02357&#34; target=&#34;_blank&#34;&gt;(Chollet, 2016)&lt;/a&gt;.
A depthwise separable convolution is made of two operations: a depthwise
convolution and a pointwise convolution.&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;standard convolution&lt;/strong&gt; works on the spatial dimension of the feature maps and on
the input and output channels. It has a computational cost of
$D_f^2 * M * N * D_k^2$; with $D_f$ the dimension of the input feature maps,
$M$ and $N$ the number of input and output channels, and $D_k$ the kernel size.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/convolution.png&#34; alt=&#34;convolution&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;depthwise convolution&lt;/strong&gt; maps a single convolution on each input channel separately.
Therefore its number of output channels is the same of the number of input channels.
Its computational cost is $D_f^2 * M * D_k^2$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/depthwise_conv.png&#34; alt=&#34;depthwise conv&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The last operation is a &lt;strong&gt;pointwise convolution&lt;/strong&gt;. It is a convolution with a
kernel size of 1x1 that simply combines the features created by the depthwise
convolution. Its computational cost is $M * N * D_f^2$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/pointwise_conv.png&#34; alt=&#34;pointwise conv&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The computational cost of the &lt;strong&gt;depthwise separable convolution&lt;/strong&gt; is the sum of
the costs of the depthwise and pointwise operations. Compared to a standard
convolution it offers a computation reduction of $\frac{1}{N} + \frac{1}{D_k^2}$.
With a kernel size of 3x3, it results in 8 times less operations!&lt;/p&gt;

&lt;p&gt;MobileNet also provides two parameters allowing to reduce further more its
number of operations:&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;width multiplier&lt;/strong&gt; (between 0 and 1) thins the number of channels. At
each layer instead of producing $N$ channels, it will produce $\alpha * N$.
This multiplier can be used to handle a trade-off between the desired latency
and the performance.&lt;/p&gt;

&lt;p&gt;Another multiplier exists: the &lt;strong&gt;resolution multiplier&lt;/strong&gt;. It scales the input
size of the image, between 224 to 128. Because the MobileNet uses a global
average pooling instead of a flatten, you can train your MobileNet on
224x224 images, then use it on 128x128 images! Indeed with a global pooling,
the fully connected classifier at the end of the network depends only the number
of channels not the feature maps spatial dimension.&lt;/p&gt;

&lt;h3 id=&#34;2-shufflenet&#34;&gt;2. ShuffleNet&lt;/h3&gt;

&lt;p&gt;Arxiv link: &lt;a href=&#34;https://arxiv.org/abs/1707.01083&#34; target=&#34;_blank&#34;&gt;(Zhang et al, 2017)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ShuffleNet introduces the three variants of the Shuffle unit. It is composed
of &lt;strong&gt;group convolutions&lt;/strong&gt; and &lt;strong&gt;channel shuffles&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/shufflenet.png&#34; alt=&#34;shufflenet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;group convolution&lt;/strong&gt; is simply several convolutions, each taking a portion
of the input channels. In the following image you can see a group convolution,
with 3 groups, each taking one of the 3 input channels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/group_conv.png&#34; alt=&#34;group conv&#34; /&gt;&lt;/p&gt;

&lt;p&gt;It was at first introduced by AlexNet &lt;a href=&#34;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks&#34; target=&#34;_blank&#34;&gt;(Krizhevsky et al, 2012)&lt;/a&gt;
to split a network into two GPUs.&lt;/p&gt;

&lt;p&gt;It greatly diminishes the computational cost. Let us take a practicable example:
If there are 4 input channels, and 8 output channels and we choose to have
two groups, each taking 2 input channels and 4 output channels.&lt;/p&gt;

&lt;p&gt;With one group the computational cost would be $D_f^2 * D_k^2 * 4 * 8$, while with
two groups the cost is $(D_f^2 * D_k^2 * 2 * 4) * 2$ or $D_f^2 * D_k^2 * 4 * 4$.
Half as many operations! The authors reached best results with 8 groups, thus the reduction is even more important.&lt;/p&gt;

&lt;p&gt;Finally the authors add a channel shuffle that randomly mix the output channels
of the group convolution. The trick to produce this randomness can be seen
&lt;a href=&#34;https://github.com/arthurdouillard/keras-shufflenet/blob/master/shufflenet.py#L37-L48&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;3-effnet&#34;&gt;3. EffNet&lt;/h3&gt;

&lt;p&gt;Arxiv link: &lt;a href=&#34;https://arxiv.org/abs/1801.06434&#34; target=&#34;_blank&#34;&gt;(Freeman et al, 2018)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;EffNet uses &lt;strong&gt;spatial separable convolutions&lt;/strong&gt;. It is similar to
MobileNet&amp;rsquo;s depthwise separable convolutions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/effnet.png&#34; alt=&#34;effnet&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The separable depthwise convolution is the rectangle colored in blue for
EffNet block. It is made of depthwise convolution with a line kernel (1x3),
followed by a separable pooling, and finished by a depthwise convolution with a
column kernel (3x1)&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see the computational gain. A normal depthwise with a 3x3 kernel would have
a cost of $3^2 * D_f^2 * M$. The first depthwise with a 1x3 kernel has a
computational cost of $3 * D_f^2 * M$. The separable pooling halves the feature
maps height and has a marginal cost. The second depthwise, with a 3x1 kernel,
has then a cost of $3 * \frac{D_f^2}{2} * M$. Thus the whole cost is
$1.5 * (3 * D_f^2 * M)$. Half less than the normal depthwise!&lt;/p&gt;

&lt;p&gt;Another optimization done by EffNet over MobileNet and ShuffleNet, is the
absence of &amp;ldquo;normal convolution&amp;rdquo; at the beginning:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/effnet2.png&#34; alt=&#34;effnet2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To quote the authors (emphasis mine):&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Both MobileNet and ShuffleNet avoided replacing the first layer with the
claim that this layer is already rather cheap to begin with. We respectfully
disagree with this claim and believe that every optimisation counts. After
having optimised the rest of the layers in the network, the first layer
becomes proportionally larger. In our experiments, &lt;strong&gt;replacing the first layer
with our EffNet block saves ∼ 30% of the computations for the respective layer&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h1 id=&#34;4-conclusion&#34;&gt;4. Conclusion&lt;/h1&gt;

&lt;p&gt;MobileNet, ShuffleNet, and EffNet are CNN architectures conceived to optimize
the number of operations. Each replaced the standard convolution with their
own version.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MobileNet&lt;/strong&gt; (&lt;a href=&#34;https://github.com/arthurdouillard/keras-mobilenet&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;)
depthwise separable convolution uses a depthwise
convolution followed by a pointwise convolution. In a addition it introduces
two hyperparameters: the width multiplier that thins the number of channels,
and the resolution multiplier that reduces the feature maps spatial dimensions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;ShuffleNet&lt;/strong&gt; (&lt;a href=&#34;https://github.com/arthurdouillard/keras-shufflenet&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;)
uses pointwise convolution in groups. In order to combine the features produced
by each group, a shuffle layer is also introduced.&lt;/p&gt;

&lt;p&gt;Finally &lt;strong&gt;EffNet&lt;/strong&gt; (&lt;a href=&#34;https://github.com/arthurdouillard/keras-effnet&#34; target=&#34;_blank&#34;&gt;github&lt;/a&gt;)
uses spatial separable convolution, which is simply a depthwise convolution
splitted along spatial axis with a separable pooling between them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/efficient_cmp.png&#34; alt=&#34;cmp&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This article was at first published in &lt;a href=&#34;https://towardsdatascience.com/3-small-but-powerful-convolutional-networks-27ef86faa42d&#34; target=&#34;_blank&#34;&gt;Towards Data Science&lt;/a&gt;
and has also been &lt;a href=&#34;https://yq.aliyun.com/articles/592935&#34; target=&#34;_blank&#34;&gt;translated in Chinese&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ball Lightning</title>
      <link>/books/ball-lightning/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/ball-lightning/</guid>
      <description>&lt;p&gt;Ball Lightning is an early book of Cixin Liu, the Three-Body Problem trilogy author.
While not as good as its trilogy, the book was interesting to read because of its theme:
Science.&lt;/p&gt;

&lt;p&gt;The protagonists try to understand the ball lightning (&lt;a href=&#34;https://en.wikipedia.org/wiki/Ball_lightning&#34; target=&#34;_blank&#34;&gt;a real phenomenon&lt;/a&gt;).
The whole book is structured around their experiments, their thoughts, and failures.
The end of the book goes a bit far, and is clearly &amp;ldquo;science-fiction&amp;rdquo;, but still in a &amp;ldquo;hard&amp;rdquo;, plausible way.
This won&amp;rsquo;t surprise those who read the trilogy.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Mr Ma and Son</title>
      <link>/books/mr-ma-and-son/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/mr-ma-and-son/</guid>
      <description>&lt;p&gt;This is the story of a father and his son. Coming from Beijing ([[China]]), they come to London with different goals. The son hopes to learn and to held a business, the father just follow the stream.&lt;/p&gt;

&lt;p&gt;The story itself isn&amp;rsquo;t very interesting. What I liked in this book was the description of [[Racism]] and [[Inter-Generational Gap]].&lt;/p&gt;

&lt;p&gt;English men are deeply racist against Chinese men, but the latter &amp;mdash;especially the old generation&amp;ndash; also hold preconceived ideas on the western culture.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Three-Body Problem trilogy</title>
      <link>/books/three-body/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/three-body/</guid>
      <description>&lt;p&gt;Except the Sun Tzu&amp;rsquo;s Art of War, the Three-Body Problem first tome was my first
Chinese book. The first book set the tone, starting in communist-era in the 70s.&lt;/p&gt;

&lt;p&gt;When I introduce this trilogy to friends, I often say &amp;mdash;no spoilers:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Several of world best theoretical physicists suicide. They all leave a note,
saying in substance: &amp;ldquo;Physic doesn&amp;rsquo;t exist&amp;rdquo;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You don&amp;rsquo;t need to know more to go buy this book. It&amp;rsquo;s hard-science-fiction at its
best.&lt;/p&gt;

&lt;p&gt;WARNING, SPOILERS-AHEAD:&lt;/p&gt;

&lt;p&gt;The first half of the first book (The Three-Body Problem) is very good. The initial
problem, physic doesn&amp;rsquo;t exist, is both original and extremely exciting. I dislike
the second half, especially the explanation about sophons and how they would
communicate by quantic entanglement (&lt;a href=&#34;https://briankoberlein.com/blog/quantum-entanglement/&#34; target=&#34;_blank&#34;&gt;which doesn&amp;rsquo;t work like this&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The second book was particularly surprising. I really like the Wallfacers concepts:
four men whose objective is to find a way to defeat high-tech aliens in 300 years.
And they have to keep their plans secret to avoid leaking info through the sophons.
The jump in the future was similar to the movie &lt;em&gt;the 5th element&lt;/em&gt; where everything
just seems crazy. And finally come the key idea of this book, and trilogy: the dark forest
hypothesis. It&amp;rsquo;s both terrifying and plausible.&lt;/p&gt;

&lt;p&gt;The third book is at first nice, a lot of tech progress is done. However its very
end seems a bit far fetched, closer to fantasy than hard-sf. But I guess when talking
about eons in the future, anything is possible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Very Short Introduction: Food</title>
      <link>/books/very-short-intro-food/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/books/very-short-intro-food/</guid>
      <description>&lt;p&gt;Very Short Introduction (VSI) is an Oxford collection about various subjects, ranging from humanities
(arts, religion, politics, etc.) to very technical field (neuroscience, physics, etc.). With a small
format (~100 pages), they aim to be a good introduction.&lt;/p&gt;

&lt;p&gt;VSI: Food fulfills perfectly this mission. The book is simple enough for a neophyte in food to understand,
and still manages to cover a wide breadth of topics, all centered around food. It&amp;rsquo;s far easier than its
cousin VSI: Nutrition.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Densely Connected Convolutional Networks</title>
      <link>/post/densenet/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/densenet/</guid>
      <description>

&lt;p&gt;This article contains note of the research paper:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1608.06993&#34; target=&#34;_blank&#34;&gt;Densely Connected Convolutional Networks&lt;/a&gt; by
Cornell Uni, Tsinghua Uni, and Facebook Research.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This paper was awarded the &lt;a href=&#34;http://cvpr2017.thecvf.com/&#34; target=&#34;_blank&#34;&gt;CVPR 2017&lt;/a&gt; Best Paper Award.&lt;/p&gt;

&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;DenseNet&lt;/strong&gt; is a new CNN architecture that reached State-Of-The-Art (SOTA) results
on classification datasets (CIFAR, SVHN, ImageNet) using less parameters.&lt;/p&gt;

&lt;p&gt;Thanks to its new use of residual it can be deeper than the usual networks and
still be easy to optimize.&lt;/p&gt;

&lt;h3 id=&#34;general-architecture&#34;&gt;General Architecture&lt;/h3&gt;

&lt;p&gt;DenseNet is composed of &lt;strong&gt;Dense blocks&lt;/strong&gt;. In those blocks, the layers are
&lt;em&gt;densely&lt;/em&gt; connected together: Each layer receive in input all previous layers
output feature maps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/densenet.png&#34; alt=&#34;dense block&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This extreme use of residual creates a &lt;strong&gt;deep supervision&lt;/strong&gt; because each layer
receive more supervision from the loss function thanks to the shorter connections.&lt;/p&gt;

&lt;h4 id=&#34;1-dense-block&#34;&gt;1. Dense block&lt;/h4&gt;

&lt;p&gt;A dense block is a group of layers connected to all their previous layers. A single
layer looks like this:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Batch Normalization&lt;/li&gt;
&lt;li&gt;ReLU activation&lt;/li&gt;
&lt;li&gt;3x3 Convolution&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The authors found that the pre-activation mode (BN and ReLU before the Conv) was
more efficient than the usual post-activation mode.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that the authors recommend a zero padding before the convolution in order
to have a fixed size.&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&#34;2-transition-layer&#34;&gt;2. Transition layer&lt;/h4&gt;

&lt;p&gt;Instead of summing the residual like in &lt;a href=&#34;https://arxiv.org/abs/1603.05027&#34; target=&#34;_blank&#34;&gt;ResNet&lt;/a&gt;,
DenseNet concatenates all the feature maps.&lt;/p&gt;

&lt;p&gt;It would be impracticable to concatenate feature maps of different sizes (although
some resizing may work). Thus in each dense block, the feature maps of
each layer has the same size.&lt;/p&gt;

&lt;p&gt;However down-sampling is essential to CNN. Transition layers between two
dense blocks assure this role.&lt;/p&gt;

&lt;p&gt;A transition layer is made of:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Batch Normalization&lt;/li&gt;
&lt;li&gt;1x1 Convolution&lt;/li&gt;
&lt;li&gt;Average pooling&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;growth-rate&#34;&gt;Growth rate&lt;/h3&gt;

&lt;p&gt;Concatenating residuals instead of summing them has a downside when the model
is very deep: It generates a lot of input channels!&lt;/p&gt;

&lt;p&gt;You may now wonder how could I say in the introduction that DenseNet has less
parameters than an usual SotA networks. There are two reasons:&lt;/p&gt;

&lt;p&gt;First of all a DenseNet&amp;rsquo;s convolution generates a low number of feature maps.
The authors recommend 32 for optimal performance but shows SotA results with only
12 output channels!&lt;/p&gt;

&lt;p&gt;The number of output feature maps of a layer is defined as the &lt;strong&gt;growth
rate&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;DenseNet has lower need of wide layers because as layers are densely connected
there is little redundancy in the learned features. All layers of a same dense block
share a &lt;em&gt;collective knowledge&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The growth rate regulates how much new information each layer contributes
to the global state.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&#34;bottleneck&#34;&gt;Bottleneck&lt;/h3&gt;

&lt;p&gt;The second reason DenseNet has few parameters despite concatenating many residuals
together is that each 3x3 convolution can be upgraded with a bottleneck.&lt;/p&gt;

&lt;p&gt;A layer of a dense block with a bottleneck will be:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Batch Normalization&lt;/li&gt;
&lt;li&gt;ReLU activation&lt;/li&gt;

&lt;li&gt;&lt;p&gt;1x1 Convolution bottleneck producing: $\text{grow rate} * 4$ feature maps.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Batch Normalization&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ReLU activation&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;3x3 Convolution&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;With a growth rate of 32, the tenth layer would have in input 288 feature maps!
Thanks to the bottleneck at most 128 feature maps would be fed to a layer. This
helps the network have hundred, if not thousand, layers.&lt;/p&gt;

&lt;h3 id=&#34;compression&#34;&gt;Compression&lt;/h3&gt;

&lt;p&gt;The authors further improves the compactness of the model with a &lt;em&gt;compression&lt;/em&gt;.
This compression happens in the transition layer.&lt;/p&gt;

&lt;p&gt;Normally the transition layer&amp;rsquo;s convolution does not change the number of feature
maps. In the case of the compression, its number of output feature maps is
$\theta * m$. With $m$ the number of input feature maps and $\theta$ a
compression factor between 0 and 1.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that the compression factor $\theta$ has the same role as the parameter
$\alpha$ in &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34; target=&#34;_blank&#34;&gt;MobileNet&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;The final architecture of DenseNet is the following:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/figures/densenet_archi.png&#34; alt=&#34;densenet architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To summarize, the DenseNet architecture uses the residual mechanism to its maximum
by making every layer (of a same dense block) connect to their subsequent
layers.&lt;/p&gt;

&lt;p&gt;This model&amp;rsquo;s compactness makes the learned features non-redundant as they are all
shared through a &lt;em&gt;common knowledge&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;It is also far more easy to train deep network with the dense connections because
of an &lt;em&gt;implicit deep supervision&lt;/em&gt; where the gradient is flowing back more easily
thanks to the short connections.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
