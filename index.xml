<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arthur Douillard on Arthur Douillard</title>
    <link>/</link>
    <description>Recent content in Arthur Douillard on Arthur Douillard</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0200</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion</title>
      <link>/publication/dytox/</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0100</pubDate>
      
      <guid>/publication/dytox/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Tackling Catastrophic Forgetting and Background Shift in Continual Semantic Segmentation</title>
      <link>/publication/objectrehearsal/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0200</pubDate>
      
      <guid>/publication/objectrehearsal/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DeepCourse</title>
      <link>/project/deepcourse/</link>
      <pubDate>Thu, 26 Aug 2021 00:00:00 +0200</pubDate>
      
      <guid>/project/deepcourse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Vision Transformers</title>
      <link>/post/visual_transformers/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0200</pubDate>
      
      <guid>/post/visual_transformers/</guid>
      <description>

&lt;p&gt;There are currently two broads axis of research in Deep Learning: finding better
architectures, or finding better losses to train them. Since AlexNet
(&lt;a href=&#34;https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf&#34; target=&#34;_blank&#34;&gt;Kriwhevsky et al.&lt;/a&gt;),
Convolutional Neural Network is the main architecture used in Computer Vision.
Through convolutions, with its efficient patch prior, many visual tasks were unlocked.&lt;/p&gt;

&lt;p&gt;Many variations of AlexNet were invented, but if we were to name only one, it would be
the ResNet (&lt;a href=&#34;https://arxiv.org/abs/1512.03385&#34; target=&#34;_blank&#34;&gt;He et al.&lt;/a&gt;) and its residual shortcut.&lt;/p&gt;

&lt;p&gt;The main drawback of convolutions is their lack of global reach. A problem, relatively similar
to the difficulty of Recurent Neural Networks (e.g. RNN, LSTM, GRU) to model long series of tokens.
The NLP community saw its &amp;ldquo;imagenet&amp;rdquo; moment when the Transformer (&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34;&gt;Vaswani et al.&lt;/a&gt;)
architecture was unveiled. First designed with a somewhat complex encoder-decoder,
it was later refined in a serie of identical blocks in the BERT (&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34;&gt;Devlin et al.&lt;/a&gt;)
model. Using transfer learning, most (all?) tasks of NLP were attacked via a BERT-like
architecture.&lt;/p&gt;

&lt;p&gt;Two questions:
- What are exactly transformers?
- And can (should?) we apply them to Computer Vision?&lt;/p&gt;

&lt;h1 id=&#34;what-are-transformers&#34;&gt;What are Transformers?&lt;/h1&gt;

&lt;p&gt;I can only recommend the excellent blog post of Jay Alammar &amp;ldquo;&lt;a href=&#34;https://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34;&gt;The Illustrated Transformer&lt;/a&gt;&amp;ldquo;
which explains very well this architecture. But here is a quick recap, a transformer is basically that:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;transformer.png&#34; alt=&#34;transformer diagram&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This whole &lt;strong&gt;block&lt;/strong&gt; is repeated multiple times, the output of the first goes into the second and so on.
Contrary to CNNs or MLPs, the number of dimensions doesn&amp;rsquo;t change across blocks.&lt;/p&gt;

&lt;p&gt;You can also notice &lt;strong&gt;residuals&lt;/strong&gt; twice. They are no different from the residuals in a ResNet, I guess that
a trick so good it is found everywhere.&lt;/p&gt;

&lt;h2 id=&#34;tokens&#34;&gt;Tokens&lt;/h2&gt;

&lt;p&gt;The four red squares on the left are the &lt;strong&gt;tokens&lt;/strong&gt;. There are only four on the image, but actually
you can use as much as you (and your RAM) want. A token is simply an piece of data that
was embedded. For example in NLP, you could have one token per word of your sentence. From
this word, we can use its &lt;strong&gt;word2vec&lt;/strong&gt; correspondence giving a latent vector. To also indicate
where this word is located in the sentence, we add a positional encoding.&lt;/p&gt;

&lt;p&gt;In this image, there is four tokens entering the block, and four tokens exiting the block. Therefore
at the end of the N blocks, we will still have four tokens, but only one of them will
be useful! The first token, here denoted by a &amp;lsquo;C&amp;rsquo;, is a special token called &lt;strong&gt;class token&lt;/strong&gt;. It doesn&amp;rsquo;t
belong to any word of the sentence, but is actually a vector that is learned during gradient descent.
Once this class token has gone through the N blocks, it will be finally given to
a classifier that will predict whatever you training for.&lt;/p&gt;

&lt;p&gt;Put it simply, this class token will go through the network and will extract as
much as possible useful information from the &lt;em&gt;word&lt;/em&gt; tokens, in order to produce
a good representation for the classifier.&lt;/p&gt;

&lt;h2 id=&#34;multi-headed-self-attention&#34;&gt;Multi-Headed Self-Attention&lt;/h2&gt;

&lt;p&gt;This is the main part of the transformer. First, let&amp;rsquo;s concentrate on the &amp;lsquo;self-attention&amp;rsquo;.&lt;/p&gt;

&lt;p&gt;We start with the matrix $X \in \mathbb{R}^{T \times d}$ containing all $T$ tokens. This matrix
is going to be linearly transformed (aka matrix multiplication, aka go to a fully-connected layer)
three times in parallel:&lt;/p&gt;

&lt;p&gt;$$Q = X W_q$$
$$K = X W_k$$
$$V = X W_v$$&lt;/p&gt;

&lt;p&gt;They represent respectively the &lt;strong&gt;Q&lt;/strong&gt;uery, the &lt;strong&gt;K&lt;/strong&gt;ey, and the &lt;strong&gt;V&lt;/strong&gt;alue.&lt;/p&gt;

&lt;p&gt;We compute the attention $A \in \mathbb{R}^{T \times T}$:&lt;/p&gt;

&lt;p&gt;$$A = \operatorname{softmax}(\frac{Q K^T}{\sqrt{d}})$$&lt;/p&gt;

&lt;p&gt;As the name imply, this attention will determine how much a token correspond to another.
Notice that the shape of attention matrix $A$ is $T \times T$. For each row $i$, the value
of the column $j$ is the strength of the relationship between the word $i$ and word $j$.&lt;/p&gt;

&lt;p&gt;We then multiply this attention matrix with the value matrix to &lt;em&gt;select&lt;/em&gt; the data:&lt;/p&gt;

&lt;p&gt;$$Z = A V$$&lt;/p&gt;

&lt;p&gt;Where $Z \in \mathbb{R}^{T \times d}$ which has the original shape.&lt;/p&gt;

&lt;p&gt;However, the attention matrix is quite restrictive: if the attention weight between
tokens $i$ and $j$ is large, then because of the softmax it will be low between
tokens $i$ and $u$. In order to model multiple kinds of attention, we use &lt;strong&gt;multiple heads&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s say we have two heads, then we do the previous self-attention but with different linear transformations:&lt;/p&gt;

&lt;p&gt;$$Q_1 = X W_{q_1}\,\, \text{and}\,\, Q_2 = X W_{q_2}$$
$$K_1 = X W_{k_1}\,\, \text{and}\,\, K_2 = X W_{k_2}$$
$$V_1 = X W_{v_1}\,\, \text{and}\,\, V_2 = X W_{v_2}$$&lt;/p&gt;

&lt;p&gt;$$Z_1 = \operatorname{softmax}(\frac{Q_1 K_1^T}{\sqrt{d}})\,\, \text{and}\,\, Z_2 = \operatorname{softmax}(\frac{Q_2 K_2^T}{\sqrt{d}})$$&lt;/p&gt;

&lt;p&gt;But then, we have a matrix $Z&amp;rsquo; = [Z_1 Z_2] \in \mathbb{R}^{T \times 2d}$ that is
bigger than expected. We simply use again a linear transformation $W_o \in \mathbb{R}^{2d \times d}$:&lt;/p&gt;

&lt;p&gt;$$Z = Z&amp;rsquo; W_o$$&lt;/p&gt;

&lt;p&gt;Here is a simplified version in PyTorch, first with a single head:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;SelfAttention&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, embed_dim):
    super()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()

    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scale &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embed_dim &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_o &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim)

  &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
    &lt;span style=&#34;color:#75715e&#34;&gt;# x is of shape (Batch size, nb of tokens, embedding dimension)&lt;/span&gt;
    B, N, C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape

    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_q(x)  &lt;span style=&#34;color:#75715e&#34;&gt;# (Batch size, nb of tokens, embedding dimension)&lt;/span&gt;
    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_k(x)
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_v(x)

    attention &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;matmul(q, k&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transpose(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scale
    attention &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;softmax(attention, dim&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)  &lt;span style=&#34;color:#75715e&#34;&gt;# (Batch size, nb of tokens, nb of tokens)&lt;/span&gt;

    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;matmul(attention, v)  &lt;span style=&#34;color:#75715e&#34;&gt;# (Batch size, nb of tokens, embedding dimension)&lt;/span&gt;
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;W_o(x)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And then, slightly more complex, with multi-heads:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;MultiHeadsSelfAttention&lt;/span&gt;(nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Module):
  &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self, embed_dim, num_heads):
    super()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__init__()

    head_dim &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; embed_dim &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; num_heads
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scale &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; head_dim &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_heads &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; num_heads

    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim, bias&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
    self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;projection &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(embed_dim, embed_dim)

  &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;forward&lt;/span&gt;(self, x):
    B, N, C &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape

    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;q(x)
    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;k(x)
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;v(x)

    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(B, N, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_heads, C &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_heads)
    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(B, N, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_heads, C &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_heads)
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(B, N, self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_heads, C &lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;num_heads)

    q &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; q&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permute(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
    k &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; k&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permute(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
    v &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; v&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permute(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)

    attention &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;matmul(q, k&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;transpose(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scale
    attention &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;softmax(attention, dim&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;matmul(attention, v)  &lt;span style=&#34;color:#75715e&#34;&gt;# B, H, N, Hd&lt;/span&gt;
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;permute(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;reshape(B, N, C)
    x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;projection(x)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;Note that in practice, to make the dimension of $Q$/$K$/$V$ independent to the number
of heads, we usually fix the maximum embedding size and split it equally between heads some space.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;layer-norm-and-mlp&#34;&gt;Layer Norm and MLP&lt;/h2&gt;

&lt;p&gt;The rest of the transformer is made of a MLP and two layers norms. The former
is simply several fully-connected layers with non-linear activations. The latter
are a form of &lt;a href=&#34;https://arthurdouillard.com/post/normalization/&#34; target=&#34;_blank&#34;&gt;normalization layer&lt;/a&gt;
where the input is subtracted by its mean and divided by its standard deviation, and
then is linearly transformed by a set of weights and bias.&lt;/p&gt;

&lt;h1 id=&#34;vit-vision-transformer&#34;&gt;ViT: Vision Transformer&lt;/h1&gt;

&lt;p&gt;Contrary to convolutions and recurrent networks, transformers don&amp;rsquo;t really have a prior
of the input data. So we should easily apply it on images, right?&lt;/p&gt;

&lt;p&gt;Some have incorporated part of the self-attention, as Axial-DeepLab (&lt;a href=&#34;https://arxiv.org/abs/2003.07853&#34; target=&#34;_blank&#34;&gt;Wang et al.&lt;/a&gt;).
But none to the best of my knowledge, used the whole transformer architecture on images before
ViT (&lt;a href=&#34;https://arxiv.org/abs/2010.11929&#34; target=&#34;_blank&#34;&gt;Dosovitskiy et al.&lt;/a&gt;).
Why? Well, in the case of images the number of tokens is huge. A $224 \times 224$ image
has $50,176$ pixels/tokens. This is too big to fit on a GPU.&lt;/p&gt;

&lt;p&gt;The key idea, maybe inspired by BagNet (&lt;a href=&#34;https://arxiv.org/abs/1904.00760&#34; target=&#34;_blank&#34;&gt;Brendel et al.&lt;/a&gt;),
is to consider a group of pixels as a patch. Instead of thousands of tokens, the image is
split into $14 \times 14 = 196$ tokens/patches, each made of $16 \times 16$ pixels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;vit.png&#34; alt=&#34;vit architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The transformer architecture is exactly the same as one you could find in NLP. The
only difference is how to embed the tokens. In this case, it&amp;rsquo;s simply a convolution applied
patch per patch (called &amp;lsquo;linear projection&amp;rsquo; in the image). In addition of positional encoding
based on the position of the patch in the image is used.&lt;/p&gt;

&lt;p&gt;The performances of ViT are very impressive. But a major drawback is that this architecture
has almost no prior for images. Therefore, while it can potentially reach better results given
infinitely large amount of data, it is very hard to train it with few images. The authors of ViT,
all from Google, use JFT-300M a large private dataset to pretrain their model on it before
transferring the knowledge to ImageNet-{21/1k}.&lt;/p&gt;

&lt;h1 id=&#34;deit-a-more-efficient-training-of-vision-transformer&#34;&gt;DeiT: A more efficient training of Vision Transformer&lt;/h1&gt;

&lt;p&gt;DeiT (&lt;a href=&#34;https://arxiv.org/abs/2012.12877&#34; target=&#34;_blank&#34;&gt;Touvron et al.&lt;/a&gt;) is an extention of ViT with two
main contributions: a more efficient training and a transformer-based knowledge distillation.&lt;/p&gt;

&lt;p&gt;Through tons of regularization, the authors manage to train a DeiT from scratch on ImageNet with
good performance. Two crucial regularizations are random erasing of the image pixels,
and stochastic depth (&lt;a href=&#34;https://arxiv.org/abs/1603.09382&#34; target=&#34;_blank&#34;&gt;Huang et al.&lt;/a&gt;). The latter is
a kind of dropout, but where a whole transformer block is dropped with only the residual used.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;deit_reg.png&#34; alt=&#34;deit regularization ablation, table 8 of paper&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The second contribution of DeiT is to propose to improve the training of the transformer
by using a teacher model. The teacher/student strategy is often seen where a large trained
teacher model produces novel targets to the student through knowledge distillation
(&lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34; target=&#34;_blank&#34;&gt;Hinton et al.&lt;/a&gt;). Basically, the student is trained
to predict the labels of the image but also to mimick the probabilities of the teacher.&lt;/p&gt;

&lt;p&gt;The first interesting conclusion is that it&amp;rsquo;s better to use a large CNN
(a RegNet (&lt;a href=&#34;https://arxiv.org/abs/2003.13678&#34; target=&#34;_blank&#34;&gt;Radosavovic et al.&lt;/a&gt;)) as a teacher than
a large vision transformer. Intuitively, we may hypothesize that is because a convolution-based
teacher has learned different knowledge and thus may diversify better the predictions of the student.&lt;/p&gt;

&lt;p&gt;The second conclusion is that using the output of the student classifier is not the best way
to do knowledge distillation. Recall that the transformer&amp;rsquo;s classifier input is the class token that
has gone through all blocks. DeiT instead add a new token, called a &lt;strong&gt;distillation token&lt;/strong&gt;,
and with it a new classifier. This classifier will be solely devoted to the task of distillation
with the teacher, while the original classifier with class token will resume the image classification task.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;deit.png&#34; alt=&#34;deit architecture&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;convit-make-prior-great-again&#34;&gt;ConViT: Make Prior Great Again&lt;/h1&gt;

&lt;p&gt;ViT needs a 300M dataset to train. DeiT &lt;em&gt;only&lt;/em&gt; needs a 1.2M dataset (ImageNet) to train.
But can&amp;rsquo;t we learn with a few thousand of images like a good old resNet can?&lt;/p&gt;

&lt;p&gt;You need a prior to learn with few data point.&lt;/p&gt;

&lt;p&gt;This is the message of ConViT (&lt;a href=&#34;https://arxiv.org/abs/2103.10697&#34; target=&#34;_blank&#34;&gt;D&amp;rsquo;Ascoli et al.&lt;/a&gt;). They
modify the usual Self-Attention (SA) into a &lt;strong&gt;Gated Positional Self-Attention&lt;/strong&gt; (GPSA):&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;convit.png&#34; alt=&#34;convit architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As shown on the diagram, only the attention part is modified, while the multiplication
with the value matrix is left unchanged. This attention is now a linear combination
between the self-attention and a positional prior. A &lt;strong&gt;learned gate&lt;/strong&gt; (aka a learned vector followed
by a sigmoid) balances between both branch, although its initialization enforces at first
a strong emphasis on the positional prior. This positional prior, initialized the
right way, can mimick a convolutional kernel.&lt;/p&gt;

&lt;p&gt;A locality strength factor $\alpha$ can modulate how big this kernel should be:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;convit_locality.png&#34; alt=&#34;convit locality factor&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The authors find that at the training end, the early transformer blocks (2 - 7)
mainly use the positional prior, while the first and final blocks (8 - 10) use
the global reach of the self-attention.&lt;/p&gt;

&lt;p&gt;With few data, the positional prior has an important role as the following graph shows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;convit_efficiency.png&#34; alt=&#34;convit efficiency per sample&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;cait-further-refinements-of-deit&#34;&gt;CaiT: Further Refinements of DeiT&lt;/h1&gt;

&lt;p&gt;The authors from DeiT later released CaiT (&lt;a href=&#34;https://arxiv.org/abs/2103.17239&#34; target=&#34;_blank&#34;&gt;Touvron et al.&lt;/a&gt;),
a refinement of of DeiT.&lt;/p&gt;

&lt;p&gt;The first contribution is LayerScale, a ponderation scheme for the main branch in
the residual architecture. As for ResNet, each block has two branches: an identity
branch that often does nothing, and the branch with learnable parameters. In the case of
transformers, those parameters are either the MLP or the Multi-headed Self-Attention.&lt;/p&gt;

&lt;p&gt;Previous works (ReZero &lt;a href=&#34;https://arxiv.org/abs/2003.04887&#34; target=&#34;_blank&#34;&gt;Bachlenchner et al.&lt;/a&gt;,
SkipInit &lt;a href=&#34;https://arxiv.org/abs/2002.10444&#34; target=&#34;_blank&#34;&gt;De et al.&lt;/a&gt;, and FixUp &lt;a href=&#34;https://arxiv.org/abs/1901.09321&#34; target=&#34;_blank&#34;&gt;Zhang et al.&lt;/a&gt;)
wondered if we could train a very deep ResNet without batch normalization. They all found, with minor
variations, that it was possible if main branch of a ResNet block was weighted by
a learned scalar $\in \mathbb{R}$. LayerScale proposes more or less the same, but
with one scale per dimension $\in \mathbb{R}^D$, initialized to a small $\epsilon$ value:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;layer_scale.png&#34; alt=&#34;layer scale&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The second contribution of CaiT, is very similar to a contribution of ConViT (authors
are partially the same): CaiT also introduces the class token after several blocks.
The last few blocks with class tokens implement a &lt;strong&gt;Class Attention&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Contrary to &lt;strong&gt;Self-Attention&lt;/strong&gt;, this Class Attention &lt;em&gt;freezes&lt;/em&gt; the patch tokens:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;cait.png&#34; alt=&#34;cait architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Put it more simply, the patch tokens aren&amp;rsquo;t updated anymore after each block of Class Attention.
Moreover, the attention is not done between all patches tokens $X_p \in \mathbb{R}^{(T-1) \times D}$ anymore (at a quadratic cost $T^2$),
but only with the class token $X_c \in \mathbb{R}^{1 \times D}$ (at a linear cost $T$):&lt;/p&gt;

&lt;p&gt;$$Q = X_c W_q$$&lt;/p&gt;

&lt;p&gt;$$K = [X_c\, X_p] W_k$$&lt;/p&gt;

&lt;p&gt;$$V = [X_c X_p] W_v$$&lt;/p&gt;

&lt;p&gt;The computational speed up is easily seen as the attention product is not anymore quadratic
w.r.t the number of patches. Furthermore, it gives a nice performance gain, as the Self-Attention
layers are now allowed to focus on extracting class-agnostic features.&lt;/p&gt;

&lt;h1 id=&#34;the-future-of-vision-transformers&#34;&gt;The Future of Vision Transformers&lt;/h1&gt;

&lt;p&gt;ViT first proves that it was possible to train transformers on visual tasks. DeiT then showed
that with carefully designed regularizations, the training could be done on relatively
small scale datasets. This contribution, and the fantastic &lt;a href=&#34;https://github.com/rwightman/pytorch-image-models&#34; target=&#34;_blank&#34;&gt;Timm library&lt;/a&gt;,
opened a gold rush on transformers.&lt;/p&gt;

&lt;p&gt;I won&amp;rsquo;t enumerate all variants here. Some may be only a slight tweak and will be forgotten,
others may prove to be a stepping stone in the transformers world. Only time will tell.&lt;/p&gt;

&lt;p&gt;But, that&amp;rsquo;s assuming that transformer is indeed the right solution. &lt;strong&gt;Attention is all we need?&lt;/strong&gt;
Recent research on MLPs (yes, you read correctly) indicates that maybe attention is
only a special case of a more general framework. I&amp;rsquo;ll cover it in a later post!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Continuum: Simple Management of Complex Continual Learning Scenarios</title>
      <link>/publication/continuum/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0100</pubDate>
      
      <guid>/publication/continuum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PLOP: Learning without Forgetting for Continual Semantic Segmentation</title>
      <link>/publication/plop/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0100</pubDate>
      
      <guid>/publication/plop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TP Deep Learning RDFIA</title>
      <link>/rdfia/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0200</pubDate>
      
      <guid>/rdfia/</guid>
      <description>

&lt;p&gt;RDFIA / Master DAC &amp;amp; IMA / Sorbonne&lt;/p&gt;

&lt;p&gt;Le cours est organisé par le professeur Matthieu Cord. Vos assistants de TPs auquels
vous devrez envoyer vos travaux sont Asya Grechka (asya.grechka@lip6.fr), Alexandre Rame (alexandre.rame@lip6.fr) et moi-même Arthur
Douillard (arthur.douillard@lip6.fr).&lt;/p&gt;

&lt;p&gt;Pour simplifier notre tâche vous êtes priés de nous adresser les mails avec pour objet
&lt;code&gt;[RDFIA][TP-&amp;lt;numero&amp;gt;]&lt;/code&gt;.&lt;/p&gt;

&lt;h2 id=&#34;rappels&#34;&gt;Rappels&lt;/h2&gt;

&lt;p&gt;Les TPs seront en Python3 et plusieurs bibliothèques seront utilisées. Voici
quelques liens pour rappel, ou pour vous familiariser en avance:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://learnxinyminutes.com/docs/python/&#34; target=&#34;_blank&#34;&gt;Rappel de Python&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.scipy.org/doc/numpy/user/quickstart.html&#34; target=&#34;_blank&#34;&gt;Rappel de Numpy&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scikit-learn.org/stable/tutorial/basic/tutorial.html&#34; target=&#34;_blank&#34;&gt;Introduction de Scikit-Learn&lt;/a&gt;. L&amp;rsquo;api est très similaire quelque soit l&amp;rsquo;algorithme (init / fit / predict)&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html&#34; target=&#34;_blank&#34;&gt;Introduction de Pytorch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Les cours seront ajoutés au fur et à mesure.&lt;/p&gt;

&lt;p&gt;Calendrier où les cours de RDFIA sont indiqués: &lt;a href=&#34;https://sucal.aminedjeghri.tk/calendar/M2_IMA&#34; target=&#34;_blank&#34;&gt;calendrier&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;tp-1-2-sift-bag-of-words&#34;&gt;TP 1 - 2 : SIFT / Bag of words&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;7 &amp;amp; 14 &amp;amp; 21 Octobre 2020&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;del&gt;A rendre pour avant le 21 Octobre 2020 à 23h59.&lt;/del&gt;
&lt;strong&gt;MaJ: A rendre pour avant le 30 Octobre 2020 à 23h59.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Suite à la crise du Covid vous êtes présents en demi-groupes.  Ce TP 1-2
sera fait en 3 semaines (7, 14, et 21 Octobre). Le deuxième groupe ne nous aura donc que une fois en
présentiel mais nous serons plus indulgents, de plus nous pourrons répondre aux questions le 28 Octobre.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pour les prochains TPs (3 -&amp;gt; $\infty$) nous continuerons au rythme normal des années précédentes, il
faudra donc commencer le TP en remote.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp1-2.pdf&#34;&gt;TP1-2.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1jL8yy91z6RI0JJIxMQi6Odkh3uMeb7-H?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab&lt;/a&gt; (faire &amp;ldquo;File -&amp;gt; Save a copy in Drive&amp;rdquo;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NB: There is a mistake in our conv_separable, you should see that the images with horizontal and vertical gradients have been inversed. This only impair the visualization of the function compute_grad, but no worry the quality of the final sift stay the same.&lt;/p&gt;

&lt;p&gt;Pour aller plus loin, TP sur les SVMs (non noté!):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp2-bis.pdf&#34;&gt;TP2-bis.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1xkgV6yz2E6_41aYdIC8uSro6gl7eLHn6?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-3-4-introduction-aux-réseaux-de-neurones&#34;&gt;TP 3 - 4: Introduction aux réseaux de neurones&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;28 Octobre &amp;amp; 4 Novembre 2020&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp3-4.pdf&#34;&gt;TP3-4.pdf&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1MrenVA2opTP0zgut8_Q4O-VUa978Y2DI?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Pour le second groupe, tp en visio à 13h45 ici: &lt;a href=&#34;https://zoom.us/j/97871580043?pwd=NXB6Z29sUGtESTJQOXYvNkp4U0dFZz09&#34; target=&#34;_blank&#34;&gt;Zoom Link&lt;/a&gt;, password &lt;strong&gt;rdfia&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Solution gradient: &lt;a href=&#34;/files/rdfia_resources/tp3-4_math.pdf&#34;&gt;tp3-4_math.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-5-6-réseaux-convolutionnels-pour-l-image&#34;&gt;TP 5 - 6: Réseaux convolutionnels pour l&amp;rsquo;image&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;18 &amp;amp; 25 Novembre 2020&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Cette fois-ci, les deux groupes auront TPs en même temps, par zoom.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp5-6.pdf&#34;&gt;TP5-6.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1ZfD37TsJudcyUjff-D8Gagylf-R5NuHd?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zoom 18 Novembre 16h: &lt;a href=&#34;https://zoom.us/j/98138914049?pwd=ZTVkbWZsc21vS2tKeUF0cWFVeStOdz09&#34; target=&#34;_blank&#34;&gt;zoom link&lt;/a&gt;, password &lt;strong&gt;rdfia&lt;/strong&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Zoom 25 Novembre 16h: &lt;a href=&#34;https://zoom.us/j/91842902968?pwd=S3ZxQ1N0TFl6Y0syMyticGdlTHlodz09&#34; target=&#34;_blank&#34;&gt;zoom link&lt;/a&gt;, password &lt;strong&gt;rdfia&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enregistrement du zoom du 25 Nov: &lt;a href=&#34;https://drive.google.com/drive/folders/1lBxnQ3Yh4_Q-P2A26F5Uv12ooa5BvP2r?usp=sharing&#34; target=&#34;_blank&#34;&gt;video&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Le rendu est déplacé au 11 Décembre, 23h59.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vu que certains ont mal compris, nous considerons pour le premier rendu (TP1-2) que les questions
avec étoiles sont des bonus. Pour le rendu 2 (TP3-4-5-6) et les suivants, ces questions sont des questions
avec plus de points, les questions bonus seront, comme précisé dès le départ, dénotés par le mot &amp;ldquo;bonus&amp;rdquo;.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;tp-7-transfer-learning&#34;&gt;TP 7: Transfer Learning&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;9 Décembre 2020&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp7.pdf&#34;&gt;TP7.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1_RnEZX1Fp1z6seRM2c3mpHHowvDeQ1ka?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;a href=&#34;https://zoom.us/j/93572572065?pwd=STlWM0JwLzZJUTE5aXNldEhUeFUyZz09&#34; target=&#34;_blank&#34;&gt;zoom link&lt;/a&gt;, password &lt;strong&gt;rdfia&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://alexrame.github.io/&#34; target=&#34;_blank&#34;&gt;Alex&lt;/a&gt;&amp;rsquo;s slides: &lt;a href=&#34;/files/rdfia_resources/alex_slides_tp7.pdf&#34;&gt;slides.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-8-visualization&#34;&gt;TP 8: Visualization&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;16 Décembre 2020&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp8.pdf&#34;&gt;TP8.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Colab: &lt;a href=&#34;https://colab.research.google.com/drive/1ubvt5LuJ0SNqsw3rnHRbYHL3Ycp3f3rk?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;a href=&#34;https://zoom.us/j/96910655374?pwd=eWt2ZWoydXEvZHJwSEFEZHY1U3c1QT09&#34; target=&#34;_blank&#34;&gt;zoom link&lt;/a&gt;, password &lt;strong&gt;rdfia&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;tp-9-10-gan&#34;&gt;TP 9-10: GAN&lt;/h2&gt;

&lt;p&gt;&lt;em&gt;6 &amp;amp; 13 Janvier 2021&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Énoncé: &lt;a href=&#34;/files/rdfia_resources/tp9-10.pdf&#34;&gt;TP9-10.pdf&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Colab 1ère séance: &lt;a href=&#34;https://colab.research.google.com/drive/1judQvIGv965KBdmVRgrkHzGQICVzyA5s?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab 1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Zoom du 6 Janvier: &lt;a href=&#34;https://zoom.us/j/98963023208?pwd=RC9tQ2hXY28zVG9LVXd2N053ZjljZz09&#34; target=&#34;_blank&#34;&gt;zoom link&lt;/a&gt;, password &lt;strong&gt;rdfia&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enregistrement du 6 Janvier par Asya: &lt;a href=&#34;https://drive.google.com/file/d/1y_wTD5xAURFLthZX37vSqce9hypaqrdm/view?usp=sharing&#34; target=&#34;_blank&#34;&gt;video.mp4&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;FIX&lt;/strong&gt;: the link for the celeba and celeba64 datasets have been changed (the latter was broken). They are now
&lt;a href=&#34;http://webia.lip6.fr/~douillard/rdfia/celeba.zip&#34; target=&#34;_blank&#34;&gt;http://webia.lip6.fr/~douillard/rdfia/celeba.zip&lt;/a&gt; and &lt;a href=&#34;http://webia.lip6.fr/~douillard/rdfia/celeba64.zip&#34; target=&#34;_blank&#34;&gt;http://webia.lip6.fr/~douillard/rdfia/celeba64.zip&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Colab 2ème séance: &lt;a href=&#34;https://colab.research.google.com/drive/1t1N3-EtzWu6mY_-5Hr7Bb0jTgMTC-S4I?usp=sharing&#34; target=&#34;_blank&#34;&gt;colab 2&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Zoom du 13 Janvier: &lt;a href=&#34;https://zoom.us/j/91509429294?pwd=Y3VEV2xwUlZkdnVub01DV211NVpFdz09&#34; target=&#34;_blank&#34;&gt;zoom link&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Enregistrement du 13 Janvier par Alex + slides: &lt;a href=&#34;https://drive.google.com/drive/folders/14RSgh5ik8qV5bhdAf53MMWzzi5mXiCAW?usp=sharing&#34; target=&#34;_blank&#34;&gt;Google Drive&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;mises-à-jour&#34;&gt;Mises à jour:&lt;/h4&gt;

&lt;p&gt;2020-10-05, 22:34: Add TP 1-2.&lt;/p&gt;

&lt;p&gt;2020-10-07, 13:35: Add Colab link.&lt;/p&gt;

&lt;p&gt;2020-10-07, 14:15: Change Colab link.&lt;/p&gt;

&lt;p&gt;2020-10-14, 10:54: Update de date de rendu et précision covid.&lt;/p&gt;

&lt;p&gt;2020-10-21, 11:17: Add bonus on svm.&lt;/p&gt;

&lt;p&gt;2020-10-23, 14:32: Add warning about minor mistake.&lt;/p&gt;

&lt;p&gt;2020-10-27, 15:50: Add TP 3-4.&lt;/p&gt;

&lt;p&gt;2020-10-28, 09:27: Fix deadline of TP3-4.&lt;/p&gt;

&lt;p&gt;2020-11-03, 21:14: Ajout d&amp;rsquo;un lien zoom pour le prochain tp du 4 Nov.&lt;/p&gt;

&lt;p&gt;2020-11-03, 14:25: Ajout d&amp;rsquo;une cheatsheet gradient.&lt;/p&gt;

&lt;p&gt;2020-11-16, 12:42: Ajout du TP5-6 conv.&lt;/p&gt;

&lt;p&gt;2020-11-26, 10:28: Mise à jour de la date de rendu du second TP + précision sur les bonus.&lt;/p&gt;

&lt;p&gt;2020-12-08, 11:11: Ajout du TP 7 transfer.&lt;/p&gt;

&lt;p&gt;2020-12-15, 16:36: Ajout des TPs 8 - 9 - 10.&lt;/p&gt;

&lt;p&gt;2021-01-07, 13:46: Ajout du lien vers le cours du 6 Janvier, fix des urls pour celeba.&lt;/p&gt;

&lt;p&gt;2021-01-25, 20:50: Ajout du lien vers l&amp;rsquo;enregistrement et les slides du TP 10.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Insights from the Future for Continual Learning</title>
      <link>/publication/ghost/</link>
      <pubDate>Sat, 01 Aug 2020 00:00:00 +0200</pubDate>
      
      <guid>/publication/ghost/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.13513&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/2004.13513&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Review of Continual Learning at CVPR&#39;20</title>
      <link>/post/cvpr20-continual/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0200</pubDate>
      
      <guid>/post/cvpr20-continual/</guid>
      <description>

&lt;p&gt;I review in this article, papers published at CVPR 2020 about Continual Learning. If you think I made a mistake or miss an important paper, please tell me!&lt;/p&gt;

&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#1-few-shots&#34;&gt;1. Few-Shots&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#1-1-few-shot-class-incremental-learning&#34;&gt;1.1. Few-Shot Class-Incremental Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#1-2-incremental-few-shot-object-detection&#34;&gt;1.2. Incremental Few-Shot Object Detection&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#2-conditional-channel-gated-networks-for-task-aware-continual-learning&#34;&gt;2. Conditional Channel Gated Networks for Task-Aware Continual Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#3-incremental-learning-in-online-scenario&#34;&gt;3. Incremental Learning in Online Scenario&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#4-itaml-an-incremental-task-agnostic-meta-learning&#34;&gt;4. iTAML: An Incremental Task-Agnostic Meta-learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#5-modeling-the-background-for-incremental-learning&#34;&gt;5. Modeling the Background for Incremental Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#6-adinet-attribute-drive-incremental-network-for-retina-image-classification&#34;&gt;6. ADINET: Attribute Drive Incremental Network for Retina Image Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#7-semantic-drift-compensation-for-class-incremental-learning&#34;&gt;7. Semantic Drift Compensation for Class-Incremental Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#8-maintaining-discrimination-and-fairness-in-class-incremental-learning&#34;&gt;8. Maintaining Discrimination and Fairness in Class Incremental Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#9-mnemonics-training-multi-class-incremental-learning-without-forgetting&#34;&gt;9. Mnemonics Training: Multi-Class Incremental Learning without Forgetting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#10-towards-backward-compatible-representation-learning&#34;&gt;10. Towards Backward-Compatible Representation Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/nav&gt;


&lt;h2 id=&#34;1-few-shots&#34;&gt;1. Few-Shots&lt;/h2&gt;

&lt;p&gt;Many papers this year in Continual Learning were about few-shot learning. Besides the CVPR papers I&amp;rsquo;ll present, there is also a workshop paper (&lt;em&gt;Cognitively-Inspired Model for Incremental Learning Using a Few Examples, Ayub et al. CVPR Workshop 2020&lt;/em&gt;) and an arXiv (&lt;em&gt;Defining Benchmarks for Continual Few-Shot Learning, Antoniou et al. arxiv:2004.11967&lt;/em&gt;).&lt;/p&gt;

&lt;h3 id=&#34;1-1-few-shot-class-incremental-learning&#34;&gt;1.1. Few-Shot Class-Incremental Learning&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2004.10956&#34; target=&#34;_blank&#34;&gt;2004.10956&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, Yihong Gong&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Tao et al.&lt;/em&gt; proposes in this paper a mix between Few-Shot and Continual Learning. They benchmark their model &amp;mdash;TOPIC&amp;ndash; on the CIFAR100, miniImageNet, and CUB200. The first task is large (60 classes for CIFAR100), then the following tasks have few classes (5 &amp;lsquo;n-way&amp;rsquo;) and few training samples per class (5 &amp;lsquo;k-shots&amp;rsquo;).&lt;/p&gt;

&lt;p&gt;The author uses &lt;strong&gt;Neural Gas&lt;/strong&gt; (&lt;em&gt;Martinetz et al. 1991&lt;/em&gt;) and &lt;strong&gt;Competitive Hebbian Learning&lt;/strong&gt; (&lt;em&gt;Martinetz, 1993&lt;/em&gt;). This neural network seems similar to the Self-Organizing Maps.&lt;/p&gt;

&lt;p&gt;The Neural Gas is an undirected graph, where each node $j$, is defined as $(m_j, \Lambda_j, z_j, c_j)$:
- $m_j$ is a centroid vector, similar to what we can expect after a K-Means (kNN for CL)
- $\Lambda_j$ is the variance matrix of each dimension of the vector
- $z_j$ and $c_j$ are respectively the assigned images and labels&lt;/p&gt;

&lt;p&gt;The graph is created after the first task, once the features extractor has been trained. They sample 400 features randomly among the training samples and use them as initial centroids.&lt;/p&gt;

&lt;p&gt;Nodes are updated by some kind of moving average (Eq.3 of the paper), where all centroids go towards the incoming sample. The move in the latent space is weighted by a learning rate and more importantly by the L2 distance rank: close features will affect more the centroid than far features.&lt;/p&gt;

&lt;p&gt;To create the edges between nodes, they use Competitive Hebbian Learning. Hebbian Learning in Neuroscience stipulates that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Neurons that fire together, wire together&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this case, if two nodes are respectively the closest and second closest nodes to an input features, an edge is created. The edge has an age that is incremented each time no &amp;ldquo;firing together&amp;rdquo; happened. Past an age threshold, the edge is removed.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181113.png&#34; alt=&#34;image 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The model is trained with a usual softmax+cross-entropy but the competitive Hebbian learning is done after each step. Note that the latter doesn&amp;rsquo;t produce gradients for backpropagation.&lt;/p&gt;

&lt;p&gt;Finally, the inference is not really explained but I guess that each node has a &amp;ldquo;label&amp;rdquo; by voting which training sample labels were most associated with. Given a test input sample, its label is determined by the label of its closest node.&lt;/p&gt;

&lt;p&gt;They &lt;strong&gt;stabilize&lt;/strong&gt; the training by constraining the centroids to stay close to their previous position with a loss they called &amp;ldquo;anchor loss&amp;rdquo;. It&amp;rsquo;s actually a Mahalanobis loss, where the distance is weighted per dimension with the inverse of the variance (i.e. precision).&lt;/p&gt;

&lt;p&gt;They also add a &lt;strong&gt;regularization&lt;/strong&gt; called &amp;ldquo;min-max loss&amp;rdquo; to separate new centroids (added with new tasks) from previous centroids. It is similar to the hinge loss used by &lt;em&gt;Hou et al. 2019&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181114.png&#34; alt=&#34;image 2&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;1-2-incremental-few-shot-object-detection&#34;&gt;1.2. Incremental Few-Shot Object Detection&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2003.04668&#34; target=&#34;_blank&#34;&gt;2003.04668&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Juan-Manuel Perez-Rua, Xiatian Zhu, Timothy Hospedales, Tao Xiang&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Perez-Rua et al.&lt;/em&gt; propose to use together three settings: Continual Learning, Object Detection (aka finding object boxes in an image), and Few-Shot (with few training samples).&lt;/p&gt;

&lt;p&gt;Their setting is made of a first large task, with many classes &amp;amp; data, then the following tasks add new classes with only 10 labeled examples per class (10 k-shots). This is impressive because object detection is harder than classification!&lt;/p&gt;

&lt;p&gt;During the first task, they train a &lt;strong&gt;CenterNet&lt;/strong&gt;. Similar to CornerNet, class-agnostic features are extracted by a ResNet then class-specific heatmaps are generated. The most active zones are chosen to be boxes&amp;rsquo; centers. Two additional heads regress the boxes&amp;rsquo; width and height.&lt;/p&gt;

&lt;p&gt;Once their CenterNet has been trained on the base classes, they train a Meta-Learning-based generator. This module must learn to produce the &amp;ldquo;class-codes&amp;rdquo;, aka the class-specific weights used by the detector. To do so, they train the generator on the base classes split into episodes. All weights are frozen except the generator&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181359.png&#34; alt=&#34;image 3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;For the following tasks, there is no training. Given a new class, the meta-generator produces on-the-fly new weights, and the inference is immediately done.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s worth noting that the results on novel classes are quite low:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181426.png&#34; alt=&#34;image 4&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;2-conditional-channel-gated-networks-for-task-aware-continual-learning&#34;&gt;2. Conditional Channel Gated Networks for Task-Aware Continual Learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2004.00070&#34; target=&#34;_blank&#34;&gt;2004.00070&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, Babak Ehteshami Bejnordi&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Abati et al.&lt;/em&gt; propose an interesting view of sub-networks in Continual Learning. Previous methods proposed to learn sub-networks inside a unique network (think Lottery Ticket Hypothesis (&lt;a href=&#34;https://arxiv.org/abs/1803.03635&#34; target=&#34;_blank&#34;&gt;Frankle &amp;amp; Carbin, 2018&lt;/a&gt;)). Those sub-networks can be learned by an evolutionary algorithm (&lt;a href=&#34;https://arxiv.org/abs/1701.08734&#34; target=&#34;_blank&#34;&gt;Fernando et al., 2017&lt;/a&gt;), L1 sparsity (&lt;a href=&#34;https://arxiv.org/abs/1903.04476&#34; target=&#34;_blank&#34;&gt;Golkar et al., 2019&lt;/a&gt;), or learned gating (&lt;a href=&#34;https://arxiv.org/abs/1910.06562&#34; target=&#34;_blank&#34;&gt;Hung et al., 2019&lt;/a&gt;). However they all have a major constraint: they need the task id in inference to choose the right sub-networks, a setting called Multi-Head Evaluation. Having the task id in inference makes the problem much easier, and I think it is not realistic.&lt;/p&gt;

&lt;p&gt;The authors propose to train sub-networks with learned gating. The right sub-network will be chosen inference with a Task Classifier, therefore they don&amp;rsquo;t use the task id! To the best of my knowledge, they are only the second to do this (&lt;a href=&#34;https://openreview.net/forum?id=SJgwNerKvB&#34; target=&#34;_blank&#34;&gt;von Oswald et al. 2020&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Each residual block of their ResNet has $T$ (number of tasks) gate networks that choose the which filters to enable or disable:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181732.png&#34; alt=&#34;image 5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The selection of filters to pass or block is discrete and thus non-differentiable. Thus they use the Gumbel Softmax Sampling similarly to (&lt;a href=&#34;https://arxiv.org/abs/1811.08737&#34; target=&#34;_blank&#34;&gt;Guo et al. 2019&lt;/a&gt;). The forward pass is discrete, but the backward pass is continuous. After each task, they record on a validation set which gates have fired. Their associated filters will be frozen for the following tasks, but still usable!&lt;/p&gt;

&lt;p&gt;The interesting part of this paper is how they train the task classifier. During training, all gates are fired in parallel:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181738.png&#34; alt=&#34;image 6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This means that given a single input image, they have $T$ parallel stream of activations. All those streams&amp;rsquo; outputs are concatenated and fed to a task classifier (a two layers MLP) that classifies the task id. This task id will then be used to chose the right stream to give to the task-specific classifier.&lt;/p&gt;

&lt;p&gt;I really like this method, however, it&amp;rsquo;s unfortunate that they don&amp;rsquo;t compare their model to the lastest SotA and on large-scale datasets. Their largest dataset is ImageNet-50. Furthermore, I would like to see which gates fire the most, early layers or later layers?&amp;hellip;&lt;/p&gt;

&lt;h2 id=&#34;3-incremental-learning-in-online-scenario&#34;&gt;3. Incremental Learning in Online Scenario&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2003.13191&#34; target=&#34;_blank&#34;&gt;2003.13191&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jiangpeng He, Runyu Mao, Zeman Shao, Fengqing Zhu&lt;/p&gt;

&lt;p&gt;&lt;em&gt;He et al.&lt;/em&gt; claim to learn in an &amp;ldquo;Online Scenario&amp;rdquo;: new classes are added as usual, with also old classes new samples. This sounds similar to the &lt;strong&gt;New Instances and Classes&lt;/strong&gt; (NIC) (&lt;a href=&#34;http://proceedings.mlr.press/v78/lomonaco17a.html&#34; target=&#34;_blank&#34;&gt;Lomonaco et al., 2017&lt;/a&gt;). They claim novelty while it&amp;rsquo;s not really true (on top of my head, Aljundi et al. and Lomonaco et al. have worked on this).&lt;/p&gt;

&lt;p&gt;The authors propose two contributions: first of all they use at first a &lt;strong&gt;Nereast Class Mean&lt;/strong&gt; (NCM) (kNN for CL) classifier. Similar to iCaRL&amp;rsquo;s NME, class means are computed. To handle concept drift, they update the means for new samples with a moving average:&lt;/p&gt;

&lt;p&gt;$$\mu_{y}^{\phi} \leftarrow \frac{n_{y i}}{n_{y i}+1} \mu_{y}^{\phi}+\frac{1}{n_{y i}+1} \phi\left(\mathbf{x}_{i}\right)$$&lt;/p&gt;

&lt;p&gt;Furthermore, during the early tasks, they use in inference their NCM classifier because they remark that it behaves well with data scarcity. When the model is better trained, having seen enough samples, they switch their classifier for the class probabilities from a softmax. It&amp;rsquo;s interesting to mix those inference classifiers (classification and metric-based) contrary to &lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2019/papers/Hou_Learning_a_Unified_Classifier_Incrementally_via_Rebalancing_CVPR_2019_paper.pdf&#34; target=&#34;_blank&#34;&gt;Hou et al., 2019&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2004.13513&#34; target=&#34;_blank&#34;&gt;Douillard et al., 2020&lt;/a&gt; that evaluated separately the two methods. However &lt;em&gt;He et al.&lt;/em&gt;&amp;rsquo;s switch from one method to the other is an hyperparameter, it would have been nice to do so based on some Uncertainty measure.&lt;/p&gt;

&lt;p&gt;Their second contribution is a &lt;strong&gt;modified cross-distillation&lt;/strong&gt;: in addition of an usual distillation loss on probabilities with temperature (&lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34; target=&#34;_blank&#34;&gt;Hinton et al., 2015&lt;/a&gt;), they modify the classification loss. It is still a cross-entropy but the probabilities associated to old classes are a linear combination of the old and new model outputs:&lt;/p&gt;

&lt;p&gt;$$\tilde{p}^{(i)}=\left\{\begin{array}{cc}\beta p^{(i)}+(1-\beta) \hat{p}^{(i)} &amp;amp; 0 \lt i \leq n \\\ p^{(i)} &amp;amp; n \lt i \leq n+m\end{array}\right.$$&lt;/p&gt;

&lt;p&gt;They also finetune their model on a balanced set like &lt;a href=&#34;https://arxiv.org/abs/1807.09536&#34; target=&#34;_blank&#34;&gt;Castro et al., 2018&lt;/a&gt; did.&lt;/p&gt;

&lt;p&gt;Finally, they evaluate their model on CIFAR100, ImageNet100, and Food-101. Unfortunately, they evaluate their &amp;ldquo;Online&amp;rdquo; NIC setting only on Food-101 and solely compare against a simple baseline, not any SotA models (adapted to the setting). CIFAR100 and ImageNet100 are evaluated in the classic NC setting. They have slightly better performance than SotA on the former and are equivalent to BiC on the latter. I&amp;rsquo;m quite annoyed that they claimed in the abstract to &amp;ldquo;out-perform&amp;rdquo; SotA on ImageNet1000 while they actually only evaluate on the smaller-scale ImageNet100. BiC really shines on ImageNet1000, I&amp;rsquo;d have liked to see how their model fares in this harsher dataset.&lt;/p&gt;

&lt;h2 id=&#34;4-itaml-an-incremental-task-agnostic-meta-learning&#34;&gt;4. iTAML: An Incremental Task-Agnostic Meta-learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2003.11652&#34; target=&#34;_blank&#34;&gt;2003.11652&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Rajasegaran et al.&lt;/em&gt; propose to a novel Meta-Learning model. Several approaches exist using this branch of methods for Continual Learning, they are however the first to the best of my knowledge to do so without task id in inference.&lt;/p&gt;

&lt;p&gt;The goal of Meta-Learning is to &amp;ldquo;learn to learn&amp;rdquo;. In this case, they want to obtain a network that learns to produce a set of parameters that can then be tuned quickly to a particular task. Following existing Meta-Learning models, they train in two loops. The inner loop learns an actual task, while the outer loop learns to produce a good initialization for the inner loop (i.e. &amp;ldquo;learn to learn&amp;rdquo;). Inspired by Reptile (&lt;a href=&#34;https://arxiv.org/abs/1803.02999&#34; target=&#34;_blank&#34;&gt;Nichol et al., 2018&lt;/a&gt;), their outer loop is trained on the difference between the base parameters and the parameters learned after the inner loop. The different with Reptile is that the inner loop learns separately each task, producing mean inner loop parameters:&lt;/p&gt;

&lt;p&gt;$$\Phi=\Phi_{b a s e}-\eta \frac{1}{t} \sum_{i=1}^{t}\left(\Phi_{b a s e}-\Phi_{i}\right)=\eta \frac{1}{t} \sum_{i=1}^{t} \Phi_{i}+(1-\eta) \Phi_{b a s e}$$&lt;/p&gt;

&lt;p&gt;This forces the meta-model to find a set of parameters good for all tasks:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006191653.png&#34; alt=&#34;image 7&#34; /&gt;&lt;/p&gt;

&lt;p&gt;During inference, they proceed in two steps: finding the task id, and then tuning the model for this task. To find the task id, they record for the test subset which task predictions are the most activated in average, and choose the maximum value:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006191656.png&#34; alt=&#34;image 8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Then, given the predicted task id, they sample all exemplars memory (aka Rehearsal Learning) belonging to this task and learn in a single inner loop task-specific parameters. The resulting model is then used to classify the test samples.&lt;/p&gt;

&lt;p&gt;Note that while it&amp;rsquo;s a very interesting approach, their model stands on the quality of the task classification. They claim it&amp;rsquo;s almost 100% accuracy but it only for a reason: in their setting, the model is evaluated on each seen task separately. In a real setting, samples from different tasks are mixed together. There their algorithm 2 won&amp;rsquo;t work. Therefore I don&amp;rsquo;t think this model is truly &amp;ldquo;task-agnostic&amp;rdquo; but it&amp;rsquo;s definitely a good step forward.&lt;/p&gt;

&lt;p&gt;They evaluate various models, from meta-learning to continual learning domains, on MNIST, SVHN, CIFAR100, ImageNet100, ImageNet1000, and Celeb-10k. It&amp;rsquo;s a bit strange however that BiC (a very good alternative for large scale datasets) is evaluated on Celeb-10k but not ImageNet100 (where it would have beaten iTAML).&lt;/p&gt;

&lt;h2 id=&#34;5-modeling-the-background-for-incremental-learning&#34;&gt;5. Modeling the Background for Incremental Learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2002.00718&#34; target=&#34;_blank&#34;&gt;2002.00718&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulò, Elisa Ricci, Barbara Caputo&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Cermelli et al.&lt;/em&gt; attacks the problem of Semantic Segmentation for Continual Learning. In semantic segmentation, the goal is to give a class to each pixel. Two cars next to each other will have the same pixels labels. This is particularly difficult in Continual Learning for the same reasons as Object Detection: at task $t$, the class &amp;ldquo;car&amp;rdquo; may be a background, but then at task $t+1$ we have to predict it. However, our model may have seen images containing cars in the first task and thus has learned to not detect those. Likewise, images from task $t+1$ may be annotated so that the &amp;ldquo;person&amp;rdquo; learned previously is now part of the background.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006211828.png&#34; alt=&#34;image 9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To solve the problem mentionned before, they revisit both the cross-entropy and distillation losses. The former is split in two part: if the pixel probability belongs to the current task&amp;rsquo;s set of classes it is kept unchanged. Otherwise, it is the probability of having &lt;strong&gt;either an old class or the background&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$\begin{array}{l}\qquad \ell_{c e}^{\theta^{t}}(x, y)=-\frac{1}{|\mathcal{I}|} \sum_{i \in \mathcal{I}} \log \tilde{q}_{x}^{t}\left(i, y_{i}\right) \\\ \text { where: } \\\ \qquad \tilde{q}_{x}^{t}(i, c)=\left\{\begin{array}{ll}q_{x}^{t}(i, c) &amp;amp; \text { if } c \neq \mathrm{b} \\\ \sum_{k \in \mathcal{Y}^{t-1}} q_{x}^{t}(i, k) &amp;amp; \text { if } c=\mathrm{b}\end{array}\right.\end{array}$$&lt;/p&gt;

&lt;p&gt;Likewise, the distillation loss is changed if the pixel belongs to the background, according to the current task, to the probability of having &lt;strong&gt;either a new class or the background&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;$$\ell_{k d}^{\theta^{t}}(x, y)=-\frac{1}{|\mathcal{I}|} \sum_{i \in \mathcal{I}} \sum_{c \in \mathcal{V} t-1} q_{x}^{t-1}(i, c) \log \hat{q}_{x}^{t}(i, c)$$&lt;/p&gt;

&lt;p&gt;$$\hat{q}_{x}^{t}(i, c)=\left\{\begin{array}{ll}q_{x}^{t}(i, c) &amp;amp; \text { if } c \neq \mathrm{b} \\\ \sum_{k \in \mathcal{C}^{t}} q_{x}^{t}(i, k) &amp;amp; \text { if } c=\mathrm{b}\end{array}\right.$$&lt;/p&gt;

&lt;p&gt;This handles the case where the previous model considers a current pixel as background while the current model considers it as a new class.&lt;/p&gt;

&lt;p&gt;Finally, the classifier weights for new classes are initialized with the background weight.&lt;/p&gt;

&lt;h2 id=&#34;6-adinet-attribute-drive-incremental-network-for-retina-image-classification&#34;&gt;6. ADINET: Attribute Drive Incremental Network for Retina Image Classification&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;http://openaccess.thecvf.com/content_CVPR_2020/papers/Meng_ADINet_Attribute_Driven_Incremental_Network_for_Retinal_Image_Classification_CVPR_2020_paper.pdf&#34; target=&#34;_blank&#34;&gt;CVPR webpage&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Qier Meng, Satoh Shin&amp;rsquo;ichi&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Meng and Shin&amp;rsquo;ichi&lt;/em&gt; use Continual Learning for retinal images classification. They found that retinal diseases have a large variety of types and that current methods didn&amp;rsquo;t allow them to train incrementally each type a new patient came in.&lt;/p&gt;

&lt;p&gt;Their originality lies in their usage of attributes. Retinal images have been annotated with a disease label (&amp;ldquo;AMD&amp;rdquo;, &amp;ldquo;DR&amp;rdquo;&amp;hellip;) but also with several attributes (&amp;ldquo;hermorrhage&amp;rdquo;, &amp;ldquo;macular edema&amp;rdquo;&amp;hellip;). Therefore in addition to the classic distillation loss with temperature scaling applied to the disease prediction, they also distill the attributes prediction of the previous model with a BCE.&lt;/p&gt;

&lt;p&gt;In addition to the two distillation losses, they also refine their attributes prediction with a &amp;ldquo;weight estimation&amp;rdquo;. It measures how much of a contribution an attribute has to distinguish classes. It&amp;rsquo;s similar to doing a self-attention on all attributes to find which one is the most important. This weight estimation is then used to ponder the attribute predictions. They didn&amp;rsquo;t detail much the rationale behind this weight estimation but empirical results show small but consistent gains.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006232230.png&#34; alt=&#34;image 9 bis&#34; /&gt;&lt;/p&gt;

&lt;p&gt;They evaluate both medical and academic datasets. For the later they used ImageNet-150k-sub: it contains 100 classes of ImageNet1000, but only 150 training images were selected per class instead of ~1200. I&amp;rsquo;ve never seen a model evaluated on this dataset, but it looks like a more challenging dataset than ImageNet100. They display a significant improvement over the 2017&amp;rsquo;s iCaRL.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s interesting to predict attributes in the context of Continual Learning. I hypothesize that it forces the model to learn fine-grained features common to all tasks and may reduce catastrophic forgetting.&lt;/p&gt;

&lt;h2 id=&#34;7-semantic-drift-compensation-for-class-incremental-learning&#34;&gt;7. Semantic Drift Compensation for Class-Incremental Learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2004.00440&#34; target=&#34;_blank&#34;&gt;2004.00440&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Lu Yu, Bartłomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, Joost van de Weijer&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Rebuffi et al., 2017&lt;/em&gt; uses class means with a k-NN to classify samples in inference. Those class means are updated after each task by re-extracting features of rehearsal samples of old classes with the new ConvNet. This supposes that we have access to previous data, at least in a limited amount.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Yu et al.&lt;/em&gt; propose to update the class means without even using previous data. First, they compute the embedding drift between the start and the end of the current task on current data:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006221421.png&#34; alt=&#34;image 10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;$$\boldsymbol{\delta}_{i}^{t-1 \rightarrow t}=\mathbf{z}_{i}^{t}-\mathbf{z}_{i}^{t-1}, y_{i} \in C^{t}$$&lt;/p&gt;

&lt;p&gt;Then for each old classes, they compute the mean vector of drift:&lt;/p&gt;

&lt;p&gt;$$\begin{array}{c}\hat{\Delta}_{c^{s}}^{t-1 \rightarrow t}=\frac{\sum_{i}\left[y_{i} \in C^{t}\right] w_{i} \delta_{i}^{t-1 \rightarrow t}}{\sum_{i}\left[y_{i} \in C^{t}\right] w_{i}} \ w_{i}=e^{-\frac{\left|x_{i}^{t-1}-\mu_{c^{s}}^{t-1}\right|^{2}}{2 \sigma^{2}}}\end{array}$$&lt;/p&gt;

&lt;p&gt;The drift is weighted by $w_i$, which gives a lower weight to outliers. Thus samples will low confidence won&amp;rsquo;t affect as much the drift computation than &amp;ldquo;archetypal&amp;rdquo; samples.&lt;/p&gt;

&lt;p&gt;Finally this drift is computed at after each task, starting from the second one, and is added continuously to the class mean vectors:&lt;/p&gt;

&lt;p&gt;$$\hat{\mu}_{c^{s}}^{t}=\mu_{c^{s}}^{s}+\hat{\Delta}_{c^{s}}^{s \rightarrow s+1}+\ldots+\hat{\Delta}_{c^{s}}^{t-1 \rightarrow t}$$&lt;/p&gt;

&lt;p&gt;They add their method, nicknamed SDC, to the model EWC. They show on CIFAR100 and ImageNet100 excellent performance only beaten by &lt;em&gt;Hou et al., 2019&lt;/em&gt;. It&amp;rsquo;s important to note that &lt;em&gt;Hou et al., 2019&lt;/em&gt; use exemplars while &lt;em&gt;Yu et al.&lt;/em&gt; don&amp;rsquo;t. On the other hand, according to their &lt;a href=&#34;https://github.com/yulu0724/SDC-IL/blob/master/test.py#L231&#34; target=&#34;_blank&#34;&gt;code&lt;/a&gt; they are in a Multi-Head Evaluation setting where they know the task id during inference. Thus they classify a sample among the task classes instead of all seen classes as do &lt;em&gt;Rebuffi et al., 2017&lt;/em&gt; or &lt;em&gt;Hou et al., 2019&lt;/em&gt;. Their setting is not really comparable to &lt;em&gt;Hou et al., 2019&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;8-maintaining-discrimination-and-fairness-in-class-incremental-learning&#34;&gt;8. Maintaining Discrimination and Fairness in Class Incremental Learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/1911.07053&#34; target=&#34;_blank&#34;&gt;1911.07053&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, Shutao Xia&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Zhao et al.&lt;/em&gt; propose a method directly in the line of &lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Belouadah and Popescu, 2019&lt;/em&gt;&lt;/a&gt;) (IL2M) and &lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34; target=&#34;_blank&#34;&gt;&lt;em&gt;Wu et al., 2019&lt;/em&gt;&lt;/a&gt;) (BiC). Both works remarked that a bias towards new classes and detrimental to old classes. IL2M uses some statistics to correct this bias, while BiC recalibrates (Recalibration) the probabilities using a linear model learned on validation data.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Zhao et al.&lt;/em&gt; use a simpler solution that they call &lt;strong&gt;Weight Aligning&lt;/strong&gt; (WA). They saw, as &lt;em&gt;Hou et al., 2019&lt;/em&gt;, that the norm of the weights associated with old classes is lower than those associated with new classes:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006221420.png&#34; alt=&#34;image 11&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Hou et al., 2019&lt;/em&gt; and &lt;em&gt;Douillard et al., 2020&lt;/em&gt; use a cosine classifier so that all norms are equal to 1. &lt;em&gt;Zhao et al.&lt;/em&gt; instead re-normalize the weights based on the norm ratio:&lt;/p&gt;

&lt;p&gt;$$\begin{array}{c}\widehat{\mathbf{W}}_{n e w}=\gamma \cdot \mathbf{W}_{n e w} \\\ \text { where } \\\ \gamma=\frac{M e a n\left(N o r m_{o l d}\right)}{M e a n\left(N o r m_{n e w}\right)}\end{array}$$&lt;/p&gt;

&lt;p&gt;They remark that:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;[&amp;hellip;] we only make the average norms become equal, in other words, within new classes (or old classes), the relative magnitude of the norms of the weight vectors does not change. Such a design is mainly used to ensure the data within new classes (or old classes) can be separated well.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This weight alignment is done between each task. Furthermore, they clip after each optimization step the value of the weights to be positive to make the weights norm more consistent with their corresponding logits (after ReLU).&lt;/p&gt;

&lt;p&gt;In their ablation, they show that weight alignment provides more gain than knowledge distillation which is quite impressive. Using both lead them to a significant gain over BiC and IL2M on large scale datasets like ImageNet1000.&lt;/p&gt;

&lt;p&gt;Overall I liked their method as it is very simple and yet efficient.&lt;/p&gt;

&lt;h2 id=&#34;9-mnemonics-training-multi-class-incremental-learning-without-forgetting&#34;&gt;9. Mnemonics Training: Multi-Class Incremental Learning without Forgetting&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2002.10211&#34; target=&#34;_blank&#34;&gt;2002.10211&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yaoyao Liu, An-An Liu, Yuting Su, Bernt Schiele, Qianru Sun&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Liu et al.&lt;/em&gt; propose in this work two important contributions which make it my favorite paper in this review. The first, advertised, is an improvement of Rehearsal Learning. The second, a little hidden in the paper, is Meta-Learning-inspired method to adapt gracefully to new distribution.&lt;/p&gt;

&lt;p&gt;In rehearsal learning, we feed old samples to the model to reduce forgetting. Obviously, we won&amp;rsquo;t use all old samples, but a very limited amount. Here the authors use 20 images per class, like &lt;em&gt;Hou et al., 2019&lt;/em&gt; and &lt;em&gt;Douillard et al., 2020&lt;/em&gt;. &lt;em&gt;Rebuffi et al., 2017&lt;/em&gt; proposed with iCaRL to a &lt;strong&gt;herding&lt;/strong&gt; selection which finds iteratively the barycenter of the class distribution. However &lt;em&gt;Castro et al., 2018&lt;/em&gt; remarked that taking the closest samples to the class mean, or even random samples (!), worked as well.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Liu et al.&lt;/em&gt; significantly improve those solutions by transforming the selected exemplars. First, they randomly select samples, then given a trained &amp;amp; fixed model, they optimize the exemplars as the pixel-level:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006221447.png&#34; alt=&#34;image 12&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The optimized exemplars must lead to a decreased loss on the new classes data (present in large amounts).&lt;/p&gt;

&lt;p&gt;The modification is very minor visually: we only see a little bit of noise overlayed on the original images. The authors found that this optimization of the images leads to a set of exemplars well distributed on the class boundaries:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006221545.png&#34; alt=&#34;image 13&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This optimization is done at the task end, once exemplars from new classes have been selected. The authors also choose to finetune the exemplars of old classes that have been selected in previous tasks. However, in this case, we don&amp;rsquo;t have anymore a large amount of old classes&amp;rsquo; data to act as ground truth for the old classes exemplars optimization. Therefore, they split the exemplars set in half. One split is optimized using the second for ground truth and vice-versa.&lt;/p&gt;

&lt;p&gt;The second contribution, and the major one, is unfortunately not very advertised in this paper. The authors re-use an idea from one of their previous papers in Meta-Learning. Instead of tuning all the ConvNet parameters for each task, they only slightly adapt them:  for each kernel, a small kernel of spatial dimensions equal to one is learned (likewise for the biases). This small kernel is expanded to the base kernel dimension and element-wise multiplied to it:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006221558.png&#34; alt=&#34;image 14&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Intuitively, this method called &amp;ldquo;Meta-Transfer&amp;rdquo; does a small &amp;ldquo;shift &amp;amp; scaling&amp;rdquo;. Most of the network is kept frozen and thus don&amp;rsquo;t forget too much. The small adaptation enables the network to learn new classes.&lt;/p&gt;

&lt;p&gt;They evaluated CIFAR100, ImageNet100, and ImageNet1000 in various settings and beat all previous SotA (especially &lt;em&gt;Hou et al., 2019&lt;/em&gt; and &lt;em&gt;Wu et al., 2019&lt;/em&gt;). In my recent paper (&lt;a href=&#34;https://arxiv.org/abs/2004.13513&#34; target=&#34;_blank&#34;&gt;Douillard et al., 2020&lt;/a&gt;), our model PODNet outperforms their&amp;rsquo;s in several settings and evaluate even on even longer incremental training.&lt;/p&gt;

&lt;h2 id=&#34;10-towards-backward-compatible-representation-learning&#34;&gt;10. Towards Backward-Compatible Representation Learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;PDF&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2003.11942&#34; target=&#34;_blank&#34;&gt;2003.11942&lt;/a&gt;&lt;br /&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yantao Shen, Yuanjun Xiong, Wei Xia, Stefano Soatto&lt;/p&gt;

&lt;p&gt;This paper is not directly related to Continual Learning but rather to Visual Search.  &lt;em&gt;Shen et al.&lt;/em&gt; raise the issue of &lt;strong&gt;backfilling&lt;/strong&gt;:&lt;/p&gt;

&lt;p&gt;On a visual search model, embeddings of a large gallery have been computed once. Then given a query image,  we extract its features and compare them to the gallery features collection. For example, I take a picture of a shirt and want to know what are the most similar clothes available in a store.&lt;/p&gt;

&lt;p&gt;A problem arises when a new model is trained. This model may be different from the previous one because the data is was trained on or because the architecture and losses were changed. The gallery features collection is not up-to-date anymore and we need to extract gain the features of the whole collection with the new model. This can be very costly when the gallery is made of billions of images.&lt;/p&gt;

&lt;p&gt;The authors propose to make the features extractor &amp;ldquo;backward-compatible&amp;rdquo;. It&amp;rsquo;s mean that query features extracted by the new model are in the same latent space of the old model:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181635.png&#34; alt=&#34;image 15&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To produce a new model backward-compatible with a previous model that may be different, the authors add a loss over the classification loss:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;202006181639.png&#34; alt=&#34;image 16&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The first part of the loss update the new model on new $t$ dataset $T_{\text{new}}$. For the second part, the best alternative proposed is training the old classifier $w_{c\,\text{old}}$ with the new features extractor $w_\theta$ on the new dataset $T_\text{new} = T_\text{BCT}$. Because the new dataset can contain new classes, the old classifier is extended with new weights. They are initialized with the mean features extracted by $w_{\theta\, \text{old}}$ like Weight Imprinting did in Metric-Learning (&lt;a href=&#34;https://arxiv.org/abs/1712.07136&#34; target=&#34;_blank&#34;&gt;Qi et al., 2018&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Overall, their model is still far from the upper-bound (recomputing the gallery with the new model) but they improve significantly over simple baselines and beat LwF (&lt;a href=&#34;https://arxiv.org/abs/1606.09282&#34; target=&#34;_blank&#34;&gt;Li &amp;amp; Hoiem, 2016&lt;/a&gt;) by 3 points. I think this model is quite &amp;ldquo;simple&amp;rdquo; compared to SotA Continual Learning but it is interesting to see actual applications of the domain.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Continuum</title>
      <link>/project/continuum/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0200</pubDate>
      
      <guid>/project/continuum/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Inclearn</title>
      <link>/project/inclearn/</link>
      <pubDate>Mon, 27 Apr 2020 00:00:00 +0200</pubDate>
      
      <guid>/project/inclearn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PODNet: Pooled Outputs Distillation for Small-Tasks Incremental Learning</title>
      <link>/publication/podnet/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0100</pubDate>
      
      <guid>/publication/podnet/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2004.13513&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/abs/2004.13513&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Deep Neural Networks incrementally forever</title>
      <link>/post/incremental-learning/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0100</pubDate>
      
      <guid>/post/incremental-learning/</guid>
      <description>

&lt;p&gt;The hallmark of human intelligence is the capacity to learn. A toddler has comparable
aptitudes to reason about space, quantities, or causality than other ape species (&lt;a href=&#34;https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;). The difference of our cousins and us is the ability to learn from others.&lt;/p&gt;

&lt;p&gt;The recent deep learning hype aims to reach the Artificial General Intelligence (AGI):
an AI that would express (supra-)human-like intelligence. Unfortunately current deep learning models are flawed in many ways: one of them is that they are unable to learn
continuously as human does through years of schooling, and so on.&lt;/p&gt;

&lt;h2 id=&#34;why-do-we-want-our-models-to-learn-continuously&#34;&gt;Why do we want our models to learn continuously?&lt;/h2&gt;

&lt;p&gt;Regardless of the far away goal of AGI, there are several practicals reasons why
we want our model to learn continuously. Before describing a few of them, I&amp;rsquo;ll mention
two constraints:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Our model cannot review all previous knowledge each time it needs to
learn new facts. &lt;em&gt;As a child in 9th grade, you don&amp;rsquo;t review all the syllabus of 8th
grade as it&amp;rsquo;s supposed to have been already memorized.&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Our model needs to learn continuously without forgetting any previously learned knowledge.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A real applications of these two constraints is robotics: a robot in the wild should
learn continuously its environment. Furthermore due to hardware limitation, it
may neither store all previous data nor spend too much computational resource.&lt;/p&gt;

&lt;p&gt;Another application is what I do at &lt;a href=&#34;https://www.heuritech.com/&#34; target=&#34;_blank&#34;&gt;Heuritech&lt;/a&gt;: we
detect fashion trends. However every day across the globe a new trend may appear.
It is impracticable to review our large trends database each time we need to learn
a new one.&lt;/p&gt;

&lt;p&gt;Now that the necessity of learning continuously has been explained, let us differentiate three scenarios (&lt;a href=&#34;https://arxiv.org/abs/1705.03550&#34; target=&#34;_blank&#34;&gt;Lomonaco and Maltoni, 2017&lt;/a&gt;):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Learning new data of known classes (&lt;em&gt;online learning&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Learning new classes (&lt;em&gt;class-incremental learning&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;The union of the two previous scenarios&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this article I will focus only on the second scenario. Note however that the
methods used are fairly similar between scenario.&lt;/p&gt;

&lt;p&gt;More practically this article will cover models that learn incrementally new classes.
The model will see only new classes&amp;rsquo; data, as we aim to remember well old classes.
After each task, the model is trained on a all seen classes using a separate test set:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/incremental_base.jpg&#34; alt=&#34;*Figure 1: Several steps of incremental learning.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 1: Several steps of incremental learning.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As seen in the image above, each step produces a new accuracy score. Following
(&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34; target=&#34;_blank&#34;&gt;Rebuffi et al, 2017&lt;/a&gt;) the final score is the
average of all previous task accuracy score. It&amp;rsquo;s called the &lt;strong&gt;average incremental accuracy&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;naive-solution-transfer-learning&#34;&gt;Naive solution: transfer learning&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Transfer learning&lt;/strong&gt; allows to transfer the knowledge gained on one task (e.g
&lt;em&gt;ImageNet and its 1000 classes&lt;/em&gt;) to another task (e.g &lt;em&gt;classify cats &amp;amp; dogs&lt;/em&gt;) (&lt;a href=&#34;https://arxiv.org/abs/1403.6382&#34; target=&#34;_blank&#34;&gt;Razavian et al, 2014&lt;/a&gt;). Usually the backbone
(a ConvNet in Computer Vision, like ResNet) is kept while a new classifier is
plugged in on top of it. During transfer, we train the new classifier &amp;amp;
&lt;strong&gt;fine-tune&lt;/strong&gt; the backbone.&lt;/p&gt;

&lt;p&gt;Finetuning the backbone is essential to reach good performance on the destination
task. However we don&amp;rsquo;t have access anymore to the original task data. Therefore our
model is now optimized only for the new task. While at the training end, we will have good performance on this new task, the old task will suffer a significant drop of
performance.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S1364661399012942&#34; target=&#34;_blank&#34;&gt;French, 1995&lt;/a&gt;)
described this phenomenon as a &lt;strong&gt;catastrophic forgetting&lt;/strong&gt;. To solve it, we must find an optimal trade-off between
&lt;strong&gt;rigidity&lt;/strong&gt; (being good on old tasks) and &lt;strong&gt;plasticity&lt;/strong&gt; (being good on new tasks).&lt;/p&gt;

&lt;h2 id=&#34;three-broad-strategies&#34;&gt;Three broad strategies&lt;/h2&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1802.07569&#34; target=&#34;_blank&#34;&gt;Parisi et al, 2018&lt;/a&gt;) defines 3 broad strategies:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;External Memory&lt;/strong&gt; storing a small amount of previous tasks data&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Constraints&lt;/strong&gt;-based methods avoiding forgetting on previous tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model Plasticity&lt;/strong&gt; extending the capacity&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;1-external-memory&#34;&gt;1. External Memory&lt;/h4&gt;

&lt;p&gt;As said previously we cannot keep all our previous data for several reasons. We
can however relax this constraint by limiting access to previous data to a bounded
amount.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Rehearsal learning&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/1611.07725&#34; target=&#34;_blank&#34;&gt;Rebuffi et al, 2017&lt;/a&gt;)&amp;rsquo;s iCaRL
assumes we dispose of a limited amount of space to store previous data. Our
external memory could have a capacity of 2,000 images. After learning new
classes, a few amount of those classes data could be kept in it while
the rest would be discarded.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/incremental_memory.jpg&#34; alt=&#34;*Figure 2: Several steps of incremental learning with a memory storing a subset of previous data.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 2: Several steps of incremental learning with a memory storing a subset of previous data.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Pseudo-Rehearsal learning&lt;/strong&gt; (&lt;a href=&#34;https://arxiv.org/abs/1705.08690&#34; target=&#34;_blank&#34;&gt;Shin et al, 2017&lt;/a&gt;; &lt;a href=&#34;https://arxiv.org/abs/1711.10563&#34; target=&#34;_blank&#34;&gt;Kemker and Kanan, 2018&lt;/a&gt;)
assume instead that we cannot keep previous data, like images, but that we can
store the class distribution statistics. With this, a generative model can generate
on-the-fly old classes data. This approach is however very reliant on the quality
of the generative model; generated data are still subpar
to real data (&lt;a href=&#34;https://arxiv.org/abs/1905.10887&#34; target=&#34;_blank&#34;&gt;Ravuri and Vinyals, 2019&lt;/a&gt;).
Furthermore it is still crucial to also avoid a forgetting in the generator.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/incremental_gan.jpg&#34; alt=&#34;*Figure 3: Several steps of incremental learning with a generator generating previous data.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 3: Several steps of incremental learning with a generator generating previous data.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Generally (pseudo-)rehearsal-based methods outperforms methods only using new classes
data. It&amp;rsquo;s then fair to compare their performance separately.&lt;/p&gt;

&lt;h3 id=&#34;2-constraints-based-methods&#34;&gt;2. Constraints-based methods&lt;/h3&gt;

&lt;p&gt;Intuitively, forcing the current model $M^t$ to be similar to its previous version $M^{t-1}$
will avoid forgetting. There is a large array of methods aiming to do so. However
they all have to balance a &lt;strong&gt;rigidity&lt;/strong&gt; (encouraging similarity between
$M^t$ and $M^{t-1}$) and &lt;strong&gt;plasticity&lt;/strong&gt; (letting enough slack to $M^t$ to learn
new classes).&lt;/p&gt;

&lt;p&gt;We can separate those methods in three broads categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Those enforcing a similarity of the activations&lt;/li&gt;
&lt;li&gt;Those enforcing a similarity of the weights&lt;/li&gt;
&lt;li&gt;And those enforcing a similarity of the gradients&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&#34;2-1-constraining-the-activations&#34;&gt;2.1. Constraining the activations&lt;/h4&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1606.09282&#34; target=&#34;_blank&#34;&gt;Li and Hoiem, 2016&lt;/a&gt;)&amp;rsquo;s LwF introduced knowledge
distillation from (&lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34; target=&#34;_blank&#34;&gt;Hinton et al, 2015&lt;/a&gt;): given
a same image, $f^t$&amp;rsquo;s base probabilities should be similar to $f^{t-1}$&amp;rsquo;s probabilities:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/knowledge_distillation.jpg&#34; alt=&#34;*Figure 4: Base probabilities are distilled from the previous model to the new one.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 4: Base probabilities are distilled from the previous model to the new one.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;The distillation loss can simply be a binary cross-entropy between old and new
probabilities.&lt;/p&gt;

&lt;p&gt;Model output probabilities is just one kind of activation among others.
(&lt;a href=&#34;http://dahua.me/publications/dhl19_increclass.pdf&#34; target=&#34;_blank&#34;&gt;Hou et al, 2019&lt;/a&gt;)&amp;rsquo;s UCIR used a
similarity-based between the extracted features $h^{t-1}$ and $h^t$ of the old
and new model:&lt;/p&gt;

&lt;p&gt;$$L_\text{Less-Forget} = 1-\langle \frac{h^t}{\Vert h^t \Vert_2}, \frac{h^{t-1}}{\Vert h^{t-1} \Vert_2}\rangle$$&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/less_forget.jpg&#34; alt=&#34;*Figure 5: New model embeddings must be similar from the old one.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 5: New model embeddings must be similar from the old one.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;To sum up, encouraging the new model to mimic the activations of its previous
version reduces the forgetting of old classes. A different but similar approach
is reduce the difference between the new and old model weights:&lt;/p&gt;

&lt;h4 id=&#34;2-2-constraining-the-weights&#34;&gt;2.2. Constraining the weights&lt;/h4&gt;

&lt;p&gt;A naive method would be to minimize a distance between the new and old weights
likewise $L = (\mathbf{W}^t - \mathbf{W}^{t-1})^2$. However, as remarked by
(&lt;a href=&#34;https://arxiv.org/abs/1612.00796&#34; target=&#34;_blank&#34;&gt;Kirkpatrick et al, 2016&lt;/a&gt;)&amp;rsquo;s EWC, the resulting new
weights would be under-performing for both old and new classes. Then, the authors
suggested to modulate the regularization according to neurons importance.&lt;/p&gt;

&lt;p&gt;Important neurons for task $T-1$ must not change in the new model. On the other
hand, unimportant neurons can be more freely modified, to learn efficiently the new
task $T$:&lt;/p&gt;

&lt;p&gt;$$L = I (W^{t-1} - W^t)^2$$&lt;/p&gt;

&lt;p&gt;With $W^{t-1}$ and $W^{t}$ the weights of respectively the old and new model, and
$I$ a neurons importance matrix defined from $W^{t-1}$.&lt;/p&gt;

&lt;p&gt;In EWC, the neurons importance are defined with the Fisher information, but variants
exist. Following research (&lt;a href=&#34;https://arxiv.org/abs/1703.04200&#34; target=&#34;_blank&#34;&gt;Zenke et al, 2017&lt;/a&gt;;
&lt;a href=&#34;https://arxiv.org/abs/1801.10112&#34; target=&#34;_blank&#34;&gt;Chaudhry et al, 2018&lt;/a&gt;) builds on the same idea
with refinement of the neurons importance definition.&lt;/p&gt;

&lt;h4 id=&#34;2-3-constraining-the-gradients&#34;&gt;2.3. Constraining the gradients&lt;/h4&gt;

&lt;p&gt;Finally a third category of constraints exist: constraining the gradients. Introduced
by (&lt;a href=&#34;https://arxiv.org/abs/1706.08840&#34; target=&#34;_blank&#34;&gt;Lopez-Paz and Ranzato, 2017&lt;/a&gt;)&amp;rsquo;s GEM, the key idea
is that the the new model&amp;rsquo;s loss should be lower or equal to the old model&amp;rsquo;s loss
on old samples stored in a memory (&lt;em&gt;cf rehearsal learning&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;$$L(f^t, M) \le L(f^{t-1}, M)$$&lt;/p&gt;

&lt;p&gt;The authors rephrase this constraint as an angle constraint on the gradients:&lt;/p&gt;

&lt;p&gt;$$\langle \frac{\partial L(f^t, M)}{\partial f^t}, \frac{\partial L(f^{t-1}, M)}{\partial f^{t-1}} \rangle \ge 0$$&lt;/p&gt;

&lt;p&gt;Put it more simply, we want the gradients of the new model to &amp;ldquo;&lt;em&gt;go in the same
direction&lt;/em&gt;&amp;rdquo; as they would have with the previous model.&lt;/p&gt;

&lt;p&gt;If this constraint is respected, it&amp;rsquo;s likely that the new model won&amp;rsquo;t forget old
classes. Otherwise the incoming gradients $g$ must be &amp;ldquo;&lt;em&gt;fixed&lt;/em&gt;&amp;ldquo;: they are reprojected
to their closest valid alternative $\tilde{g}$ by minimizing this quadratic program:&lt;/p&gt;

&lt;p&gt;$$\text{minimize}_{\tilde{g}}\, \Vert g^t - \tilde{g} \Vert_2^2$$&lt;/p&gt;

&lt;p&gt;$$\text{subject to}\, \langle g^{t-1}, \tilde{g} \rangle \ge 0$$&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/gem.jpg&#34; alt=&#34;*Figure 6: Gradients must keep going in the same direction, otherwise their direction is fixed.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 6: Gradients must keep going in the same direction, otherwise their direction is fixed.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As you may guess, solving this program for each violating gradients, before
updating the model weights is very costly in time. (&lt;a href=&#34;https://arxiv.org/abs/1812.00420&#34; target=&#34;_blank&#34;&gt;Chaudhry et al, 2018&lt;/a&gt;
; &lt;a href=&#34;https://arxiv.org/abs/1903.08671&#34; target=&#34;_blank&#34;&gt;Aljundi et al, 2019&lt;/a&gt;) improve the algorithm
speed by different manners, including sampling a representative subset of the gradients
constraints.&lt;/p&gt;

&lt;h3 id=&#34;3-plasticity&#34;&gt;3. Plasticity&lt;/h3&gt;

&lt;p&gt;Other algorithms modify the network structure to reduce &lt;em&gt;catastrophic forgetting&lt;/em&gt;.
The first strategy is to add new neurons to the current model.
(&lt;a href=&#34;https://arxiv.org/abs/1708.01547&#34; target=&#34;_blank&#34;&gt;Yoon et al, 2017&lt;/a&gt;)&amp;rsquo;s DEN first trains on the
new task. If its loss is not good enough, new neurons are added at several
layers and they will be dedicated to learn on the new task. Furthermore the authors
choose to freeze some of the already-existing neurons. Those neurons, that are
particularly important for the old tasks, must not change in order to reduce forgetting.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/den.jpg&#34; alt=&#34;*Figure 7: DEN adds new neurons for the new tasks, and selectively fine-tunes existing neurons.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 7: DEN adds new neurons for the new tasks, and selectively fine-tunes existing neurons.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;While expanding the network capacity makes sense in an incremental setting where
our model learns indefinitely, it&amp;rsquo;s worth noting that existing deep learning models
are over-parametrized. The initial capacity can be enough to learn many tasks, at
the condition that it&amp;rsquo;s used appropriately. As (&lt;a href=&#34;https://arxiv.org/abs/1803.03635&#34; target=&#34;_blank&#34;&gt;Frankle and Carbin, 2019&lt;/a&gt;)&amp;rsquo;s
Lottery Ticket Hypothesis formalized, large networks are made of very efficient sub-networks.&lt;/p&gt;

&lt;p&gt;Each sub-network can be dedicated to only one task:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/subnetwork.jpg&#34; alt=&#34;*Figure 8: Among a large single network, several subnetworks can be uncovered, each specialized for a task.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 8: Among a large single network, several subnetworks can be uncovered, each specialized for a task.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Several methods exist to uncover those sub-networks: (&lt;a href=&#34;https://arxiv.org/abs/1701.08734&#34; target=&#34;_blank&#34;&gt;Fernando et al, 2017&lt;/a&gt;)&amp;rsquo;s
PathNet uses evolutionary algorithm, (&lt;a href=&#34;https://arxiv.org/abs/1903.04476&#34; target=&#34;_blank&#34;&gt;Golkar et al, 2019&lt;/a&gt;)
sparsify the whole network with a L1 regularization, and (&lt;a href=&#34;https://arxiv.org/abs/1910.06562&#34; target=&#34;_blank&#34;&gt;Hung et al, 2019&lt;/a&gt;)&amp;rsquo;s
CPG learns binary masks activating and deactivating connections to produce sub-networks.&lt;/p&gt;

&lt;p&gt;It is worth noting that methods based on sub-networks assume to know on which task
they are evaluated on. This setting, called &lt;strong&gt;multi-heads&lt;/strong&gt; is challenging but fundamentally
easier than &lt;strong&gt;single-head&lt;/strong&gt; evaluation where models are evaluated on all tasks
in the same time.&lt;/p&gt;

&lt;h2 id=&#34;dealing-with-class-imbalance&#34;&gt;Dealing with class imbalance&lt;/h2&gt;

&lt;p&gt;We saw previously three strategy to avoid forgetting (rehearsal, constraints,
and plasticity). Those methods can be used together. Rehearsal is often used in addition
of constraints.&lt;/p&gt;

&lt;p&gt;Moreover another challenge of incremental learning is the large class imbalance
between new and old classes. For example, on some benchmarks, new classes could
be made of 500 images each, while old classes would only have 20 images each stored
in memory.&lt;/p&gt;

&lt;p&gt;This class imbalance further encourages, wrongly, the model to be over-confident
for new classes while being under-confident for old classes. Catastrophic forgetting
is furthermore exacerbated.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1807.09536&#34; target=&#34;_blank&#34;&gt;Castro et al, 2018&lt;/a&gt;) train for each
task their model under this class imbalance, but fine-tune it after with under-sampling:
old &amp;amp; new classes are sampled to have as much images.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1905.13260&#34; target=&#34;_blank&#34;&gt;Wu et al, 2019&lt;/a&gt;) consider to use re-calibration
(&lt;a href=&#34;http://proceedings.mlr.press/v70/guo17a.html&#34; target=&#34;_blank&#34;&gt;Guo et al, 2017&lt;/a&gt;): a small linear
model is learned on validation to &amp;ldquo;&lt;em&gt;fix&lt;/em&gt;&amp;rdquo; the over-confidence on new classes. It
is only applied for new classes logits. (&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34;&gt;Belouadah and Popescu, 2019&lt;/a&gt;) proposed
concurrently a similar solution fixing the new classes logits, but using instead
class statistics.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;http://openaccess.thecvf.com/content_ICCV_2019/papers/Belouadah_IL2M_Class_Incremental_Learning_With_Dual_Memory_ICCV_2019_paper.pdf&#34; target=&#34;_blank&#34;&gt;Hou et al, 2019&lt;/a&gt;)
remarked that weights &amp;amp; biases of the classifier layer have larger magnitude for
new classes than older classes. To reduce this effect, they replace the usual classifier
by a cosine classifier where weights and features are L2 normalized. Moreover they
freeze the classifier weights associated to old classes.&lt;/p&gt;

&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this article we saw what is incremental learning: learning model with classes
coming incrementally; what is its challenge: avoiding forgetting the previous classes to
the benefice only of new classes; and broad strategies to solve this domain.&lt;/p&gt;

&lt;p&gt;This domain is far from being solved. The upper bound is a model trained in a single
step on all data. Current solutions are considerably worse than this.&lt;/p&gt;

&lt;p&gt;On a personal note, my team and I have submitted an article for a conference on this
subject. If it&amp;rsquo;s accepted, I&amp;rsquo;ll make a blog article on it. Furthermore I have made
a library to train incremental model: &lt;a href=&#34;https://github.com/arthurdouillard/incremental_learning.pytorch&#34; target=&#34;_blank&#34;&gt;inclearn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How To Be Confident In Your Neural Network Confidence</title>
      <link>/post/miscalibration/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0200</pubDate>
      
      <guid>/post/miscalibration/</guid>
      <description>

&lt;p&gt;Those notes are based on the research paper
&amp;ldquo;&lt;strong&gt;On Calibration of Modern Neural Networks&lt;/strong&gt;&amp;rdquo; by &lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;(Guo et al, 2017.)&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;how-to-be-confident-in-your-neural-network-confidence&#34;&gt;How To Be Confident In Your Neural Network Confidence?&lt;/h1&gt;

&lt;p&gt;Very large and deep models, as ResNet, are far more accurate than their older counterparts, as LeNet, on computer vision datasets such as CIFAR100. &lt;strong&gt;However while
they are better at classifying images, we are less confident in their own confidence!&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Most neural networks for classification uses as last activation a softmax: it
produces a distribution of probabilities for each target (cat, dog, boat, etc.).
These probabilities sum to one. We may expect that if for a given image, our
model associate a score of 0.8 to the target ‘boat’, our model is confident at
80% that this is the right target.&lt;/p&gt;

&lt;p&gt;Over 100 images that were detected as boat, we can expect approximately that 80
images are indeed real boats, while the 20 remaining were false positives.&lt;/p&gt;

&lt;p&gt;It was true for shallow model as LeNet but as newer models gained in accuracy
&lt;strong&gt;their confidences became decorrelated from the “real confidence”&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;This does not work anymore for deep neural networks:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration.png&#34; alt=&#34;*Figure 1: Miscalibration in modern neural network [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 1: Miscalibration in modern neural network [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;As you can see, older networks as LeNet had a low accuracy (55%) but their
confidence was actually in line with the accuracy! Modern networks as ResNet have
a higher accuracy (69%) but as showed in figure 1, they are over-confident.&lt;/p&gt;

&lt;p&gt;This discrepancy between the model confidence and the actual accuracy is called
&lt;strong&gt;miscalibration&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&#34;why-it-is-important&#34;&gt;Why It Is Important&lt;/h2&gt;

&lt;p&gt;Outside of toy datasets used in the academy, it can be useful to know how much
confident our model is.&lt;/p&gt;

&lt;p&gt;Imagine we have a model predicting frauds. We want to flag some transaction as
suspicious based on the model confidence that it is a fraud.
We could definitely compute an optimal threshold on the validation set, and then
every confidence above this threshold would be flagged as a fraud. However
this computed threshold could be 0.2 or 0.9 but would probably make much sense to a human.&lt;/p&gt;

&lt;p&gt;A model without miscalibration would help the users to interpret better the
predictions.&lt;/p&gt;

&lt;h2 id=&#34;why-it-happens&#34;&gt;Why It Happens&lt;/h2&gt;

&lt;p&gt;The authors explores empirically what are the causes of this miscalibration in
modern networks.&lt;/p&gt;

&lt;p&gt;They measure the miscalibration with the &lt;strong&gt;E&lt;/strong&gt;xpected &lt;strong&gt;C&lt;/strong&gt;alibration &lt;strong&gt;E&lt;/strong&gt;rror (ECE):
the average difference between the confidence and the accuracy. This metric should
be minimized.&lt;/p&gt;

&lt;h3 id=&#34;higher-capacity-cross-entropy&#34;&gt;Higher Capacity &amp;amp; Cross-Entropy&lt;/h3&gt;

&lt;p&gt;The most interpretable cause of the miscalibration is the increase of capacity
and the cross-entropy loss.&lt;/p&gt;

&lt;p&gt;Model capacity can be seen as a measurement of how much a model can memorize.
With an infinite capacity, the model could simply learn by heart the whole
training dataset. A trade-off has to be made between a low and high capacity.
If it is too low the model wouldn’t be able to learn essential features of your
data. If it is too high, the model will learn too much and overfit instead of
generalize. Indeed comprehension is compression: by leaving few enough capacity
the model has to pick up the most representative features (pretty much in the
same way PCA works) and will then generalize better (but too few capacity &amp;amp; no
learning will happen!).&lt;/p&gt;

&lt;p&gt;The new architectures such as ResNet have way more capacity than the older
LeNet (25M parameters for the former and 20k for the latter). This high
capacity led to better accuracy: the training set can almost be learned by heart.&lt;/p&gt;

&lt;p&gt;In addition the models optimizes the cross-entropy loss that force them to be
right AND to be very confident. The higher capacity helped to lower the
cross-entropy loss and thus encourages deep neural networks to be over-confident.
As you’ve seen on figure 1, the new models are now over-confident.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_capacity.png&#34; alt=&#34;*Figure 2: More capacity (in depth or width) increases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 2: More capacity (in depth or width) increases the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;the-mysterious-batch-normalization&#34;&gt;The Mysterious Batch Normalization&lt;/h3&gt;

&lt;p&gt;Batch Normalization normalizes the tensors in a network. It greatly improves the
training convergence &amp;amp; the final performance. Why exactly it works that well
is still a bit undefined (&lt;a href=&#34;/posts/normalization&#34;&gt;see more&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The authors remark empirically that using Batch Normalization increased the miscalibration
but could not find an exact reason why.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_bn.png&#34; alt=&#34;*Figure 3: Batch Normalization increases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 3: Batch Normalization increases the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Could the help given by this method in training facilitate the over-confidence?&lt;/p&gt;

&lt;h3 id=&#34;regularization&#34;&gt;Regularization&lt;/h3&gt;

&lt;p&gt;The weight decay is an additional loss that penalizes the L2 norm of the weights.
The larger the weights, the bigger the norm and thus the loss. By constraining the weights
magnitude, it avoid the model finding extreme weight values that could make it overfit.&lt;/p&gt;

&lt;p&gt;The authors found that increasing the regularization decreases the model accuracy
as expected. However it also decreased the miscalibration! The answer is then again
because regularization avoid overfitting &amp;amp; thus over-confidence.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_reg.png&#34; alt=&#34;*Figure 4: More regularization decreases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 4: More regularization decreases the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h2 id=&#34;how-to-fix-miscalibration&#34;&gt;How To Fix Miscalibration&lt;/h2&gt;

&lt;p&gt;This article&amp;rsquo;s title, &amp;ldquo;&lt;em&gt;How To Be Confident In Your Neural Network Confidence&lt;/em&gt;&amp;rdquo;,
led you to believe that you would discover how to reduce miscalibration.&lt;/p&gt;

&lt;p&gt;You&amp;rsquo;re not going to reduce the capacity, remove Batch Normalization, and increase
the regularization: you&amp;rsquo;ll hurt too much your precious accuracy.&lt;/p&gt;

&lt;p&gt;Fortunately there are post-processing solutions. The authors describe several
but the most effective one is also the simplest: &lt;strong&gt;Temperature Scaling&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Instead of computing the softmax like this:&lt;/p&gt;

&lt;p&gt;$$\text{softmax}(x)_i = \frac{e^{y_i}}{\Sigma_j^N e^{y_j}}$$&lt;/p&gt;

&lt;p&gt;All the logits (values just before the final activation, here softmax) are divided
by the same value called temperature:&lt;/p&gt;

&lt;p&gt;$$\text{softmax}(x)_i = \frac{e^{\frac{y_i}{T}}}{\Sigma_j^N e^{\frac{y_j}{T}}}$$&lt;/p&gt;

&lt;p&gt;Similar to (&lt;a href=&#34;https://arxiv.org/abs/1503.02531&#34; target=&#34;_blank&#34;&gt;Hinton et al, 2015.&lt;/a&gt;), this temperature
&lt;em&gt;softens the probabilities&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Extreme probabilities (high confidence) are more decreased than smaller probabilities
(low confidence). The authors find the optimal temperature by minimizing the
Expected Calibration Error on the validation set.&lt;/p&gt;

&lt;p&gt;The miscalibration is almost entirely corrected:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/miscalibration_tempscaling.png&#34; alt=&#34;*Figure 5: Temperature Scaling fixes the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 5: Temperature Scaling fixes the miscalibration. [&lt;a href=&#34;https://arxiv.org/abs/1706.04599&#34; target=&#34;_blank&#34;&gt;source&lt;/a&gt;]&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;Another cool feature of Temperature Scaling: because all logits are divided by the
same value, and that softmax is a &lt;a href=&#34;https://en.wikipedia.org/wiki/Monotonic_function&#34; target=&#34;_blank&#34;&gt;monotone function&lt;/a&gt;,
the accuracy remains unchanged!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Normalization in Deep Learning</title>
      <link>/post/normalization/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/normalization/</guid>
      <description>

&lt;p&gt;Deep Neural Networks (DNNs) are notorious for requiring less feature engineering than
Machine Learning algorithms. For example convolutional networks learn by themselves
the right convolution kernels to apply on an image. No need of carefully
handcrafted kernels.&lt;/p&gt;

&lt;p&gt;However a common point to all kinds of neural networks is the &lt;strong&gt;need of normalization&lt;/strong&gt;.
Normalizing is often done on the input, but it can also take place inside the
network. In this article I&amp;rsquo;ll try to describe what the literature is saying about
this.&lt;/p&gt;

&lt;p&gt;This article is not exhaustive but it tries to cover the major algorithms. If
you feel I missed something important, tell me!&lt;/p&gt;

&lt;h3 id=&#34;normalizing-the-input&#34;&gt;Normalizing the input&lt;/h3&gt;

&lt;p&gt;It is &lt;em&gt;extremely&lt;/em&gt; common to normalize the input
&lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf&#34; target=&#34;_blank&#34;&gt;(lecun-98b)&lt;/a&gt;, especially
for computer vision tasks. Three normalization schemes are often seen:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Normalizing the pixel values between 0 and 1 (as &lt;a href=&#34;https://pytorch.org/vision/stable/transforms.html&#34; target=&#34;_blank&#34;&gt;Torch&amp;rsquo;s ToTensor does&lt;/a&gt;):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Normalizing the pixel values between -1 and 1 (as &lt;a href=&#34;https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L47-L50&#34; target=&#34;_blank&#34;&gt;Tensorflow does&lt;/a&gt;):&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;127.5&lt;/span&gt;
img &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Why is it recommended? Let&amp;rsquo;s take a neuron, where:&lt;/p&gt;

&lt;p&gt;$$y = w \cdot x$$&lt;/p&gt;

&lt;p&gt;The partial derivative of $y$ for $w$ that we use during backpropagation is:&lt;/p&gt;

&lt;p&gt;$$\frac{\partial y}{\partial w} = X^T$$&lt;/p&gt;

&lt;p&gt;The scale of the data has an effect on the magnitude of the gradient for
the weights. If the gradient is big, you should reduce the learning rate.
However you usually have different gradient magnitudes in a same batch. Normalizing
the image to smaller pixel values is a cheap price to pay while making easier to
tune an optimal learning rate for input images.&lt;/p&gt;

&lt;p&gt;Furthermore, we usually apply a second type of normalization after the first one, called &lt;strong&gt;constrast normalization&lt;/strong&gt;.
As the name imply it normalize the contrast so that the model doesn&amp;rsquo;t learn this spurious correlation.
To do so, we normalize according to the dataset mean &amp;amp; standard deviation (as &lt;a href=&#34;https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L52-L55&#34; target=&#34;_blank&#34;&gt;Torch does&lt;/a&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255.&lt;/span&gt;
mean &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.485&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.456&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.406&lt;/span&gt;] &lt;span style=&#34;color:#75715e&#34;&gt;# Here it&amp;#39;s ImageNet statistics&lt;/span&gt;
std &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0.229&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.224&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.225&lt;/span&gt;]

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;): &lt;span style=&#34;color:#75715e&#34;&gt;# Considering an ordering NCHW (batch, channel, height, width)&lt;/span&gt;
    img[i, :, :] &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; mean[i]
    img[i, :, :] &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; std[i]&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id=&#34;1-batch-normalization&#34;&gt;1. Batch Normalization&lt;/h3&gt;

&lt;p&gt;We&amp;rsquo;ve seen previously how to normalize the input, now let&amp;rsquo;s see a normalization
inside the network.&lt;/p&gt;

&lt;p&gt;(&lt;a href=&#34;https://arxiv.org/abs/1502.03167&#34; target=&#34;_blank&#34;&gt;Ioffe &amp;amp; Szegedy, 2015&lt;/a&gt;) declared that DNN
training was suffering from the &lt;em&gt;internal covariate shift&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The authors describe it as:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;[&amp;hellip;] the distribution of each layer’s inputs changes during training, as the
parameters of the previous layers change.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Their answer to this problem was to apply to the pre-activation a Batch
Normalization (BN):&lt;/p&gt;

&lt;p&gt;$$BN(x) = \gamma \frac{x - \mu_B}{\sigma_B} + \beta$$&lt;/p&gt;

&lt;p&gt;$\mu_B$ and $\sigma_B$ are the mean and the standard deviation of the batch.
$\gamma$ and $\beta$ are learned parameters.&lt;/p&gt;

&lt;p&gt;The batch statistics are computed for a whole channel:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/batch_norm.png&#34; alt=&#34;*Statistics are computed for a whole batch, channel per channel.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Statistics are computed for a whole batch, channel per channel.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;$\gamma$ and $\beta$ are essential because they enable the BN to represent
the identity transform if needed. If it couldn&amp;rsquo;t, the resulting BN&amp;rsquo;s transformation
(with a mean of 0 and a variance of 1) fed to a sigmoid non-linearity would
be constrained to its linear regime.&lt;/p&gt;

&lt;p&gt;While during training the mean and standard deviation are computed on the batch,
during test time BN uses the whole dataset statistics using a moving average/std.&lt;/p&gt;

&lt;p&gt;Batch Normalization has showed a considerable training acceleration to existing
architectures and is now an almost de facto layer. It has however for weakness
to use the batch statistics at training time: With small batches or with a dataset
non &lt;a href=&#34;https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables&#34; target=&#34;_blank&#34;&gt;i.i.d&lt;/a&gt;
it shows weak performance. In addition to that, the difference between training
and test time of the mean and the std can be important, this can lead to a difference of performance between the two modes.&lt;/p&gt;

&lt;h3 id=&#34;1-1-batch-renormalization&#34;&gt;1.1. Batch ReNormalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.03275&#34; target=&#34;_blank&#34;&gt;(Ioffe, 2017)&lt;/a&gt;&amp;rsquo;s Batch Renormalization (BR)
introduces an improvement over Batch Normalization.&lt;/p&gt;

&lt;p&gt;BN uses the statistics ($\mu_B$ &amp;amp; $\sigma_B$) of the batch. BR introduces
two new parameters $r$ &amp;amp; $d$ aiming to constrain the mean and std of BN,
reducing the extreme difference when the batch size is small.&lt;/p&gt;

&lt;p&gt;Ideally the normalization should be done with the instance&amp;rsquo;s statistic:&lt;/p&gt;

&lt;p&gt;$$\hat{x} = \frac{x - \mu}{\sigma}$$&lt;/p&gt;

&lt;p&gt;By choosing $r = \frac{\sigma_B}{\sigma}$ and $d = \frac{\mu_B - \mu}{\sigma}$:&lt;/p&gt;

&lt;p&gt;$$\hat{x} = \frac{x - \mu}{\sigma} = \frac{x - \mu_B}{\sigma_B} \cdot r + d$$&lt;/p&gt;

&lt;p&gt;The authors advise to constrain the maximum absolute values of $r$ and $d$.
At first to 1 and 0, behaving like BN, then to relax gradually those bounds.&lt;/p&gt;

&lt;h3 id=&#34;1-2-internal-covariate-shift&#34;&gt;1.2. Internal Covariate Shift?&lt;/h3&gt;

&lt;p&gt;Ioffe &amp;amp; Szegedy argued that the changing distribution of the pre-activation hurt
the training. While Batch Norm is widely used in SotA research, there is still
controversy (&lt;a href=&#34;https://youtu.be/Qi1Yry33TQE?t=17m4s&#34; target=&#34;_blank&#34;&gt;Ali Rahami&amp;rsquo;s Test of Time&lt;/a&gt;)
about what this algorithm is solving.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1805.11604&#34; target=&#34;_blank&#34;&gt;(Santurkar et al, 2018)&lt;/a&gt; refuted the Internal
Covariate Shift influence. To do so, they compared three models, one baseline,
one with BN, and one with random noise added &lt;em&gt;after&lt;/em&gt; the normalization.&lt;/p&gt;

&lt;p&gt;Because of the random noise, the activation&amp;rsquo;s input is not &lt;em&gt;normalized&lt;/em&gt; anymore
and its distribution change at every time test.&lt;/p&gt;

&lt;p&gt;As you can see on the following figure, they found that the random shift of distribution
didn&amp;rsquo;t produce extremely different results:&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/cmp_icf.png&#34; alt=&#34;*Comparison between standard net, net with BN, and net with noisy BN.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Comparison between standard net, net with BN, and net with noisy BN.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;On the other hand they found that the Batch Normalization improved the
&lt;a href=&#34;https://en.wikipedia.org/wiki/Lipschitz_continuity&#34; target=&#34;_blank&#34;&gt;Lipschitzness&lt;/a&gt; of the loss
function. In simpler term, the loss is smoother, and thus its gradient as well.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/smoothed_loss.png&#34; alt=&#34;*Figure 3: Loss with and without Batch Normalization.*&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    &lt;em&gt;Figure 3: Loss with and without Batch Normalization.&lt;/em&gt;
    
    
    
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;p&gt;According to the authors:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Improved Lipschitzness of the gradients gives us confidence that when we take
a larger step in a direction of a computed gradient, this gradient direction
remains a fairly accurate estimate of the actual gradient direction after
taking that step.  It thus enables any (gradient–based) training algorithm to
take larger steps without the danger of running into a sudden change of the
loss landscape such as flat region (corresponding to vanishing gradient) or
sharp local minimum (causing exploding gradients).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The authors also found that replacing BN by a $l_1$, $l_2$, or $l_{\infty}$
lead to similar results.&lt;/p&gt;

&lt;h3 id=&#34;2-computing-the-mean-and-variance-differently&#34;&gt;2. Computing the mean and variance differently&lt;/h3&gt;

&lt;p&gt;Algorithms similar to Batch Norm have been developed where the mean &amp;amp; variance
are computed differently.&lt;/p&gt;

&lt;figure&gt;

&lt;img src=&#34;/figures/normalization.png&#34; /&gt;



&lt;figcaption data-pre=&#34;Figure &#34; data-post=&#34;:&#34; &gt;
  &lt;p style=&#34;text-align: center&#34;&gt;
    
    &lt;a href=&#34;https://arxiv.org/abs/1803.08494&#34;&gt; 
    source
    &lt;/a&gt; 
  &lt;/p&gt; 
&lt;/figcaption&gt;

&lt;/figure&gt;

&lt;h3 id=&#34;2-1-layer-normalization&#34;&gt;2.1. Layer Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34; target=&#34;_blank&#34;&gt;(Ba et al, 2016)&lt;/a&gt;&amp;rsquo;s layer norm (LN) normalizes
each image of a batch independently using all the channels. The goal is have constant
performance with a large batch or a single image. &lt;strong&gt;It&amp;rsquo;s used in recurrent neural
networks&lt;/strong&gt; where the number of time steps can differ between tasks.&lt;/p&gt;

&lt;p&gt;While all time steps share the same weights, each should have its own statistic.
BN needs previously computed batch statistics, which would be impossible if there
are more time steps at test time than training time. LN is time steps independent
by simply computing the statistics on the incoming input.&lt;/p&gt;

&lt;h3 id=&#34;2-2-instance-normalization&#34;&gt;2.2. Instance Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1607.08022&#34; target=&#34;_blank&#34;&gt;(Ulyanov et al, 2016)&lt;/a&gt;&amp;rsquo;s instance norm (IN)
normalizes each channel of each batch&amp;rsquo;s image independently. &lt;strong&gt;The goal is to
normalize the constrast of the content image&lt;/strong&gt;. According to the authors, only the
style image contrast should matter.&lt;/p&gt;

&lt;h3 id=&#34;2-3-group-normalization&#34;&gt;2.3. Group Normalization&lt;/h3&gt;

&lt;p&gt;According to &lt;a href=&#34;https://arxiv.org/abs/1803.08494&#34; target=&#34;_blank&#34;&gt;(Wu and He, 2018)&lt;/a&gt;, convolution
filters tend to group in related tasks (frequency, shapes, illumination, textures).&lt;/p&gt;

&lt;p&gt;They normalize each image in a batch independently so the model is batch size
independent. Moreover they normalize the channels per group arbitrarily defined
(usually 32 channels per group). All filters of a same group should specialize
in the same task.&lt;/p&gt;

&lt;h3 id=&#34;3-normalization-on-the-network&#34;&gt;3. Normalization on the network&lt;/h3&gt;

&lt;p&gt;Previously shown methods normalized the inputs, there are methods were the normalization
happen in the network rather than on the data.&lt;/p&gt;

&lt;h3 id=&#34;3-1-weight-normalization&#34;&gt;3.1. Weight Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1602.07868&#34; target=&#34;_blank&#34;&gt;(Salimans and Kingma, 2016)&lt;/a&gt; found that
decoupling the length of the weight vectors from their direction accelerated the
training.&lt;/p&gt;

&lt;p&gt;A fully connected layer does the following operation:&lt;/p&gt;

&lt;p&gt;$$y = \phi(W \cdot x + b)$$&lt;/p&gt;

&lt;p&gt;In weight normalization, the weight vectors is expressed the following way:&lt;/p&gt;

&lt;p&gt;$$W = \frac{g}{\Vert V \Vert}V$$&lt;/p&gt;

&lt;p&gt;$g$ and $V$ being respectively a learnable scalar and a learnable matrix.&lt;/p&gt;

&lt;h3 id=&#34;3-2-cosine-normalization&#34;&gt;3.2. Cosine Normalization&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/1702.05870&#34; target=&#34;_blank&#34;&gt;(Luo et al, 2017)&lt;/a&gt; normalizes both the weights
and the input by replacing the classic dot product by a cosine similarity:&lt;/p&gt;

&lt;p&gt;$$y = \phi(\frac{W \cdot X}{\Vert W \Vert \Vert X \Vert})$$&lt;/p&gt;

&lt;h3 id=&#34;4-conclusion&#34;&gt;4. Conclusion&lt;/h3&gt;

&lt;p&gt;Batch normalization (BN) is still the most represented method among new
architectures despite its defect: the dependence on the batch size. Batch
renormalization (BR) fixes this problem by adding two new parameters to
approximate instance statistics instead of batch statistics.&lt;/p&gt;

&lt;p&gt;Layer norm (LN), instance norm (IN), and group norm (GN), are similar to
BN. Their difference lie in the way statistics are computed.&lt;/p&gt;

&lt;p&gt;LN was conceived for RNNs, IN for style transfer, and GN for CNNs.&lt;/p&gt;

&lt;p&gt;Finally weigh norm and cosine norm normalize the network&amp;rsquo;s weight instead of simply
the input data.&lt;/p&gt;

&lt;p&gt;EDIT: this post has been recommended by &lt;a href=&#34;https://forums.fast.ai/t/lesson-6-official-resources-and-updates/31441&#34; target=&#34;_blank&#34;&gt;FastAI&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
