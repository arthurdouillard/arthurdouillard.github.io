<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Arthur Douillard</title>
    <link>/post/</link>
    <description>Recent content in Posts on Arthur Douillard</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0100</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Vision Transformers</title>
      <link>/post/visual_transformers/</link>
      <pubDate>Thu, 20 May 2021 00:00:00 +0200</pubDate>
      
      <guid>/post/visual_transformers/</guid>
      <description>There are currently two broads axis of research in Deep Learning: finding better architectures, or finding better losses to train them. Since AlexNet (Kriwhevsky et al.), Convolutional Neural Network is the main architecture used in Computer Vision. Through convolutions, with its efficient patch prior, many visual tasks were unlocked.
Many variations of AlexNet were invented, but if we were to name only one, it would be the ResNet (He et al.</description>
    </item>
    
    <item>
      <title>Review of Continual Learning at CVPR&#39;20</title>
      <link>/post/cvpr20-continual/</link>
      <pubDate>Sun, 21 Jun 2020 00:00:00 +0200</pubDate>
      
      <guid>/post/cvpr20-continual/</guid>
      <description>I review in this article, papers published at CVPR 2020 about Continual Learning. If you think I made a mistake or miss an important paper, please tell me!
Table of Contents    1. Few-Shots  1.1. Few-Shot Class-Incremental Learning 1.2. Incremental Few-Shot Object Detection  2. Conditional Channel Gated Networks for Task-Aware Continual Learning 3. Incremental Learning in Online Scenario 4. iTAML: An Incremental Task-Agnostic Meta-learning 5. Modeling the Background for Incremental Learning 6.</description>
    </item>
    
    <item>
      <title>Learning Deep Neural Networks incrementally forever</title>
      <link>/post/incremental-learning/</link>
      <pubDate>Wed, 11 Dec 2019 00:00:00 +0100</pubDate>
      
      <guid>/post/incremental-learning/</guid>
      <description>The hallmark of human intelligence is the capacity to learn. A toddler has comparable aptitudes to reason about space, quantities, or causality than other ape species (source). The difference of our cousins and us is the ability to learn from others.
The recent deep learning hype aims to reach the Artificial General Intelligence (AGI): an AI that would express (supra-)human-like intelligence. Unfortunately current deep learning models are flawed in many ways: one of them is that they are unable to learn continuously as human does through years of schooling, and so on.</description>
    </item>
    
    <item>
      <title>How To Be Confident In Your Neural Network Confidence</title>
      <link>/post/miscalibration/</link>
      <pubDate>Thu, 30 May 2019 00:00:00 +0200</pubDate>
      
      <guid>/post/miscalibration/</guid>
      <description>Those notes are based on the research paper &amp;ldquo;On Calibration of Modern Neural Networks&amp;rdquo; by (Guo et al, 2017.).
How To Be Confident In Your Neural Network Confidence? Very large and deep models, as ResNet, are far more accurate than their older counterparts, as LeNet, on computer vision datasets such as CIFAR100. However while they are better at classifying images, we are less confident in their own confidence!
Most neural networks for classification uses as last activation a softmax: it produces a distribution of probabilities for each target (cat, dog, boat, etc.</description>
    </item>
    
    <item>
      <title>Normalization in Deep Learning</title>
      <link>/post/normalization/</link>
      <pubDate>Fri, 10 Aug 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/normalization/</guid>
      <description>Deep Neural Networks (DNNs) are notorious for requiring less feature engineering than Machine Learning algorithms. For example convolutional networks learn by themselves the right convolution kernels to apply on an image. No need of carefully handcrafted kernels.
However a common point to all kinds of neural networks is the need of normalization. Normalizing is often done on the input, but it can also take place inside the network. In this article I&amp;rsquo;ll try to describe what the literature is saying about this.</description>
    </item>
    
    <item>
      <title>Detecting cars from aerial imagery for the NATO Innovation Challenge</title>
      <link>/post/nato-challenge/</link>
      <pubDate>Fri, 22 Jun 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/nato-challenge/</guid>
      <description>Imagine you’re in a landlocked country, and an infection has spread. The government has fallen, and rebels are roaming the country. If you’re the armed forces in this scenario, how do you make decisions in this environment? How can you fully understand the situation at hand?
A few weeks ago, NATO organized an innovation challenge that posed these questions. We decided to take on the challenge with the goal of finding innovative solutions in the areas of data filtering/fusing, visualization, and predictive analytics.</description>
    </item>
    
    <item>
      <title>3 Small but Powerful Convolutional Neural Networks</title>
      <link>/post/3-small-but-powerful-cnn/</link>
      <pubDate>Sun, 13 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/3-small-but-powerful-cnn/</guid>
      <description>Most CNN architectures have been developed to attain the best accuracy on ImageNet. Computing power is not limited for this competition, why bother?
However you may want to run your model on an old laptop, maybe without GPU, or even on your mobile phone. Let’s see three CNN architectures that are efficient while sacrificing little accuracy performance.
1. MobileNet Arxiv link: (Howard et al, 2017)
MobileNet uses depthwise separable convolutions.</description>
    </item>
    
    <item>
      <title>Densely Connected Convolutional Networks</title>
      <link>/post/densenet/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/densenet/</guid>
      <description>This article contains note of the research paper:
 Densely Connected Convolutional Networks by Cornell Uni, Tsinghua Uni, and Facebook Research.  This paper was awarded the CVPR 2017 Best Paper Award.
Introduction DenseNet is a new CNN architecture that reached State-Of-The-Art (SOTA) results on classification datasets (CIFAR, SVHN, ImageNet) using less parameters.
Thanks to its new use of residual it can be deeper than the usual networks and still be easy to optimize.</description>
    </item>
    
    <item>
      <title>Deep Learning Scaling Is Predictable, Empirically</title>
      <link>/post/deep-learning-scaling/</link>
      <pubDate>Fri, 20 Apr 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/deep-learning-scaling/</guid>
      <description>This post contains the notes taken from the following paper:
 Deep Learning Scaling Is Predictable, Empirically by Baidu Research.  The last years in Deep Learning have seen a rush to gigantism:
 Models are becoming deeper and deeper from the 8 layers of AlexNet to the 1001-layer ResNet. Training on large dataset is way quicker, ImageNet can now (with enough computing power) been trained in less than 20 minutes.</description>
    </item>
    
    <item>
      <title>Fast and Faster Region-based Convolutional Network</title>
      <link>/post/faster-rcnn/</link>
      <pubDate>Mon, 26 Mar 2018 00:00:00 +0200</pubDate>
      
      <guid>/post/faster-rcnn/</guid>
      <description>This post contains the notes taken from the following paper:
 Fast-RCNN by R. Girshick.
 Faster-RCNN by Microsoft Research.
  Ross Girshick is an influential researcher on object detection: he has worked on RCNN, Fast{er}-RCNN, Yolo, RetinaNet&amp;hellip;
Fast-RCNN and Faster-RCNN are both incremental improvements on the original RCNN.
Let&amp;rsquo;s see what were those improvements:
Fast-RCNN In Fast-RCNN, Girshick ditched the SVM used previously. It resulted in a 10x inference speed improvement, and a better accuracy.</description>
    </item>
    
    <item>
      <title>Selective Search for Object Recognition</title>
      <link>/post/selective-search/</link>
      <pubDate>Fri, 09 Mar 2018 00:00:00 +0100</pubDate>
      
      <guid>/post/selective-search/</guid>
      <description>This post contains the notes taken from reading of the following paper:
 Selective Search for Object Recognition.  This paper, published in 2012, describes an algorithm generating multiple possible object locations that will later be used by object recognition models. Fast-RCNN uses the Selective Search in its object proposal module.
Motivations The authors divide the domain of object recognition in three categories:
 Exhaustive Search Segmentation Other sampling strategies (using Bag-of-Words, Hough Transform, etc.</description>
    </item>
    
    <item>
      <title>Efficient Graph-Based Segmentation</title>
      <link>/post/efficient-graph-based-segmentation/</link>
      <pubDate>Wed, 07 Mar 2018 00:00:00 +0100</pubDate>
      
      <guid>/post/efficient-graph-based-segmentation/</guid>
      <description>This post contains the notes taken from reading of the following paper:
 Efficient Graph-Based Segmentation by Pedro Felzenszwalb and Daniel Huttenlocher.  I was also helped by the slides of Stanford&amp;rsquo;s CS231b.
Fast-RCNN was the state-of-the-art algorithm for object detection in 2015; its object proposal used Selective Search that itself used Efficient Graph-Based Segmentation.
The reason this segmentation was still useful almost 10 years later is because the algorithm is fast, while remaining efficient.</description>
    </item>
    
    <item>
      <title>A Few Useful Things To Know About Machine Learning</title>
      <link>/post/useful-things-to-know-about-ml/</link>
      <pubDate>Tue, 06 Feb 2018 00:00:00 +0100</pubDate>
      
      <guid>/post/useful-things-to-know-about-ml/</guid>
      <description>This post contains the notes taken from reading of the following paper:
 A few useful things to know about Machine Learning by Pedro Domingos.  This paper does not introduce any novelties in the field of Machine Learning, nor some kinds of benchmarks, but rather offers a overview of the black art of Machine Learning. Domingos covers a wide area of Machine Learning, but each parts are not explored in depth.</description>
    </item>
    
    <item>
      <title>How To Read A Paper</title>
      <link>/post/how-to-read-a-paper/</link>
      <pubDate>Tue, 06 Feb 2018 00:00:00 +0100</pubDate>
      
      <guid>/post/how-to-read-a-paper/</guid>
      <description>Preambule During my master in Data Science I have read a few papers. While I am a good reader, reading a scientific paper is still a strugle. For the year 2018, and hopefuly the next years, I have decided to read more papers. At least one a week.
My favorite method to learn something is to explain it to someone else. That&amp;rsquo;s the Feyman&amp;rsquo;s technique. It may be hard to find a patient listener thus I am making this blog to explain to the potential reader papers I am reading.</description>
    </item>
    
  </channel>
</rss>