<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.0.0">
  <meta name="generator" content="Hugo 0.49.2" />
  <meta name="author" content="Arthur Douillard">

  
  
  
  
    
  
  <meta name="description" content="Imagine you’re in a landlocked country, and an infection has spread. The gouvernement has fallen, and rebels are roaming the country. If you’re the armed forces in this scenario, how do you make decisions in this environment? How can you fully understand the situation at hand?">

  
  <link rel="alternate" hreflang="en-us" href="/post/nato-challenge/">

  


  

  

  

  
  
  
  <meta name="theme-color" content="#e0491f">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/search_bar.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-102629480-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Arthur Douillard">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Arthur Douillard">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/nato-challenge/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@Ar_Douilllard">
  <meta property="twitter:creator" content="@Ar_Douilllard">
  
  <meta property="og:site_name" content="Arthur Douillard">
  <meta property="og:url" content="/post/nato-challenge/">
  <meta property="og:title" content="Detecting cars from aerial imagery for the NATO Innovation Challenge | Arthur Douillard">
  <meta property="og:description" content="Imagine you’re in a landlocked country, and an infection has spread. The gouvernement has fallen, and rebels are roaming the country. If you’re the armed forces in this scenario, how do you make decisions in this environment? How can you fully understand the situation at hand?">
  
  
    
  <meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2018-06-22T00:00:00&#43;02:00">
  
  <meta property="article:modified_time" content="2018-06-22T00:00:00&#43;02:00">
  

  

  
<meta name="twitter:title" content="Detecting cars from aerial imagery for the NATO Innovation Challenge" />



<meta name="twitter:description" content="Imagine you’re in a landlocked country, and an infection has spread. The gouvernement has fallen, and rebels are roaming the country. If you’re the armed forces in this scenario, how do you make decisions in this environment? How can you fully understand the situation at hand?" />



<meta name="twitter:image" content="/figures/nato_result_zoom.png" />



<meta property="og:image" content=/figures/nato_result_zoom.png />



<meta property="og:description" content="Imagine you’re in a landlocked country, and an infection has spread. The gouvernement has fallen, and rebels are roaming the country. If you’re the armed forces in this scenario, how do you make decisions in this environment? How can you fully understand the situation at hand?" />


<link rel="icon" type="image/png" href="/img/favicon32.png">
<link rel="apple-touch-icon" type="image/png" href="/img/favicon180.png">


  <title>Detecting cars from aerial imagery for the NATO Innovation Challenge | Arthur Douillard</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Arthur Douillard</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#talks_list">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/books">
            
            <span>Books</span>
            
          </a>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  

















<div class="article-container pt-3">
  <h1 itemprop="name">Detecting cars from aerial imagery for the NATO Innovation Challenge</h1>

  

  
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Arthur Douillard">
  </span>
  

  <span class="article-date">
    
    <meta content="2018-06-22 00:00:00 &#43;0200 CEST" itemprop="datePublished">
    <time datetime="2018-06-22 00:00:00 &#43;0200 CEST" itemprop="dateModified">
      22 June, 2018
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Arthur Douillard">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    14 min read
  </span>
  
  

  
  

  

  
  

  

</div>

  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<p><strong>Imagine you’re in a landlocked country, and an infection has spread. The
government has fallen, and rebels are roaming the country. If you’re the armed
forces in this scenario, how do you make decisions in this environment? How can
you fully understand the situation at hand?</strong></p>

<p><img src="/figures/nato.png" alt="nato" /></p>

<p>A few weeks ago, NATO organized an innovation challenge that posed these
questions. We decided to take on the challenge with
the goal of finding innovative solutions in the areas of data filtering/fusing,
visualization, and predictive analytics.</p>

<p>For those who don’t know, NATO is an intergovernmental military alliance between
29 North American and European countries. It constitutes a system of collective
defense whereby its independent member states agree to mutual defense in response
to an attack by any external party.</p>

<p>NATO did not provide any data for the challenge, so we had to find it ourselves.
Ultimately, the solution we came up with used a variety of different techniques
including computer vision on aerial imagery, natural language processing on
press &amp; social media, geo data processing, and — of course — fancy graphs.</p>

<p>In this post, we will focus on the most technical part: object detection for
aerial imagery, walking through what kind of data we used, which architecture
was employed, and how the solution works, and finally our results. If you’re
interested in a higher-level look at the project, that’s over
<a href="https://blog.dataiku.com/data-science-and-disease-outbreak-assistance-nato-act-innovation-challenge" target="_blank">here</a>.</p>

<p>This challenge was done while I was an intern at <a href="https://www.dataiku.com/" target="_blank">Dataiku</a>.
My team was composed of an commander, a salesman, and myself as the lead/sole scientist.</p>

<h3 id="1-the-dataset">1. The dataset</h3>

<p>For the object detection portion of the project, we used the
<a href="https://gdo152.llnl.gov/cowc/" target="_blank">Cars Overhead With Context</a> (COWC) dataset,
which is provided by the Lawrence Livermore National Laboratory. It features
aerial imagery taken in six distinct locations:</p>

<ul>
<li>Toronto, Canada</li>
<li>Selwyn, New Zealand</li>
<li>Potsdam and Vaihingen*, Germany</li>
<li>Columbus (Ohio)* and Utah, USA</li>
</ul>

<p>* <em>We ultimately did not use the Columbus and Vaihingen data because the
imagery was in grayscale.</em></p>

<p>This dataset offers large imagery (up to 4 square kilometers) with good
resolution (15cm per pixel) with the center localization of every car. As
suggested in <a href="https://medium.com/the-downlinq/car-localization-and-counting-with-overhead-imagery-an-interactive-exploration-9d5a029a596b" target="_blank">this Medium post</a>, we assumed that cars have a mean size of
3 meters. We created boxes centered around each car center to achieve our
ultimate goal of predicting box (i.e., car) locations in unseen images.</p>

<figure>

<img src="/figures/cowc_example1.png" alt="*Figure 1: An example image from the COWC dataset*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 1: An example image from the COWC dataset</em>
    
    
    
  </p> 
</figcaption>

</figure>

<h3 id="2-the-architecture">2. The architecture</h3>

<p>To detect cars in these large aerial images, we used the RetinaNet <a href="https://arxiv.org/abs/1708.02002" target="_blank">(Lin et al, 2017)</a> architecture. Published in 2017 by Facebook
FAIR, this paper won the Best Student Paper of ICCV 2017.</p>

<p>Object detection architectures are split in two categories: single-stage and
two-stage.</p>

<p><strong>Two-stage architectures</strong> first categorize potential objects in two classes:
foreground or background. Then all foreground’s potential objects are classified
in more fine-grained classes: cats, dogs, cars, etc. This two-stage method is
slow but also, and of course, produces the best accuracy. The most famous
two-stage architecture is <a href="/post/faster-rcnn">Faster-RCNN</a> <a href="https://arxiv.org/abs/1506.01497" target="_blank">(Ren et al, 2015)</a>.</p>

<p>On the other hand, <strong>single-stage architectures</strong> don’t have this pre-selection
step of potential foreground objects. They are usually less accurate, but
they are also faster. RetinaNet’s single-stage architecture is an exception:
it reaches two-stage performance while having single-stage speed!</p>

<p>On the figure 2 below, you can see a comparison of object detection
architectures.</p>

<figure>

<img src="/figures/cmp_obj_detect.png" alt="*Figure 2: Performance of object detection algorithms*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 2: Performance of object detection algorithms</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>RetinaNet is made of four components. We’ll try to describe how the data is
transformed through every step.</p>

<figure>

<img src="/figures/retinanet.png" alt="*Figure 3: The RetinaNet architecture*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 3: The RetinaNet architecture</em>
    
    
    
  </p> 
</figcaption>

</figure>

<h4 id="2-1-convolutional-network">2.1. Convolutional Network</h4>

<p>First of all there is a <strong>ResNet-50</strong> <a href="https://arxiv.org/abs/1512.03385" target="_blank">(He et al., 2015)</a>.
As every convolutional neural network (CNN),
it takes an image as input and processes it through convolution kernels.
Each kernel’s output is a feature map — the first feature maps capture high-level
features (such as a line or a color). The further we go down in the network,
the smaller the feature maps become because of the pooling layers. While they
are smaller, they also represent more fined-grained information (such as an eye,
a dog ear, etc.). The input image has three channels (red, blue, green), but
every subsequent feature map has dozens of channels! Each of them represents
a different kind of feature that it captured.</p>

<p>A common classifier takes the ResNet’s last feature maps (of shape <code>(7, 7, 2048)</code>),
applies an average pooling on each channel (resulting in <code>(1, 1, 2048)</code>), and feeds
it to a fully connected layer with a softmax.</p>

<h4 id="2-2-feature-pyramid-network">2.2. Feature Pyramid Network</h4>

<p>Instead of adding a classifier after ResNet, RetinaNet adds a
<strong>Feature Pyramid Network</strong> (FPN) <a href="https://arxiv.org/abs/1612.03144" target="_blank">(Lin et al., 2016)</a>.
By picking feature maps at different layers from the ResNet, it provides
rich and multi-scale features.</p>

<figure>

<img src="/figures/fpn.png" alt="*Figure 4: The lateral connection between the backbone and the FPN*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 4: The lateral connection between the backbone and the FPN</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>However, ResNet’s first feature maps may be too crude to extract any useful
information. As you can see in figure 4, the smaller and more precise feature
maps are combined with the bigger feature maps. We first upsample the smaller
ones and then sum it with the bigger ones. Several upsampling methods exist;
here, the upsampling is done with the nearest neighbor method.</p>

<p>Each level of the FPN encodes a different kind of information at a different
scale. Thus, each of them should participate in the object detection task.
The FPN takes as input the output of the third (512 channels), fourth
(1024 channels), and fifth (2048 channels) blocks of ResNet. The third is half
the size of the fourth, and the fourth is half of the fifth.</p>

<p>We apply <a href="/post/3-small-but-powerful-cnn">pointwise convolution</a>
(convolution with a 1x1 kernel) to uniformize the number of channels of each
level to 256. Then we upsampled the smaller levels by a factor of two to match
the dimension of the bigger levels.</p>

<h4 id="2-3-anchors">2.3. Anchors</h4>

<p>At each FPN level, <strong>anchors</strong> are moved around the FPN’s feature maps.
An anchor is a rectangle with different sizes and ratios, like this:</p>

<figure>

<img src="/figures/anchors.svg" alt="*Figure 5: A sample of anchors of different sizes and ratios*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 5: A sample of anchors of different sizes and ratios</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>These anchors are the base position of the potential objects. Five sizes and
three ratios exist, thus there are 15 unique anchors. These anchors are also
scaled according to the dimension of the FPN levels. These unique anchors are
duplicated on all the possible positions in the feature maps. It results in $K$
total anchors.</p>

<p>Let’s put aside those anchors for the moment.</p>

<h4 id="2-4-regression-classification">2.4. Regression &amp; classification</h4>

<p>Each FPN’s level is fed to two <strong>Fully Convolutional Networks</strong> (FCN), which are
neural networks made only of convolutions and pooling. To fully exploit the fact
that every FPN’s level holds different kind of information, the two FCNs are
shared among all levels! Convolution layers are independent of the input size;
only their kernel size matter. Thus while each FPN’s feature maps have different
sizes, they can be all fed to the same FCNs.</p>

<p>The first FCN is the <strong>regression branch</strong>. It predicts $K x 4$
(<code>x1</code>, <code>y1</code>, <code>x2</code>, <code>y2</code> for each anchor) values. Those values are <strong>deltas</strong> that
slightly modify the original anchors so they fit the potential objects
better. All the potential objects will now have coordinates of the type:</p>

<pre><code>(x1 + dx1, y1 + dy1, x2 + dx2, y2 + dy2)
</code></pre>

<p>With <code>x?</code> and <code>y?</code>, the fixed coordinates of the anchors, and <code>dx?</code>, <code>dy?</code>,
the deltas produced by the regression branch.</p>

<p>We now have the final coordinates for all objects — that is, all potential
objects. They are not yet classified as background or car, truck, etc.</p>

<p>The second FCN is the <strong>classification branch</strong>. It is a multi-label problem
where the classifier predicts $K x N$ ($N$ being the number of classes) potential
objects with sigmoid.</p>

<h4 id="2-5-removing-duplicates">2.5. Removing duplicates</h4>

<p>At this point we have $K x 4$ coordinates and $K x N$ class scores. We now have
a problem: it is common to detect, for the same class, several boxes for a
same object!</p>

<figure>

<img src="/figures/nms_before.png" alt="*Figure 6: Several boxes have been detected for a single car.*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 6: Several boxes have been detected for a single car.</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>Therefore, for each class (even if it’s not the highest scoring class)
we apply a <strong>Non-max suppression</strong>. Tensorflow provides a <a href="https://www.tensorflow.org/api_docs/python/tf/image/non_max_suppression" target="_blank">function</a> to do it:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python">tf<span style="color:#f92672">.</span>image<span style="color:#f92672">.</span>non_max_suppression(boxes, scores, max_output_size, iou_threshold)</code></pre></div>

<p>The main gist of this method is that it will remove overlapping boxes
(such as in Figure 6) to keep only one. It also using the <code>scores</code> to keep the
most probable box.</p>

<p>A general comment on the input parameter of the Tensorflow method above:
The <code>max_output_size</code> corresponds to the maximum number of boxes we want at the
end — let’s say 300. The <code>iou_threshold</code> is a float between 0 and 1, describing
the maximum ratio of overlapping that is accepted.</p>

<figure>

<img src="/figures/nms_after.png" alt="*Figure 7: Figure 6 after the non-max-suppression has been applied.*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 7: Figure 6 after the non-max-suppression has been applied.</em>
    
    
    
  </p> 
</figcaption>

</figure>

<h4 id="2-6-keeping-the-most-probable-class">2.6. Keeping the most probable class</h4>

<p>Duplicate boxes for the same class at the same place are now removed. For each
of the remaining boxes, we are keeping only the highest-scoring class
(car, truck, etc.). If none of the classes have a score above a fixed threshold
(we used $0.4$), it’s considered to be part of the background.</p>

<h4 id="2-7-the-focal-loss">2.7. The Focal Loss</h4>

<p>All this may sound complicated, but it’s nothing new — it’s not enough to
have good accuracy. The real improvement from RetinaNet is its loss: the
<strong>Focal Loss</strong>. Single-stage architectures that don’t have potential objects
pre-selection are overwhelmed with the high frequency of background objects.
The Focal Loss deals with it by according a low weight to well-classified
examples, usually the background.</p>

<figure>

<img src="/figures/focal_loss1.png" alt="*Figure 8: We define Pt, the confidence to be right*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 8: We define Pt, the confidence to be right</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>In Figure 8, we define $p_t$, the confidence to be right in a binary
classification.</p>

<figure>

<img src="/figures/focal_loss2.png" alt="*Figure 9: The Focal Loss*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 9: The Focal Loss</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>In Figure 9, we module the cross entropy loss $-\log(p_t)$ by a factor
$(1 — p_t)^\gamma$. Here, $\gamma$ is a modulating factor oscillating between
0 and 5. The well-classified examples have a high $p_t$ , and thus a low factor.
Therefore, the loss for well-classified examples is low and forces the
model learn on harder examples. You can see in Figure 10 how much the loss is
affected.</p>

<figure>

<img src="/figures/focal_loss3.png" alt="*Figure 10: The focal loss under various modulating factors*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 10: The focal loss under various modulating factors</em>
    
    
    
  </p> 
</figcaption>

</figure>

<h3 id="3-implementation">3. Implementation</h3>

<p>We used the excellent Keras <a href="https://github.com/fizyr/keras-retinanet" target="_blank">implementation</a>
of RetinaNet by Fizyr. We also wrote a new generator, taking Pandas’ DataFrames
instead of CSV files.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DfGenerator</span>(CSVGenerator):
    <span style="color:#66d9ef">def</span> __init__(self, df, class_mapping, cols, base_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, <span style="color:#f92672">**</span>kwargs):
        <span style="color:#e6db74">&#34;&#34;&#34;Custom generator intended to work with in-memory Pandas&#39; dataframe.
</span><span style="color:#e6db74">
</span><span style="color:#e6db74">        Arguments:
</span><span style="color:#e6db74">            df: Pandas DataFrame containing paths, labels, and bounding boxes.
</span><span style="color:#e6db74">            class_mapping: Dict mapping label_str to id_int.
</span><span style="color:#e6db74">            cols: Dict Mapping &#39;col_{filename/label/x1/y1/x2/y2} to corresponding df col.
</span><span style="color:#e6db74">        &#34;&#34;&#34;</span>
        self<span style="color:#f92672">.</span>base_dir <span style="color:#f92672">=</span> base_dir
        self<span style="color:#f92672">.</span>cols <span style="color:#f92672">=</span> cols
        self<span style="color:#f92672">.</span>classes <span style="color:#f92672">=</span> class_mapping
        self<span style="color:#f92672">.</span>labels <span style="color:#f92672">=</span> {v: k <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>classes<span style="color:#f92672">.</span>items()}

        self<span style="color:#f92672">.</span>image_data <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_read_data(df)
        self<span style="color:#f92672">.</span>image_names <span style="color:#f92672">=</span> list(self<span style="color:#f92672">.</span>image_data<span style="color:#f92672">.</span>keys())

        Generator<span style="color:#f92672">.</span>__init__(self, <span style="color:#f92672">**</span>kwargs)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_read_classes</span>(self, df):
        <span style="color:#66d9ef">return</span> {row[<span style="color:#ae81ff">0</span>]: row[<span style="color:#ae81ff">1</span>] <span style="color:#66d9ef">for</span> _, row <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>iterrows()}

    <span style="color:#66d9ef">def</span> __len__(self):
        <span style="color:#66d9ef">return</span> len(self<span style="color:#f92672">.</span>image_names)

    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">_read_data</span>(self, df):
        data <span style="color:#f92672">=</span> {}
        <span style="color:#66d9ef">for</span> _, row <span style="color:#f92672">in</span> df<span style="color:#f92672">.</span>iterrows():
            img_file, class_name <span style="color:#f92672">=</span> row[self<span style="color:#f92672">.</span>cols[<span style="color:#e6db74">&#39;col_filename&#39;</span>]], row[self<span style="color:#f92672">.</span>cols[<span style="color:#e6db74">&#39;col_label&#39;</span>]]
            x1, y1 <span style="color:#f92672">=</span> row[self<span style="color:#f92672">.</span>cols[<span style="color:#e6db74">&#39;col_x1&#39;</span>]], row[self<span style="color:#f92672">.</span>cols[<span style="color:#e6db74">&#39;col_y1&#39;</span>]]
            x2, y2 <span style="color:#f92672">=</span> row[self<span style="color:#f92672">.</span>cols[<span style="color:#e6db74">&#39;col_x2&#39;</span>]], row[self<span style="color:#f92672">.</span>cols[<span style="color:#e6db74">&#39;col_y2&#39;</span>]]

            <span style="color:#66d9ef">if</span> img_file <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> data:
                data[img_file] <span style="color:#f92672">=</span> []

            <span style="color:#75715e"># Image without annotations</span>
            <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> isinstance(class_name, str) <span style="color:#f92672">and</span> np<span style="color:#f92672">.</span>isnan(class_name):
                <span style="color:#66d9ef">continue</span>

            data[img_file]<span style="color:#f92672">.</span>append({
                <span style="color:#e6db74">&#39;x1&#39;</span>: int(x1), <span style="color:#e6db74">&#39;x2&#39;</span>: int(x2),
                <span style="color:#e6db74">&#39;y1&#39;</span>: int(y1), <span style="color:#e6db74">&#39;y2&#39;</span>: int(y2),
                <span style="color:#e6db74">&#39;class&#39;</span>: class_name
            })

<span style="color:#66d9ef">return</span> data</code></pre></div>

<p>As you can see, <strong>images without annotations are kept in the training phase</strong>.
They still help the training of our algorithm, as it forces the algorithm to
not see cars everywhere (even where there aren’t any).</p>

<p>We used a pre-trained RetinaNet on <a href="http://cocodataset.org/" target="_blank">COCO</a> and then
fine-tuned it for the COWC dataset. Only the two FCNs are retrained for this
new task, while the ResNet backbone and the FPN are frozen.</p>

<p>You can see in the code block below how to load the RetinaNet and compile it.
Note that it is important to add <code>skip_mismatch=True</code> when loading the weights!
The weights were created on COCO with 80 classes, but in our case we only have
1 class, thus the number of anchors is not the same.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">load_retinanet</span>(weights, n_classes, freeze<span style="color:#f92672">=</span>True):
    modifier <span style="color:#f92672">=</span> freeze_model <span style="color:#66d9ef">if</span> freeze <span style="color:#66d9ef">else</span> None

    model <span style="color:#f92672">=</span> resnet50_retinanet(num_classes<span style="color:#f92672">=</span>num_classes, modifier<span style="color:#f92672">=</span>modifier)
    model<span style="color:#f92672">.</span>load_weights(weights, by_name<span style="color:#f92672">=</span>True, skip_mismatch<span style="color:#f92672">=</span>True)

    <span style="color:#66d9ef">return</span> model


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compile</span>(model):
    model<span style="color:#f92672">.</span>compile(
        loss<span style="color:#f92672">=</span>{
            <span style="color:#e6db74">&#39;regression&#39;</span>    : keras_retinanet<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>smooth_l1(),
            <span style="color:#e6db74">&#39;classification&#39;</span>: keras_retinanet<span style="color:#f92672">.</span>losses<span style="color:#f92672">.</span>focal()
        },
        optimizer<span style="color:#f92672">=</span>optimizers<span style="color:#f92672">.</span>adam(lr<span style="color:#f92672">=</span>configs[<span style="color:#e6db74">&#39;lr&#39;</span>], clipnorm<span style="color:#f92672">=</span><span style="color:#ae81ff">0.001</span>)
    )


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">train</span>(model, train_gen, val_gen, callbacks, n_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>):
    <span style="color:#e6db74">&#34;&#34;&#34;train_gen and val_gen are instances of DfGenerator.&#34;&#34;&#34;</span>
    model<span style="color:#f92672">.</span>fit_generator(
        train_gen,
        steps_per_epoch<span style="color:#f92672">=</span>len(train_gen),
        validation_data<span style="color:#f92672">=</span>val_gen,
        validation_steps<span style="color:#f92672">=</span>len(val_gen),
        callbacks<span style="color:#f92672">=</span>callbacks,
        epochs<span style="color:#f92672">=</span>n_epochs,
        verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>
)</code></pre></div>

<p>There is something we still need to deal with, which is the <strong>massive weight
of each image</strong>. Images from the COWC dataset are up to 4 square kilometers,
or 13k pixel wide and high. Those big images weigh 300mb. It is impracticable
to feed such large images to our RetinaNet. Therefore, we cut the images in
patches of 1000x1000 pixels (or 150x150 meters).</p>

<p>However, it would be stupid to miss cars because they’d been cut between two
patches. So to avoid this problem, we made a sliding window of 1000x1000 pixels
that moves by steps of 800 pixels. That way, there is a 200-pixel-wide overlap
between two adjacent patches.</p>

<p>This leads to another problem: we may detect cars twice. To remove duplicates,
we applied non-max suppression when binding together the small patches. Indeed,
that means we have a non-max suppression twice: after the RetinaNet and when
binding together the small patches. For the second non-max suppression, we used
a Numpy version of the algorithm. You can either use a <a href="https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/" target="_blank">fast &amp; vectorized version</a>
by PyImageSearch, or the following naive version:</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">jaccard</span>(a, b):
    <span style="color:#e6db74">&#34;&#34;&#34;Compute the jaccard score between box a and box b.&#34;&#34;&#34;</span>
    side1 <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0</span>, min(a[<span style="color:#ae81ff">2</span>], b[<span style="color:#ae81ff">2</span>]) <span style="color:#f92672">-</span> max(a[<span style="color:#ae81ff">0</span>], b[<span style="color:#ae81ff">0</span>]))
    side2 <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0</span>, min(a[<span style="color:#ae81ff">3</span>], b[<span style="color:#ae81ff">3</span>]) <span style="color:#f92672">-</span> max(a[<span style="color:#ae81ff">1</span>], b[<span style="color:#ae81ff">1</span>]))
    inter <span style="color:#f92672">=</span> side1 <span style="color:#f92672">*</span> side2

    area_a <span style="color:#f92672">=</span> (a[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">-</span> a[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">*</span> (a[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">-</span> a[<span style="color:#ae81ff">1</span>])
    area_b <span style="color:#f92672">=</span> (b[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">-</span> b[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">*</span> (b[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">-</span> b[<span style="color:#ae81ff">1</span>])

    union <span style="color:#f92672">=</span> area_a <span style="color:#f92672">+</span> area_b <span style="color:#f92672">-</span> inter

    <span style="color:#66d9ef">return</span> inter <span style="color:#f92672">/</span> union


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">naive_nms</span>(boxes, scores, threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">0.4</span>):
    scores_idx <span style="color:#f92672">=</span> scores<span style="color:#f92672">.</span>argsort()[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>] <span style="color:#75715e"># Keep highest scores first</span>
    boxes <span style="color:#f92672">=</span> boxes[scores_idx]

    indices_to_skip <span style="color:#f92672">=</span> set()
    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(boxes<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]):
        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(boxes<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]):
            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> j <span style="color:#f92672">or</span> j <span style="color:#f92672">in</span> indices_to_skip:
                <span style="color:#66d9ef">continue</span>

            <span style="color:#66d9ef">if</span> jaccard(boxes[i], boxes[j]) <span style="color:#f92672">&gt;</span> threshold:
                indices_to_skip<span style="color:#f92672">.</span>add(j)

    mask <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ones(boxes<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], np<span style="color:#f92672">.</span>bool)
    mask[np<span style="color:#f92672">.</span>array(list(indices_to_skip))] <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
    <span style="color:#66d9ef">return</span> boxes[mask]</code></pre></div>

<p>When dealing with aerial imagery, we can use a lot of data augmentation.
First of all, we can flip the horizontal axis and the vertical axis. We can also
rotate the image by any angle. If the imagery’s scale is not uniform (the
distance drone-to-ground may not be constant), it is also useful to randomly
scale down and up the pictures.</p>

<h3 id="4-results">4. Results</h3>

<p>You can see on Figures 11 and 12 below how our RetinaNet behaves on this
unseen image of Salt Lake City.</p>

<figure>

<img src="/figures/nato_result_large.png" alt="*Figure 11: 13,000 detected cars in a 4 square kilometer area of Salt Lake City*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 11: 13,000 detected cars in a 4 square kilometer area of Salt Lake City</em>
    
    
    
  </p> 
</figcaption>

</figure>

<figure>

<img src="/figures/nato_result_zoom.png" alt="*Figure 12: A zoom in of Figure 11*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 12: A zoom in of Figure 11</em>
    
    
    
  </p> 
</figcaption>

</figure>

<h3 id="5-are-we-good">5. Are we good?</h3>

<p>How can we evaluate our performance?</p>

<p>Accuracy is not enough; we need to see how many <strong>false positives</strong> and <strong>false
negatives</strong> we get. If we detect cars everywhere, we’d have a lot of false
positive, but if we miss most of the cars, that’s a lot of false negative.</p>

<p>The <strong>recall</strong> measures the former while the <strong>precision</strong> measures the latter. Finally,
the <strong>f1-score</strong> is a combination of those two metrics.</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">compute_metrics</span>(true_pos, false_pos, false_neg):
    <span style="color:#e6db74">&#34;&#34;&#34;Compute the precision, recall, and f1 score.&#34;&#34;&#34;</span>
    precision <span style="color:#f92672">=</span> true_pos <span style="color:#f92672">/</span> (true_pos <span style="color:#f92672">+</span> false_pos)
    recall <span style="color:#f92672">=</span> true_pos <span style="color:#f92672">/</span> (true_pos <span style="color:#f92672">+</span> false_neg)

    <span style="color:#66d9ef">if</span> precision <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">or</span> recall <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>:
        <span style="color:#66d9ef">return</span> precision, recall, <span style="color:#ae81ff">0</span>

    f1 <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> precision <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> recall)
    <span style="color:#66d9ef">return</span> precision, recall, f1</code></pre></div>

<p>However, we are not expecting our RetinaNet to detect the cars at the exact
right pixels. Therefore, we are computing the <strong>Jaccard Index</strong> of the detected
cars and the ground-truth cars, and if it is more than a chosen threshold, we
consider that the car was rightfully detected. Note that the Jaccard index is
often also (blandly) called <strong>Intersection-over-Union</strong> (IoU):</p>

<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">jaccard</span>(box_a, box_b):
    <span style="color:#e6db74">&#34;&#34;&#34;box_X is a tuple of the shape (x1, y1, x2, y2).&#34;&#34;&#34;</span>
    side1 <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0</span>, min(a[<span style="color:#ae81ff">2</span>], b[<span style="color:#ae81ff">2</span>]) <span style="color:#f92672">-</span> max(a[<span style="color:#ae81ff">0</span>], b[<span style="color:#ae81ff">0</span>]))
    side2 <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0</span>, min(a[<span style="color:#ae81ff">3</span>], b[<span style="color:#ae81ff">3</span>]) <span style="color:#f92672">-</span> max(a[<span style="color:#ae81ff">1</span>], b[<span style="color:#ae81ff">1</span>]))
    inter <span style="color:#f92672">=</span> side1 <span style="color:#f92672">*</span> side2

    area_a <span style="color:#f92672">=</span> (a[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">-</span> a[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">*</span> (a[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">-</span> a[<span style="color:#ae81ff">1</span>])
    area_b <span style="color:#f92672">=</span> (b[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">-</span> b[<span style="color:#ae81ff">0</span>]) <span style="color:#f92672">*</span> (b[<span style="color:#ae81ff">3</span>] <span style="color:#f92672">-</span> b[<span style="color:#ae81ff">1</span>])
    union <span style="color:#f92672">=</span> area_a <span style="color:#f92672">+</span> area_b <span style="color:#f92672">-</span> inter

    <span style="color:#66d9ef">return</span> inter <span style="color:#f92672">/</span> union


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">is_valid</span>(box_pred, box_true, threshold<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>):
    <span style="color:#66d9ef">return</span> jaccard(box_red, box_true) <span style="color:#f92672">&gt;=</span> threshold</code></pre></div>

<figure>

<img src="/figures/nato_result_color.png" alt="*Figure 13: True Positive (green), False Positive (yellow), and False Negative (red)*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 13: True Positive (green), False Positive (yellow), and False Negative (red)</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>You can see a sample on Figure 13 where true positives, false positives, and
false negatives have been plotted.</p>

<p>Note that among the four false positives, two of them are garbage bins, one is
a duplicate, and one is actually&hellip; a car! Indeed, as in every dataset, there
may be some errors in the ground-truth annotations.</p>

<p>On Figure 12, the f1-score is $0.91$. Usually in more urban environments the
f1-score is around $0.95$. The main mistake our model makes is considering
ventilation shafts on tops of buildings to be cars. To the model’s defense,
without knowledge of building, it’s quite hard to see that.</p>

<h3 id="6-conclusion">6. Conclusion</h3>

<p>For the NATO challenge, we didn’t only use car detection from aerial imagery,
but it was the main technical part of the project.</p>

<p>Oh&hellip; Did we forget to tell you the challenge results?</p>

<p>Three prizes were awarded: The NATO prize (with a trip to Norfolk), the
France prize (with $25k), and the Germany prize (with a trip to Berlin).</p>

<p>We won both the NATO and France prize!</p>

<figure>

<img src="/figures/nato_award.png" alt="*Figure 14: General Maurice, Supreme Commander Mercier, and our team*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 14: General Maurice, Supreme Commander Mercier, and our team</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>Thanks to <a href="https://medium.com/@HichamEB?source=post_page" target="_blank">Hicham El Boukkouri</a> &amp;
<a href="https://medium.com/@dsleo?source=post_page" target="_blank">Léo Dreyfus-Schmidt</a> for their review
of this blog post.</p>

<p>Note that this post was at first published on <a href="https://medium.com/data-from-the-trenches/object-detection-with-deep-learning-on-aerial-imagery-2465078db8a9" target="_blank">Dataiku&rsquo;s technical blog on Medium</a>.</p>

<p><strong>Update</strong>: Our model has been improved recently by <a href="https://www.mdpi.com/1424-8220/20/22/6485" target="_blank">Stuparu et al. 2020</a> and published in Sensors!</p>

    </div>

    




    

    
    

    
    <div class="article-widget">
      <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/normalization/" rel="next">Normalization in Deep Learning</a>
  </div>
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/3-small-but-powerful-cnn/" rel="prev">3 Small but Powerful Convolutional Neural Networks</a>
  </div>
  
</div>

    </div>
    

    
<button id="disqus-show"
  style="background-color: rgb(20, 22, 34); color: rgb(248, 248, 242); border-style: none">
  <i class="fa fa-angle-double-right"></i>&nbsp;Comments
</button>
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "arthurdouillard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


<script type="text/javascript">
  var a=document.querySelector("#disqus-show")
  var b=document.querySelector("#comments")

  b.style.display = "none";

  a.addEventListener("click",
    function(){
      ""==b.style.display?(b.style.display="none", a.innerHTML="<i class=\"fa fa fa-angle-double-right\"/i>&nbsp;Comments"):(b.style.display="",a.innerHTML="<i class=\"fa fa-angle-double-down\"></i>&nbsp;Comments");
      a.style = "background-color: rgb(20, 22, 34); color: rgb(248, 248, 242); border-style: none";
      }
  );
</script>

<style>

</style>


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>





<script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>





<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>





<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>




<script src="/js/hugo-academic.js"></script>














<script src="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/2.10.2/instantsearch.min.js" integrity="sha256-gFCtPk/sonctyfwYOgjrPoWApQ0rqB6ezBBZ7p32yGc=" crossorigin="anonymous"></script>
<script>
  const content_type = {
    'post': "Posts",
  'project': { { i18n "projects" } },
  'publication' : { { i18n "publications" } },
  'talk' : { { i18n "talks" } }
      };

  function getTemplate(templateName) {
    return document.querySelector(`#${templateName}-template`).innerHTML;
  }

  const options = {
    appId: "",
  apiKey: { { .Site.Params.search.algolia.api_key } },
  indexName: { { .Site.Params.search.algolia.index_name } },
  routing: true,
    searchParameters: {
    hitsPerPage: 10
  },
  searchFunction: function(helper) {
    let searchResults = document.querySelector('#search-hits')
    if (helper.state.query === '') {
      searchResults.style.display = 'none';
      return;
    }
    helper.search();
    searchResults.style.display = 'block';
  }
      };

  const search = instantsearch(options);

  
  search.addWidget(
    instantsearch.widgets.searchBox({
      container: '#search-box',
      placeholder: "Search..."
        })
  );

  
  search.addWidget(
    instantsearch.widgets.infiniteHits({
      container: '#search-hits',
      templates: {
        empty: '<div class="search-no-results">' + "No results found" + '</div>',
      item: getTemplate('search-hit')
          },
      cssClasses: {
      showmoreButton: 'btn btn-primary btn-outline'
    }
        })
  );

  
  search.on('render', function () {
    $('.search-hit-type').each(function (index) {
      let content_key = $(this).text();
      if (content_key in content_type) {
        $(this).text(content_type[content_key]);
      }
    });
  });

  
  search.start();
</script>


</body>

</html>

