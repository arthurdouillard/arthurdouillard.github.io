<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 3.0.0">
  <meta name="generator" content="Hugo 0.49.2" />
  <meta name="author" content="Arthur Douillard">

  
  
  
  
    
  
  <meta name="description" content="Why deep neural networks confidence are flawed and how to fix it.">

  
  <link rel="alternate" hreflang="en-us" href="/post/miscalibration/">

  


  

  

  

  
  
  
  <meta name="theme-color" content="#e0491f">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.4.1/css/all.css" integrity="sha384-5sAR7xN1Nv6T6+dT2mhtzEpVJvfS3NScPQTrOxhwjIuvcA67KV2R5Jz6kr4abQsz" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin="anonymous">

    
    
    
      
    
    

    

    

  

  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Montserrat:400,700|Roboto:400,400italic,700|Roboto+Mono">
  

  <link rel="stylesheet" href="/styles.css">
  
  <link rel="stylesheet" href="/css/search_bar.css">
  

  
  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-102629480-1', 'auto');
      
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  
  

  
  <link rel="alternate" href="/index.xml" type="application/rss+xml" title="Arthur Douillard">
  <link rel="feed" href="/index.xml" type="application/rss+xml" title="Arthur Douillard">
  

  <link rel="manifest" href="/site.webmanifest">
  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/icon-192.png">

  <link rel="canonical" href="/post/miscalibration/">

  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@Ar_Douilllard">
  <meta property="twitter:creator" content="@Ar_Douilllard">
  
  <meta property="og:site_name" content="Arthur Douillard">
  <meta property="og:url" content="/post/miscalibration/">
  <meta property="og:title" content="How To Be Confident In Your Neural Network Confidence | Arthur Douillard">
  <meta property="og:description" content="Why deep neural networks confidence are flawed and how to fix it.">
  
  
    
  <meta property="og:image" content="/img/icon-192.png">
  <meta property="og:locale" content="en-us">
  
  <meta property="article:published_time" content="2019-05-30T00:00:00&#43;02:00">
  
  <meta property="article:modified_time" content="2019-05-30T00:00:00&#43;02:00">
  

  

  
<meta name="twitter:title" content="How To Be Confident In Your Neural Network Confidence" />



<meta name="twitter:description" content="Why deep neural networks confidence are flawed and how to fix it." />



<meta name="twitter:image" content="/figures/miscalibration.png" />



<meta property="og:image" content=/figures/miscalibration.png />



<meta property="og:description" content="Why deep neural networks confidence are flawed and how to fix it." />


<link rel="icon" type="image/png" href="/img/favicon32.png">
<link rel="apple-touch-icon" type="image/png" href="/img/favicon180.png">


  <title>How To Be Confident In Your Neural Network Confidence | Arthur Douillard</title>

</head>
<body id="top" data-spy="scroll" data-target="#TableOfContents" data-offset="71" >

<nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id="navbar-main">
  <div class="container">

    
      <a class="navbar-brand" href="/">Arthur Douillard</a>
      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
        <span><i class="fas fa-bars"></i></span>
      </button>
      

    
    <div class="collapse navbar-collapse" id="navbar">

      
      
      <ul class="navbar-nav ml-auto">
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#about">
            
            <span>Home</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#publications">
            
            <span>Publications</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#posts">
            
            <span>Posts</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#projects">
            
            <span>Projects</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#talks_list">
            
            <span>Talks</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/#teaching">
            
            <span>Teaching</span>
            
          </a>
        </li>

        
        

        

        
        
        
          
        

        <li class="nav-item">
          <a class="nav-link" href="/books">
            
            <span>Books</span>
            
          </a>
        </li>

        
        

      

        

        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">

  

















<div class="article-container pt-3">
  <h1 itemprop="name">How To Be Confident In Your Neural Network Confidence</h1>

  

  
  

<div class="article-metadata">

  
  
  <span itemscope itemprop="author" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Arthur Douillard">
  </span>
  

  <span class="article-date">
    
    <meta content="2019-05-30 00:00:00 &#43;0200 CEST" itemprop="datePublished">
    <time datetime="2019-05-30 00:00:00 &#43;0200 CEST" itemprop="dateModified">
      30 May, 2019
    </time>
  </span>
  <span itemscope itemprop="publisher" itemtype="http://schema.org/Person">
    <meta itemprop="name" content="Arthur Douillard">
  </span>

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  
  

  
  

  

  
  

  

</div>

  
</div>



  <div class="article-container">

    <div class="article-style" itemprop="articleBody">
      

<p>Those notes are based on the research paper
&ldquo;<strong>On Calibration of Modern Neural Networks</strong>&rdquo; by <a href="https://arxiv.org/abs/1706.04599" target="_blank">(Guo et al, 2017.)</a>.</p>

<h1 id="how-to-be-confident-in-your-neural-network-confidence">How To Be Confident In Your Neural Network Confidence?</h1>

<p>Very large and deep models, as ResNet, are far more accurate than their older counterparts, as LeNet, on computer vision datasets such as CIFAR100. <strong>However while
they are better at classifying images, we are less confident in their own confidence!</strong></p>

<p>Most neural networks for classification uses as last activation a softmax: it
produces a distribution of probabilities for each target (cat, dog, boat, etc.).
These probabilities sum to one. We may expect that if for a given image, our
model associate a score of 0.8 to the target ‘boat’, our model is confident at
80% that this is the right target.</p>

<p>Over 100 images that were detected as boat, we can expect approximately that 80
images are indeed real boats, while the 20 remaining were false positives.</p>

<p>It was true for shallow model as LeNet but as newer models gained in accuracy
<strong>their confidences became decorrelated from the “real confidence”</strong>.</p>

<p>This does not work anymore for deep neural networks:</p>

<figure>

<img src="/figures/miscalibration.png" alt="*Figure 1: Miscalibration in modern neural network [[source](https://arxiv.org/abs/1706.04599)]*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 1: Miscalibration in modern neural network [<a href="https://arxiv.org/abs/1706.04599" target="_blank">source</a>]</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>As you can see, older networks as LeNet had a low accuracy (55%) but their
confidence was actually in line with the accuracy! Modern networks as ResNet have
a higher accuracy (69%) but as showed in figure 1, they are over-confident.</p>

<p>This discrepancy between the model confidence and the actual accuracy is called
<strong>miscalibration</strong>.</p>

<h2 id="why-it-is-important">Why It Is Important</h2>

<p>Outside of toy datasets used in the academy, it can be useful to know how much
confident our model is.</p>

<p>Imagine we have a model predicting frauds. We want to flag some transaction as
suspicious based on the model confidence that it is a fraud.
We could definitely compute an optimal threshold on the validation set, and then
every confidence above this threshold would be flagged as a fraud. However
this computed threshold could be 0.2 or 0.9 but would probably make much sense to a human.</p>

<p>A model without miscalibration would help the users to interpret better the
predictions.</p>

<h2 id="why-it-happens">Why It Happens</h2>

<p>The authors explores empirically what are the causes of this miscalibration in
modern networks.</p>

<p>They measure the miscalibration with the <strong>E</strong>xpected <strong>C</strong>alibration <strong>E</strong>rror (ECE):
the average difference between the confidence and the accuracy. This metric should
be minimized.</p>

<h3 id="higher-capacity-cross-entropy">Higher Capacity &amp; Cross-Entropy</h3>

<p>The most interpretable cause of the miscalibration is the increase of capacity
and the cross-entropy loss.</p>

<p>Model capacity can be seen as a measurement of how much a model can memorize.
With an infinite capacity, the model could simply learn by heart the whole
training dataset. A trade-off has to be made between a low and high capacity.
If it is too low the model wouldn’t be able to learn essential features of your
data. If it is too high, the model will learn too much and overfit instead of
generalize. Indeed comprehension is compression: by leaving few enough capacity
the model has to pick up the most representative features (pretty much in the
same way PCA works) and will then generalize better (but too few capacity &amp; no
learning will happen!).</p>

<p>The new architectures such as ResNet have way more capacity than the older
LeNet (25M parameters for the former and 20k for the latter). This high
capacity led to better accuracy: the training set can almost be learned by heart.</p>

<p>In addition the models optimizes the cross-entropy loss that force them to be
right AND to be very confident. The higher capacity helped to lower the
cross-entropy loss and thus encourages deep neural networks to be over-confident.
As you’ve seen on figure 1, the new models are now over-confident.</p>

<figure>

<img src="/figures/miscalibration_capacity.png" alt="*Figure 2: More capacity (in depth or width) increases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 2: More capacity (in depth or width) increases the miscalibration. [<a href="https://arxiv.org/abs/1706.04599" target="_blank">source</a>]</em>
    
    
    
  </p> 
</figcaption>

</figure>

<h3 id="the-mysterious-batch-normalization">The Mysterious Batch Normalization</h3>

<p>Batch Normalization normalizes the tensors in a network. It greatly improves the
training convergence &amp; the final performance. Why exactly it works that well
is still a bit undefined (<a href="/posts/normalization">see more</a>).</p>

<p>The authors remark empirically that using Batch Normalization increased the miscalibration
but could not find an exact reason why.</p>

<figure>

<img src="/figures/miscalibration_bn.png" alt="*Figure 3: Batch Normalization increases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 3: Batch Normalization increases the miscalibration. [<a href="https://arxiv.org/abs/1706.04599" target="_blank">source</a>]</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>Could the help given by this method in training facilitate the over-confidence?</p>

<h3 id="regularization">Regularization</h3>

<p>The weight decay is an additional loss that penalizes the L2 norm of the weights.
The larger the weights, the bigger the norm and thus the loss. By constraining the weights
magnitude, it avoid the model finding extreme weight values that could make it overfit.</p>

<p>The authors found that increasing the regularization decreases the model accuracy
as expected. However it also decreased the miscalibration! The answer is then again
because regularization avoid overfitting &amp; thus over-confidence.</p>

<figure>

<img src="/figures/miscalibration_reg.png" alt="*Figure 4: More regularization decreases the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 4: More regularization decreases the miscalibration. [<a href="https://arxiv.org/abs/1706.04599" target="_blank">source</a>]</em>
    
    
    
  </p> 
</figcaption>

</figure>

<h2 id="how-to-fix-miscalibration">How To Fix Miscalibration</h2>

<p>This article&rsquo;s title, &ldquo;<em>How To Be Confident In Your Neural Network Confidence</em>&rdquo;,
led you to believe that you would discover how to reduce miscalibration.</p>

<p>You&rsquo;re not going to reduce the capacity, remove Batch Normalization, and increase
the regularization: you&rsquo;ll hurt too much your precious accuracy.</p>

<p>Fortunately there are post-processing solutions. The authors describe several
but the most effective one is also the simplest: <strong>Temperature Scaling</strong>.</p>

<p>Instead of computing the softmax like this:</p>

<p>$$\text{softmax}(x)_i = \frac{e^{y_i}}{\Sigma_j^N e^{y_j}}$$</p>

<p>All the logits (values just before the final activation, here softmax) are divided
by the same value called temperature:</p>

<p>$$\text{softmax}(x)_i = \frac{e^{\frac{y_i}{T}}}{\Sigma_j^N e^{\frac{y_j}{T}}}$$</p>

<p>Similar to (<a href="https://arxiv.org/abs/1503.02531" target="_blank">Hinton et al, 2015.</a>), this temperature
<em>softens the probabilities</em>.</p>

<p>Extreme probabilities (high confidence) are more decreased than smaller probabilities
(low confidence). The authors find the optimal temperature by minimizing the
Expected Calibration Error on the validation set.</p>

<p>The miscalibration is almost entirely corrected:</p>

<figure>

<img src="/figures/miscalibration_tempscaling.png" alt="*Figure 5: Temperature Scaling fixes the miscalibration. [[source](https://arxiv.org/abs/1706.04599)]*" />



<figcaption data-pre="Figure " data-post=":" >
  <p style="text-align: center">
    <em>Figure 5: Temperature Scaling fixes the miscalibration. [<a href="https://arxiv.org/abs/1706.04599" target="_blank">source</a>]</em>
    
    
    
  </p> 
</figcaption>

</figure>

<p>Another cool feature of Temperature Scaling: because all logits are divided by the
same value, and that softmax is a <a href="https://en.wikipedia.org/wiki/Monotonic_function" target="_blank">monotone function</a>,
the accuracy remains unchanged!</p>

    </div>

    




    

    
    

    
    <div class="article-widget">
      <div class="post-nav">
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/incremental-learning/" rel="next">Learning Deep Neural Networks incrementally forever</a>
  </div>
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/normalization/" rel="prev">Normalization in Deep Learning</a>
  </div>
  
</div>

    </div>
    

    
<button id="disqus-show"
  style="background-color: rgb(20, 22, 34); color: rgb(248, 248, 242); border-style: none">
  <i class="fa fa-angle-double-right"></i>&nbsp;Comments
</button>
<section id="comments">
  <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "arthurdouillard" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</section>


<script type="text/javascript">
  var a=document.querySelector("#disqus-show")
  var b=document.querySelector("#comments")

  b.style.display = "none";

  a.addEventListener("click",
    function(){
      ""==b.style.display?(b.style.display="none", a.innerHTML="<i class=\"fa fa fa-angle-double-right\"/i>&nbsp;Comments"):(b.style.display="",a.innerHTML="<i class=\"fa fa-angle-double-down\"></i>&nbsp;Comments");
      a.style = "background-color: rgb(20, 22, 34); color: rgb(248, 248, 242); border-style: none";
      }
  );
</script>

<style>

</style>


  </div>
</article>

<div class="container">
  <footer class="site-footer">
  

  <p class="powered-by">
    &copy; 2018 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" id="back_to_top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

</div>


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>





<script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        CommonHTML: { linebreaks: { automatic: true } },
        tex2jax: { inlineMath: [ ['$', '$'], ['\\(','\\)'] ], displayMath: [ ['$$','$$'], ['\\[', '\\]'] ], processEscapes: false },
        TeX: { noUndefined: { attributes: { mathcolor: 'red', mathbackground: '#FFEEEE', mathsize: '90%' } } },
        messageStyle: 'none'
      });
    </script>





<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha512-+NqPlbbtM1QqiK8ZAo4Yrj2c4lNQoGv8P79DPtKzj++l5jnN39rHA/xsqn8zE9l0uSoxaCdrOgFs6yjyfbBxSg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.3/imagesloaded.pkgd.min.js" integrity="sha512-umsR78NN0D23AzgoZ11K7raBD+R6hqKojyBZs1w8WvYlsI+QuKRGBx3LFCwhatzBunCjDuJpDHwxD13sLMbpRA==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin="anonymous"></script>





<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS_CHTML-full" integrity="sha256-GhM+5JHb6QUzOQPXSJLEWP7R73CbkisjzK5Eyij4U9w=" crossorigin="anonymous" async></script>




<script src="/js/hugo-academic.js"></script>














<script src="https://cdnjs.cloudflare.com/ajax/libs/instantsearch.js/2.10.2/instantsearch.min.js" integrity="sha256-gFCtPk/sonctyfwYOgjrPoWApQ0rqB6ezBBZ7p32yGc=" crossorigin="anonymous"></script>
<script>
  const content_type = {
    'post': "Posts",
  'project': { { i18n "projects" } },
  'publication' : { { i18n "publications" } },
  'talk' : { { i18n "talks" } }
      };

  function getTemplate(templateName) {
    return document.querySelector(`#${templateName}-template`).innerHTML;
  }

  const options = {
    appId: "",
  apiKey: { { .Site.Params.search.algolia.api_key } },
  indexName: { { .Site.Params.search.algolia.index_name } },
  routing: true,
    searchParameters: {
    hitsPerPage: 10
  },
  searchFunction: function(helper) {
    let searchResults = document.querySelector('#search-hits')
    if (helper.state.query === '') {
      searchResults.style.display = 'none';
      return;
    }
    helper.search();
    searchResults.style.display = 'block';
  }
      };

  const search = instantsearch(options);

  
  search.addWidget(
    instantsearch.widgets.searchBox({
      container: '#search-box',
      placeholder: "Search..."
        })
  );

  
  search.addWidget(
    instantsearch.widgets.infiniteHits({
      container: '#search-hits',
      templates: {
        empty: '<div class="search-no-results">' + "No results found" + '</div>',
      item: getTemplate('search-hit')
          },
      cssClasses: {
      showmoreButton: 'btn btn-primary btn-outline'
    }
        })
  );

  
  search.on('render', function () {
    $('.search-hit-type').each(function (index) {
      let content_key = $(this).text();
      if (content_key in content_type) {
        $(this).text(content_type[content_key]);
      }
    });
  });

  
  search.start();
</script>


</body>

</html>

