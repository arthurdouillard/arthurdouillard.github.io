[{"authors":null,"categories":null,"content":" RDFIA / Master DAC \u0026amp; IMA / Sorbonne\nLe cours est organisé par le professeur Matthieu Cord. Vos assistants de TPs auquels vous devrez envoyer vos travaux sont Asya Grechka, Alexandre Rame et moi-même Arthur Douillard. Nos mails sont prenom.nom@lip6.fr.\nPour simplifier notre tâche vous êtes priés de nous adresser les mails avec pour objet [RDFIA][TP-\u0026lt;numero\u0026gt;].\nRappels Les TPs seront en Python3 et plusieurs bibliothèques seront utilisées. Voici quelques liens pour rappel, ou pour vous familiariser en avance:\n Rappel de Python Rappel de Numpy Introduction de Scikit-Learn. L\u0026rsquo;api est très similaire quelque soit l\u0026rsquo;algorithme (init / fit / predict) Introduction de Pytorch  Les cours seront ajoutés au fur et à mesure.\nTP 1 - 2 : SIFT / Bag of words 7 \u0026amp; 14 Octobre 2020\nA rendre pour avant le 21 Octobre 2020 à 23h59.\n Énoncé: TP1-2.pdf Code: code_tp1-2.zip Data: data_tp1-2.zip  Mises à jour: 2020-10-05, 22:34: Ajout du TP 1-2.\n","date":1601848800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601848800,"objectID":"87a4d9c6fdc30b76649c11b96524570e","permalink":"/rdfia/","publishdate":"2020-10-05T00:00:00+02:00","relpermalink":"/rdfia/","section":"","summary":"RDFIA / Master DAC \u0026amp; IMA / Sorbonne\nLe cours est organisé par le professeur Matthieu Cord. Vos assistants de TPs auquels vous devrez envoyer vos travaux sont Asya Grechka, Alexandre Rame et moi-même Arthur Douillard. Nos mails sont prenom.nom@lip6.fr.\nPour simplifier notre tâche vous êtes priés de nous adresser les mails avec pour objet [RDFIA][TP-\u0026lt;numero\u0026gt;].\nRappels Les TPs seront en Python3 et plusieurs bibliothèques seront utilisées. Voici quelques liens pour rappel, ou pour vous familiariser en avance:","tags":null,"title":"TP Deep Learning RDFIA","type":"mylayouts"},{"authors":[],"categories":null,"content":" I review in this article, papers published at CVPR 2020 about Continual Learning. If you think I made a mistake or miss an important paper, please tell me!\nTable of Contents    1. Few-Shots  1.1. Few-Shot Class-Incremental Learning 1.2. Incremental Few-Shot Object Detection  2. Conditional Channel Gated Networks for Task-Aware Continual Learning 3. Incremental Learning in Online Scenario 4. iTAML: An Incremental Task-Agnostic Meta-learning 5. Modeling the Background for Incremental Learning 6. ADINET: Attribute Drive Incremental Network for Retina Image Classification 7. Semantic Drift Compensation for Class-Incremental Learning 8. Maintaining Discrimination and Fairness in Class Incremental Learning 9. Mnemonics Training: Multi-Class Incremental Learning without Forgetting 10. Towards Backward-Compatible Representation Learning    1. Few-Shots Many papers this year in Continual Learning were about few-shot learning. Besides the CVPR papers I\u0026rsquo;ll present, there is also a workshop paper (Cognitively-Inspired Model for Incremental Learning Using a Few Examples, Ayub et al. CVPR Workshop 2020) and an arXiv (Defining Benchmarks for Continual Few-Shot Learning, Antoniou et al. arxiv:2004.11967).\n1.1. Few-Shot Class-Incremental Learning PDF: 2004.10956\nAuthors: Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, Yihong Gong\nTao et al. proposes in this paper a mix between Few-Shot and Continual Learning. They benchmark their model \u0026mdash;TOPIC\u0026ndash; on the CIFAR100, miniImageNet, and CUB200. The first task is large (60 classes for CIFAR100), then the following tasks have few classes (5 \u0026lsquo;n-way\u0026rsquo;) and few training samples per class (5 \u0026lsquo;k-shots\u0026rsquo;).\nThe author uses Neural Gas (Martinetz et al. 1991) and Competitive Hebbian Learning (Martinetz, 1993). This neural network seems similar to the Self-Organizing Maps.\nThe Neural Gas is an undirected graph, where each node $j$, is defined as $(m_j, \\Lambda_j, z_j, c_j)$: - $m_j$ is a centroid vector, similar to what we can expect after a K-Means (kNN for CL) - $\\Lambda_j$ is the variance matrix of each dimension of the vector - $z_j$ and $c_j$ are respectively the assigned images and labels\nThe graph is created after the first task, once the features extractor has been trained. They sample 400 features randomly among the training samples and use them as initial centroids.\nNodes are updated by some kind of moving average (Eq.3 of the paper), where all centroids go towards the incoming sample. The move in the latent space is weighted by a learning rate and more importantly by the L2 distance rank: close features will affect more the centroid than far features.\nTo create the edges between nodes, they use Competitive Hebbian Learning. Hebbian Learning in Neuroscience stipulates that:\n Neurons that fire together, wire together\n In this case, if two nodes are respectively the closest and second closest nodes to an input features, an edge is created. The edge has an age that is incremented each time no \u0026ldquo;firing together\u0026rdquo; happened. Past an age threshold, the edge is removed.\nThe model is trained with a usual softmax+cross-entropy but the competitive Hebbian learning is done after each step. Note that the latter doesn\u0026rsquo;t produce gradients for backpropagation.\nFinally, the inference is not really explained but I guess that each node has a \u0026ldquo;label\u0026rdquo; by voting which training sample labels were most associated with. Given a test input sample, its label is determined by the label of its closest node.\nThey stabilize the training by constraining the centroids to stay close to their previous position with a loss they called \u0026ldquo;anchor loss\u0026rdquo;. It\u0026rsquo;s actually a Mahalanobis loss, where the distance is weighted per dimension with the inverse of the variance (i.e. precision).\nThey also add a regularization called \u0026ldquo;min-max loss\u0026rdquo; to separate new centroids (added with new tasks) from previous centroids. It is similar to the hinge loss used by Hou et al. 2019.\n1.2. Incremental Few-Shot Object Detection PDF: 2003.04668\nAuthors: Juan-Manuel Perez-Rua, Xiatian Zhu, Timothy Hospedales, Tao Xiang\nPerez-Rua et al. propose to use together three settings: Continual Learning, Object Detection (aka finding object boxes in an image), and Few-Shot (with few training samples).\nTheir setting is made of a first large task, with many classes \u0026amp; data, then the following tasks add new classes with only 10 labeled examples per class (10 k-shots). This is impressive because object detection is harder than classification!\nDuring the first task, they train a CenterNet. Similar to CornerNet, class-agnostic features are extracted by a ResNet then class-specific heatmaps are generated. The most active zones are chosen to be boxes\u0026rsquo; centers. Two additional heads regress the boxes\u0026rsquo; width and height.\nOnce their CenterNet has been trained on the base classes, they train a Meta-Learning-based generator. This module must learn to produce the \u0026ldquo;class-codes\u0026rdquo;, aka the class-specific weights used by the detector. To do so, they train the generator on the base classes split into episodes. All weights are frozen except the generator\u0026rsquo;s.\nFor the following tasks, there is no training. Given a new class, the meta-generator produces on-the-fly new weights, and the inference is immediately done.\nIt\u0026rsquo;s worth noting that the results on novel classes are quite low:\n2. Conditional Channel Gated Networks for Task-Aware Continual Learning PDF: 2004.00070\nAuthors: Davide Abati, Jakub Tomczak, Tijmen Blankevoort, Simone Calderara, Rita Cucchiara, Babak Ehteshami Bejnordi\nAbati et al. propose an interesting view of sub-networks in Continual Learning. Previous methods proposed to learn sub-networks inside a unique network (think Lottery Ticket Hypothesis (Frankle \u0026amp; Carbin, 2018)). Those sub-networks can be learned by an evolutionary algorithm (Fernando et al., 2017), L1 sparsity (Golkar et al., 2019), or learned gating (Hung et al., 2019). However they all have a major constraint: they need the task id in inference to choose the right sub-networks, a setting called Multi-Head Evaluation. Having the task id in inference makes the problem much easier, and I think it is not realistic.\nThe authors propose to train sub-networks with learned gating. The right sub-network will be chosen inference with a Task Classifier, therefore they don\u0026rsquo;t use the task id! To the best of my knowledge, they are only the second to do this (von Oswald et al. 2020).\nEach residual block of their ResNet has $T$ (number of tasks) gate networks that choose the which filters to enable or disable:\nThe selection of filters to pass or block is discrete and thus non-differentiable. Thus they use the Gumbel Softmax Sampling similarly to (Guo et al. 2019). The forward pass is discrete, but the backward pass is continuous. After each task, they record on a validation set which gates have fired. Their associated filters will be frozen for the following tasks, but still usable!\nThe interesting part of this paper is how they train the task classifier. During training, all gates are fired in parallel:\nThis means that given a single input image, they have $T$ parallel stream of activations. All those streams\u0026rsquo; outputs are concatenated and fed to a task classifier (a two layers MLP) that classifies the task id. This task id will then be used to chose the right stream to give to the task-specific classifier.\nI really like this method, however, it\u0026rsquo;s unfortunate that they don\u0026rsquo;t compare their model to the lastest SotA and on large-scale datasets. Their largest dataset is ImageNet-50. Furthermore, I would like to see which gates fire the most, early layers or later layers?\u0026hellip;\n3. Incremental Learning in Online Scenario PDF: 2003.13191\nAuthors: Jiangpeng He, Runyu Mao, Zeman Shao, Fengqing Zhu\nHe et al. claim to learn in an \u0026ldquo;Online Scenario\u0026rdquo;: new classes are added as usual, with also old classes new samples. This sounds similar to the New Instances and Classes (NIC) (Lomonaco et al., 2017). They claim novelty while it\u0026rsquo;s not really true (on top of my head, Aljundi et al. and Lomonaco et al. have worked on this).\nThe authors propose two contributions: first of all they use at first a Nereast Class Mean (NCM) (kNN for CL) classifier. Similar to iCaRL\u0026rsquo;s NME, class means are computed. To handle concept drift, they update the means for new samples with a moving average:\n$$\\mu_{y}^{\\phi} \\leftarrow \\frac{n_{y i}}{n_{y i}+1} \\mu_{y}^{\\phi}+\\frac{1}{n_{y i}+1} \\phi\\left(\\mathbf{x}_{i}\\right)$$\nFurthermore, during the early tasks, they use in inference their NCM classifier because they remark that it behaves well with data scarcity. When the model is better trained, having seen enough samples, they switch their classifier for the class probabilities from a softmax. It\u0026rsquo;s interesting to mix those inference classifiers (classification and metric-based) contrary to Hou et al., 2019 and Douillard et al., 2020 that evaluated separately the two methods. However He et al.\u0026rsquo;s switch from one method to the other is an hyperparameter, it would have been nice to do so based on some Uncertainty measure.\nTheir second contribution is a modified cross-distillation: in addition of an usual distillation loss on probabilities with temperature (Hinton et al., 2015), they modify the classification loss. It is still a cross-entropy but the probabilities associated to old classes are a linear combination of the old and new model outputs:\n$$\\tilde{p}^{(i)}=\\left\\{\\begin{array}{cc}\\beta p^{(i)}+(1-\\beta) \\hat{p}^{(i)} \u0026amp; 0 \\lt i \\leq n \\\\\\ p^{(i)} \u0026amp; n \\lt i \\leq n+m\\end{array}\\right.$$\nThey also finetune their model on a balanced set like Castro et al., 2018 did.\nFinally, they evaluate their model on CIFAR100, ImageNet100, and Food-101. Unfortunately, they evaluate their \u0026ldquo;Online\u0026rdquo; NIC setting only on Food-101 and solely compare against a simple baseline, not any SotA models (adapted to the setting). CIFAR100 and ImageNet100 are evaluated in the classic NC setting. They have slightly better performance than SotA on the former and are equivalent to BiC on the latter. I\u0026rsquo;m quite annoyed that they claimed in the abstract to \u0026ldquo;out-perform\u0026rdquo; SotA on ImageNet1000 while they actually only evaluate on the smaller-scale ImageNet100. BiC really shines on ImageNet1000, I\u0026rsquo;d have liked to see how their model fares in this harsher dataset.\n4. iTAML: An Incremental Task-Agnostic Meta-learning PDF: 2003.11652\nAuthors: Jathushan Rajasegaran, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Mubarak Shah\nRajasegaran et al. propose to a novel Meta-Learning model. Several approaches exist using this branch of methods for Continual Learning, they are however the first to the best of my knowledge to do so without task id in inference.\nThe goal of Meta-Learning is to \u0026ldquo;learn to learn\u0026rdquo;. In this case, they want to obtain a network that learns to produce a set of parameters that can then be tuned quickly to a particular task. Following existing Meta-Learning models, they train in two loops. The inner loop learns an actual task, while the outer loop learns to produce a good initialization for the inner loop (i.e. \u0026ldquo;learn to learn\u0026rdquo;). Inspired by Reptile (Nichol et al., 2018), their outer loop is trained on the difference between the base parameters and the parameters learned after the inner loop. The different with Reptile is that the inner loop learns separately each task, producing mean inner loop parameters:\n$$\\Phi=\\Phi_{b a s e}-\\eta \\frac{1}{t} \\sum_{i=1}^{t}\\left(\\Phi_{b a s e}-\\Phi_{i}\\right)=\\eta \\frac{1}{t} \\sum_{i=1}^{t} \\Phi_{i}+(1-\\eta) \\Phi_{b a s e}$$\nThis forces the meta-model to find a set of parameters good for all tasks:\nDuring inference, they proceed in two steps: finding the task id, and then tuning the model for this task. To find the task id, they record for the test subset which task predictions are the most activated in average, and choose the maximum value:\nThen, given the predicted task id, they sample all exemplars memory (aka Rehearsal Learning) belonging to this task and learn in a single inner loop task-specific parameters. The resulting model is then used to classify the test samples.\nNote that while it\u0026rsquo;s a very interesting approach, their model stands on the quality of the task classification. They claim it\u0026rsquo;s almost 100% accuracy but it only for a reason: in their setting, the model is evaluated on each seen task separately. In a real setting, samples from different tasks are mixed together. There their algorithm 2 won\u0026rsquo;t work. Therefore I don\u0026rsquo;t think this model is truly \u0026ldquo;task-agnostic\u0026rdquo; but it\u0026rsquo;s definitely a good step forward.\nThey evaluate various models, from meta-learning to continual learning domains, on MNIST, SVHN, CIFAR100, ImageNet100, ImageNet1000, and Celeb-10k. It\u0026rsquo;s a bit strange however that BiC (a very good alternative for large scale datasets) is evaluated on Celeb-10k but not ImageNet100 (where it would have beaten iTAML).\n5. Modeling the Background for Incremental Learning PDF: 2002.00718\nAuthors: Fabio Cermelli, Massimiliano Mancini, Samuel Rota Bulò, Elisa Ricci, Barbara Caputo\nCermelli et al. attacks the problem of Semantic Segmentation for Continual Learning. In semantic segmentation, the goal is to give a class to each pixel. Two cars next to each other will have the same pixels labels. This is particularly difficult in Continual Learning for the same reasons as Object Detection: at task $t$, the class \u0026ldquo;car\u0026rdquo; may be a background, but then at task $t+1$ we have to predict it. However, our model may have seen images containing cars in the first task and thus has learned to not detect those. Likewise, images from task $t+1$ may be annotated so that the \u0026ldquo;person\u0026rdquo; learned previously is now part of the background.\nTo solve the problem mentionned before, they revisit both the cross-entropy and distillation losses. The former is split in two part: if the pixel probability belongs to the current task\u0026rsquo;s set of classes it is kept unchanged. Otherwise, it is the probability of having either an old class or the background:\n$$\\begin{array}{l}\\qquad \\ell_{c e}^{\\theta^{t}}(x, y)=-\\frac{1}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} \\log \\tilde{q}_{x}^{t}\\left(i, y_{i}\\right) \\\\\\ \\text { where: } \\\\\\ \\qquad \\tilde{q}_{x}^{t}(i, c)=\\left\\{\\begin{array}{ll}q_{x}^{t}(i, c) \u0026amp; \\text { if } c \\neq \\mathrm{b} \\\\\\ \\sum_{k \\in \\mathcal{Y}^{t-1}} q_{x}^{t}(i, k) \u0026amp; \\text { if } c=\\mathrm{b}\\end{array}\\right.\\end{array}$$\nLikewise, the distillation loss is changed if the pixel belongs to the background, according to the current task, to the probability of having either a new class or the background:\n$$\\ell_{k d}^{\\theta^{t}}(x, y)=-\\frac{1}{|\\mathcal{I}|} \\sum_{i \\in \\mathcal{I}} \\sum_{c \\in \\mathcal{V} t-1} q_{x}^{t-1}(i, c) \\log \\hat{q}_{x}^{t}(i, c)$$\n$$\\hat{q}_{x}^{t}(i, c)=\\left\\{\\begin{array}{ll}q_{x}^{t}(i, c) \u0026amp; \\text { if } c \\neq \\mathrm{b} \\\\\\ \\sum_{k \\in \\mathcal{C}^{t}} q_{x}^{t}(i, k) \u0026amp; \\text { if } c=\\mathrm{b}\\end{array}\\right.$$\nThis handles the case where the previous model considers a current pixel as background while the current model considers it as a new class.\nFinally, the classifier weights for new classes are initialized with the background weight.\n6. ADINET: Attribute Drive Incremental Network for Retina Image Classification PDF: CVPR webpage\nAuthors: Qier Meng, Satoh Shin\u0026rsquo;ichi\nMeng and Shin\u0026rsquo;ichi use Continual Learning for retinal images classification. They found that retinal diseases have a large variety of types and that current methods didn\u0026rsquo;t allow them to train incrementally each type a new patient came in.\nTheir originality lies in their usage of attributes. Retinal images have been annotated with a disease label (\u0026ldquo;AMD\u0026rdquo;, \u0026ldquo;DR\u0026rdquo;\u0026hellip;) but also with several attributes (\u0026ldquo;hermorrhage\u0026rdquo;, \u0026ldquo;macular edema\u0026rdquo;\u0026hellip;). Therefore in addition to the classic distillation loss with temperature scaling applied to the disease prediction, they also distill the attributes prediction of the previous model with a BCE.\nIn addition to the two distillation losses, they also refine their attributes prediction with a \u0026ldquo;weight estimation\u0026rdquo;. It measures how much of a contribution an attribute has to distinguish classes. It\u0026rsquo;s similar to doing a self-attention on all attributes to find which one is the most important. This weight estimation is then used to ponder the attribute predictions. They didn\u0026rsquo;t detail much the rationale behind this weight estimation but empirical results show small but consistent gains.\nThey evaluate both medical and academic datasets. For the later they used ImageNet-150k-sub: it contains 100 classes of ImageNet1000, but only 150 training images were selected per class instead of ~1200. I\u0026rsquo;ve never seen a model evaluated on this dataset, but it looks like a more challenging dataset than ImageNet100. They display a significant improvement over the 2017\u0026rsquo;s iCaRL.\nIt\u0026rsquo;s interesting to predict attributes in the context of Continual Learning. I hypothesize that it forces the model to learn fine-grained features common to all tasks and may reduce catastrophic forgetting.\n7. Semantic Drift Compensation for Class-Incremental Learning PDF: 2004.00440\nAuthors: Lu Yu, Bartłomiej Twardowski, Xialei Liu, Luis Herranz, Kai Wang, Yongmei Cheng, Shangling Jui, Joost van de Weijer\nRebuffi et al., 2017 uses class means with a k-NN to classify samples in inference. Those class means are updated after each task by re-extracting features of rehearsal samples of old classes with the new ConvNet. This supposes that we have access to previous data, at least in a limited amount.\nYu et al. propose to update the class means without even using previous data. First, they compute the embedding drift between the start and the end of the current task on current data:\n$$\\boldsymbol{\\delta}_{i}^{t-1 \\rightarrow t}=\\mathbf{z}_{i}^{t}-\\mathbf{z}_{i}^{t-1}, y_{i} \\in C^{t}$$\nThen for each old classes, they compute the mean vector of drift:\n$$\\begin{array}{c}\\hat{\\Delta}_{c^{s}}^{t-1 \\rightarrow t}=\\frac{\\sum_{i}\\left[y_{i} \\in C^{t}\\right] w_{i} \\delta_{i}^{t-1 \\rightarrow t}}{\\sum_{i}\\left[y_{i} \\in C^{t}\\right] w_{i}} \\ w_{i}=e^{-\\frac{\\left|x_{i}^{t-1}-\\mu_{c^{s}}^{t-1}\\right|^{2}}{2 \\sigma^{2}}}\\end{array}$$\nThe drift is weighted by $w_i$, which gives a lower weight to outliers. Thus samples will low confidence won\u0026rsquo;t affect as much the drift computation than \u0026ldquo;archetypal\u0026rdquo; samples.\nFinally this drift is computed at after each task, starting from the second one, and is added continuously to the class mean vectors:\n$$\\hat{\\mu}_{c^{s}}^{t}=\\mu_{c^{s}}^{s}+\\hat{\\Delta}_{c^{s}}^{s \\rightarrow s+1}+\\ldots+\\hat{\\Delta}_{c^{s}}^{t-1 \\rightarrow t}$$\nThey add their method, nicknamed SDC, to the model EWC. They show on CIFAR100 and ImageNet100 excellent performance only beaten by Hou et al., 2019. It\u0026rsquo;s important to note that Hou et al., 2019 use exemplars while Yu et al. don\u0026rsquo;t. On the other hand, according to their code they are in a Multi-Head Evaluation setting where they know the task id during inference. Thus they classify a sample among the task classes instead of all seen classes as do Rebuffi et al., 2017 or Hou et al., 2019. Their setting is not really comparable to Hou et al., 2019.\n8. Maintaining Discrimination and Fairness in Class Incremental Learning PDF: 1911.07053\nAuthors: Bowen Zhao, Xi Xiao, Guojun Gan, Bin Zhang, Shutao Xia\nZhao et al. propose a method directly in the line of Belouadah and Popescu, 2019) (IL2M) and Wu et al., 2019) (BiC). Both works remarked that a bias towards new classes and detrimental to old classes. IL2M uses some statistics to correct this bias, while BiC recalibrates (Recalibration) the probabilities using a linear model learned on validation data.\nZhao et al. use a simpler solution that they call Weight Aligning (WA). They saw, as Hou et al., 2019, that the norm of the weights associated with old classes is lower than those associated with new classes:\nHou et al., 2019 and Douillard et al., 2020 use a cosine classifier so that all norms are equal to 1. Zhao et al. instead re-normalize the weights based on the norm ratio:\n$$\\begin{array}{c}\\widehat{\\mathbf{W}}_{n e w}=\\gamma \\cdot \\mathbf{W}_{n e w} \\\\\\ \\text { where } \\\\\\ \\gamma=\\frac{M e a n\\left(N o r m_{o l d}\\right)}{M e a n\\left(N o r m_{n e w}\\right)}\\end{array}$$\nThey remark that:\n [\u0026hellip;] we only make the average norms become equal, in other words, within new classes (or old classes), the relative magnitude of the norms of the weight vectors does not change. Such a design is mainly used to ensure the data within new classes (or old classes) can be separated well.\n This weight alignment is done between each task. Furthermore, they clip after each optimization step the value of the weights to be positive to make the weights norm more consistent with their corresponding logits (after ReLU).\nIn their ablation, they show that weight alignment provides more gain than knowledge distillation which is quite impressive. Using both lead them to a significant gain over BiC and IL2M on large scale datasets like ImageNet1000.\nOverall I liked their method as it is very simple and yet efficient.\n9. Mnemonics Training: Multi-Class Incremental Learning without Forgetting PDF: 2002.10211\nAuthors: Yaoyao Liu, An-An Liu, Yuting Su, Bernt Schiele, Qianru Sun\nLiu et al. propose in this work two important contributions which make it my favorite paper in this review. The first, advertised, is an improvement of Rehearsal Learning. The second, a little hidden in the paper, is Meta-Learning-inspired method to adapt gracefully to new distribution.\nIn rehearsal learning, we feed old samples to the model to reduce forgetting. Obviously, we won\u0026rsquo;t use all old samples, but a very limited amount. Here the authors use 20 images per class, like Hou et al., 2019 and Douillard et al., 2020. Rebuffi et al., 2017 proposed with iCaRL to a herding selection which finds iteratively the barycenter of the class distribution. However Castro et al., 2018 remarked that taking the closest samples to the class mean, or even random samples (!), worked as well.\nLiu et al. significantly improve those solutions by transforming the selected exemplars. First, they randomly select samples, then given a trained \u0026amp; fixed model, they optimize the exemplars as the pixel-level:\nThe optimized exemplars must lead to a decreased loss on the new classes data (present in large amounts).\nThe modification is very minor visually: we only see a little bit of noise overlayed on the original images. The authors found that this optimization of the images leads to a set of exemplars well distributed on the class boundaries:\nThis optimization is done at the task end, once exemplars from new classes have been selected. The authors also choose to finetune the exemplars of old classes that have been selected in previous tasks. However, in this case, we don\u0026rsquo;t have anymore a large amount of old classes\u0026rsquo; data to act as ground truth for the old classes exemplars optimization. Therefore, they split the exemplars set in half. One split is optimized using the second for ground truth and vice-versa.\nThe second contribution, and the major one, is unfortunately not very advertised in this paper. The authors re-use an idea from one of their previous papers in Meta-Learning. Instead of tuning all the ConvNet parameters for each task, they only slightly adapt them: for each kernel, a small kernel of spatial dimensions equal to one is learned (likewise for the biases). This small kernel is expanded to the base kernel dimension and element-wise multiplied to it:\nIntuitively, this method called \u0026ldquo;Meta-Transfer\u0026rdquo; does a small \u0026ldquo;shift \u0026amp; scaling\u0026rdquo;. Most of the network is kept frozen and thus don\u0026rsquo;t forget too much. The small adaptation enables the network to learn new classes.\nThey evaluated CIFAR100, ImageNet100, and ImageNet1000 in various settings and beat all previous SotA (especially Hou et al., 2019 and Wu et al., 2019). In my recent paper (Douillard et al., 2020), our model PODNet outperforms their\u0026rsquo;s in several settings and evaluate even on even longer incremental training.\n10. Towards Backward-Compatible Representation Learning PDF: 2003.11942\nAuthors: Yantao Shen, Yuanjun Xiong, Wei Xia, Stefano Soatto\nThis paper is not directly related to Continual Learning but rather to Visual Search. Shen et al. raise the issue of backfilling:\nOn a visual search model, embeddings of a large gallery have been computed once. Then given a query image, we extract its features and compare them to the gallery features collection. For example, I take a picture of a shirt and want to know what are the most similar clothes available in a store.\nA problem arises when a new model is trained. This model may be different from the previous one because the data is was trained on or because the architecture and losses were changed. The gallery features collection is not up-to-date anymore and we need to extract gain the features of the whole collection with the new model. This can be very costly when the gallery is made of billions of images.\nThe authors propose to make the features extractor \u0026ldquo;backward-compatible\u0026rdquo;. It\u0026rsquo;s mean that query features extracted by the new model are in the same latent space of the old model:\nTo produce a new model backward-compatible with a previous model that may be different, the authors add a loss over the classification loss:\nThe first part of the loss update the new model on new $t$ dataset $T_{\\text{new}}$. For the second part, the best alternative proposed is training the old classifier $w_{c\\,\\text{old}}$ with the new features extractor $w_\\theta$ on the new dataset $T_\\text{new} = T_\\text{BCT}$. Because the new dataset can contain new classes, the old classifier is extended with new weights. They are initialized with the mean features extracted by $w_{\\theta\\, \\text{old}}$ like Weight Imprinting did in Metric-Learning (Qi et al., 2018).\nOverall, their model is still far from the upper-bound (recomputing the gallery with the new model) but they improve significantly over simple baselines and beat LwF (Li \u0026amp; Hoiem, 2016) by 3 points. I think this model is quite \u0026ldquo;simple\u0026rdquo; compared to SotA Continual Learning but it is interesting to see actual applications of the domain.\n","date":1592690400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1592690400,"objectID":"a9b437fbe47a659efb4a0cc9542a9506","permalink":"/post/cvpr20-continual/","publishdate":"2020-06-21T00:00:00+02:00","relpermalink":"/post/cvpr20-continual/","section":"post","summary":"Review of Continual Learning at CVPR'20","tags":[],"title":"Review of Continual Learning at CVPR'20","type":"post"},{"authors":null,"categories":null,"content":"","date":1587938400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587938400,"objectID":"8ef8fa5e9b3505d75ca5b18590cd1b15","permalink":"/project/continuum/","publishdate":"2020-04-27T00:00:00+02:00","relpermalink":"/project/continuum/","section":"project","summary":"A PyTorch library providing loaders for Continual Learning","tags":[],"title":"Continuum","type":"project"},{"authors":null,"categories":null,"content":"","date":1587938400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587938400,"objectID":"2c897168e4b088fbd2c6bc3731525221","permalink":"/project/inclearn/","publishdate":"2020-04-27T00:00:00+02:00","relpermalink":"/project/inclearn/","section":"project","summary":"Collection of Continual Learning paper implementations","tags":[],"title":"Inclearn","type":"project"},{"authors":["Arthur Douillard","Eduardo Valle","Charles Ollion","Thomas Robert","Matthieu Cord"],"categories":null,"content":"https://arxiv.org/abs/2004.13513\n","date":1577833200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577833200,"objectID":"9e7232643673b39868a01c1cda6a0f47","permalink":"/publication/ghost/","publishdate":"2020-01-01T00:00:00+01:00","relpermalink":"/publication/ghost/","section":"publication","summary":"https://arxiv.org/abs/2004.13513","tags":null,"title":"Insights from the Future for Continual Learning","type":"publication"},{"authors":["Arthur Douillard","Matthieu Cord","Charles Ollion","Thomas Robert","Eduardo Valle"],"categories":null,"content":"https://arxiv.org/abs/2004.13513\n","date":1577833200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577833200,"objectID":"7e559217cccfa8a1f5cc8bd67b8bc8b0","permalink":"/publication/podnet/","publishdate":"2020-01-01T00:00:00+01:00","relpermalink":"/publication/podnet/","section":"publication","summary":"https://arxiv.org/abs/2004.13513","tags":null,"title":"Small-Task Incremental Learning","type":"publication"},{"authors":[],"categories":null,"content":" The hallmark of human intelligence is the capacity to learn. A toddler has comparable aptitudes to reason about space, quantities, or causality than other ape species (source). The difference of our cousins and us is the ability to learn from others.\nThe recent deep learning hype aims to reach the Artificial General Intelligence (AGI): an AI that would express (supra-)human-like intelligence. Unfortunately current deep learning models are flawed in many ways: one of them is that they are unable to learn continuously as human does through years of schooling, and so on.\nWhy do we want our models to learn continuously? Regardless of the far away goal of AGI, there are several practicals reasons why we want our model to learn continuously. Before describing a few of them, I\u0026rsquo;ll mention two constraints:\n Our model cannot review all previous knowledge each time it needs to learn new facts. As a child in 9th grade, you don\u0026rsquo;t review all the syllabus of 8th grade as it\u0026rsquo;s supposed to have been already memorized. Our model needs to learn continuously without forgetting any previously learned knowledge.  A real applications of these two constraints is robotics: a robot in the wild should learn continuously its environment. Furthermore due to hardware limitation, it may neither store all previous data nor spend too much computational resource.\nAnother application is what I do at Heuritech: we detect fashion trends. However every day across the globe a new trend may appear. It is impracticable to review our large trends database each time we need to learn a new one.\nNow that the necessity of learning continuously has been explained, let us differentiate three scenarios (Lomonaco and Maltoni, 2017):\n Learning new data of known classes (online learning) Learning new classes (class-incremental learning) The union of the two previous scenarios  In this article I will focus only on the second scenario. Note however that the methods used are fairly similar between scenario.\nMore practically this article will cover models that learn incrementally new classes. The model will see only new classes\u0026rsquo; data, as we aim to remember well old classes. After each task, the model is trained on a all seen classes using a separate test set:\n Figure 1: Several steps of incremental learning.   As seen in the image above, each step produces a new accuracy score. Following (Rebuffi et al, 2017) the final score is the average of all previous task accuracy score. It\u0026rsquo;s called the average incremental accuracy.\nNaive solution: transfer learning Transfer learning allows to transfer the knowledge gained on one task (e.g ImageNet and its 1000 classes) to another task (e.g classify cats \u0026amp; dogs) (Razavian et al, 2014). Usually the backbone (a ConvNet in Computer Vision, like ResNet) is kept while a new classifier is plugged in on top of it. During transfer, we train the new classifier \u0026amp; fine-tune the backbone.\nFinetuning the backbone is essential to reach good performance on the destination task. However we don\u0026rsquo;t have access anymore to the original task data. Therefore our model is now optimized only for the new task. While at the training end, we will have good performance on this new task, the old task will suffer a significant drop of performance.\n(French, 1995) described this phenomenon as a catastrophic forgetting. To solve it, we must find an optimal trade-off between rigidity (being good on old tasks) and plasticity (being good on new tasks).\nThree broad strategies (Parisi et al, 2018) defines 3 broad strategies:\n External Memory storing a small amount of previous tasks data Constraints-based methods avoiding forgetting on previous tasks Model Plasticity extending the capacity  1. External Memory As said previously we cannot keep all our previous data for several reasons. We can however relax this constraint by limiting access to previous data to a bounded amount.\nRehearsal learning (Rebuffi et al, 2017)\u0026rsquo;s iCaRL assumes we dispose of a limited amount of space to store previous data. Our external memory could have a capacity of 2,000 images. After learning new classes, a few amount of those classes data could be kept in it while the rest would be discarded.\n Figure 2: Several steps of incremental learning with a memory storing a subset of previous data.   Pseudo-Rehearsal learning (Shin et al, 2017; Kemker and Kanan, 2018) assume instead that we cannot keep previous data, like images, but that we can store the class distribution statistics. With this, a generative model can generate on-the-fly old classes data. This approach is however very reliant on the quality of the generative model; generated data are still subpar to real data (Ravuri and Vinyals, 2019). Furthermore it is still crucial to also avoid a forgetting in the generator.\n Figure 3: Several steps of incremental learning with a generator generating previous data.   Generally (pseudo-)rehearsal-based methods outperforms methods only using new classes data. It\u0026rsquo;s then fair to compare their performance separately.\n2. Constraints-based methods Intuitively, forcing the current model $M^t$ to be similar to its previous version $M^{t-1}$ will avoid forgetting. There is a large array of methods aiming to do so. However they all have to balance a rigidity (encouraging similarity between $M^t$ and $M^{t-1}$) and plasticity (letting enough slack to $M^t$ to learn new classes).\nWe can separate those methods in three broads categories:\n Those enforcing a similarity of the activations Those enforcing a similarity of the weights And those enforcing a similarity of the gradients  2.1. Constraining the activations (Li and Hoiem, 2016)\u0026rsquo;s LwF introduced knowledge distillation from (Hinton et al, 2015): given a same image, $f^t$\u0026rsquo;s base probabilities should be similar to $f^{t-1}$\u0026rsquo;s probabilities:\n Figure 4: Base probabilities are distilled from the previous model to the new one.   The distillation loss can simply be a binary cross-entropy between old and new probabilities.\nModel output probabilities is just one kind of activation among others. (Hou et al, 2019)\u0026rsquo;s UCIR used a similarity-based between the extracted features $h^{t-1}$ and $h^t$ of the old and new model:\n$$L_\\text{Less-Forget} = 1-\\langle \\frac{h^t}{\\Vert h^t \\Vert_2}, \\frac{h^{t-1}}{\\Vert h^{t-1} \\Vert_2}\\rangle$$\n Figure 5: New model embeddings must be similar from the old one.   To sum up, encouraging the new model to mimic the activations of its previous version reduces the forgetting of old classes. A different but similar approach is reduce the difference between the new and old model weights:\n2.2. Constraining the weights A naive method would be to minimize a distance between the new and old weights likewise $L = (\\mathbf{W}^t - \\mathbf{W}^{t-1})^2$. However, as remarked by (Kirkpatrick et al, 2016)\u0026rsquo;s EWC, the resulting new weights would be under-performing for both old and new classes. Then, the authors suggested to modulate the regularization according to neurons importance.\nImportant neurons for task $T-1$ must not change in the new model. On the other hand, unimportant neurons can be more freely modified, to learn efficiently the new task $T$:\n$$L = I (W^{t-1} - W^t)^2$$\nWith $W^{t-1}$ and $W^{t}$ the weights of respectively the old and new model, and $I$ a neurons importance matrix defined from $W^{t-1}$.\nIn EWC, the neurons importance are defined with the Fisher information, but variants exist. Following research (Zenke et al, 2017; Chaudhry et al, 2018) builds on the same idea with refinement of the neurons importance definition.\n2.3. Constraining the gradients Finally a third category of constraints exist: constraining the gradients. Introduced by (Lopez-Paz and Ranzato, 2017)\u0026rsquo;s GEM, the key idea is that the the new model\u0026rsquo;s loss should be lower or equal to the old model\u0026rsquo;s loss on old samples stored in a memory (cf rehearsal learning).\n$$L(f^t, M) \\le L(f^{t-1}, M)$$\nThe authors rephrase this constraint as an angle constraint on the gradients:\n$$\\langle \\frac{\\partial L(f^t, M)}{\\partial f^t}, \\frac{\\partial L(f^{t-1}, M)}{\\partial f^{t-1}} \\rangle \\ge 0$$\nPut it more simply, we want the gradients of the new model to \u0026ldquo;go in the same direction\u0026rdquo; as they would have with the previous model.\nIf this constraint is respected, it\u0026rsquo;s likely that the new model won\u0026rsquo;t forget old classes. Otherwise the incoming gradients $g$ must be \u0026ldquo;fixed\u0026ldquo;: they are reprojected to their closest valid alternative $\\tilde{g}$ by minimizing this quadratic program:\n$$\\text{minimize}_{\\tilde{g}}\\, \\Vert g^t - \\tilde{g} \\Vert_2^2$$\n$$\\text{subject to}\\, \\langle g^{t-1}, \\tilde{g} \\rangle \\ge 0$$\n Figure 6: Gradients must keep going in the same direction, otherwise their direction is fixed.   As you may guess, solving this program for each violating gradients, before updating the model weights is very costly in time. (Chaudhry et al, 2018 ; Aljundi et al, 2019) improve the algorithm speed by different manners, including sampling a representative subset of the gradients constraints.\n3. Plasticity Other algorithms modify the network structure to reduce catastrophic forgetting. The first strategy is to add new neurons to the current model. (Yoon et al, 2017)\u0026rsquo;s DEN first trains on the new task. If its loss is not good enough, new neurons are added at several layers and they will be dedicated to learn on the new task. Furthermore the authors choose to freeze some of the already-existing neurons. Those neurons, that are particularly important for the old tasks, must not change in order to reduce forgetting.\n Figure 7: DEN adds new neurons for the new tasks, and selectively fine-tunes existing neurons.   While expanding the network capacity makes sense in an incremental setting where our model learns indefinitely, it\u0026rsquo;s worth noting that existing deep learning models are over-parametrized. The initial capacity can be enough to learn many tasks, at the condition that it\u0026rsquo;s used appropriately. As (Frankle and Carbin, 2019)\u0026rsquo;s Lottery Ticket Hypothesis formalized, large networks are made of very efficient sub-networks.\nEach sub-network can be dedicated to only one task:\n Figure 8: Among a large single network, several subnetworks can be uncovered, each specialized for a task.   Several methods exist to uncover those sub-networks: (Fernando et al, 2017)\u0026rsquo;s PathNet uses evolutionary algorithm, (Golkar et al, 2019) sparsify the whole network with a L1 regularization, and (Hung et al, 2019)\u0026rsquo;s CPG learns binary masks activating and deactivating connections to produce sub-networks.\nIt is worth noting that methods based on sub-networks assume to know on which task they are evaluated on. This setting, called multi-heads is challenging but fundamentally easier than single-head evaluation where models are evaluated on all tasks in the same time.\nDealing with class imbalance We saw previously three strategy to avoid forgetting (rehearsal, constraints, and plasticity). Those methods can be used together. Rehearsal is often used in addition of constraints.\nMoreover another challenge of incremental learning is the large class imbalance between new and old classes. For example, on some benchmarks, new classes could be made of 500 images each, while old classes would only have 20 images each stored in memory.\nThis class imbalance further encourages, wrongly, the model to be over-confident for new classes while being under-confident for old classes. Catastrophic forgetting is furthermore exacerbated.\n(Castro et al, 2018) train for each task their model under this class imbalance, but fine-tune it after with under-sampling: old \u0026amp; new classes are sampled to have as much images.\n(Wu et al, 2019) consider to use re-calibration (Guo et al, 2017): a small linear model is learned on validation to \u0026ldquo;fix\u0026rdquo; the over-confidence on new classes. It is only applied for new classes logits. (Belouadah and Popescu, 2019) proposed concurrently a similar solution fixing the new classes logits, but using instead class statistics.\n(Hou et al, 2019) remarked that weights \u0026amp; biases of the classifier layer have larger magnitude for new classes than older classes. To reduce this effect, they replace the usual classifier by a cosine classifier where weights and features are L2 normalized. Moreover they freeze the classifier weights associated to old classes.\nConclusion In this article we saw what is incremental learning: learning model with classes coming incrementally; what is its challenge: avoiding forgetting the previous classes to the benefice only of new classes; and broad strategies to solve this domain.\nThis domain is far from being solved. The upper bound is a model trained in a single step on all data. Current solutions are considerably worse than this.\nOn a personal note, my team and I have submitted an article for a conference on this subject. If it\u0026rsquo;s accepted, I\u0026rsquo;ll make a blog article on it. Furthermore I have made a library to train incremental model: inclearn. The library wasn\u0026rsquo;t updated since a few months as I\u0026rsquo;m currently cleaning my code. Be sure to check it out later.\n","date":1576018800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576018800,"objectID":"02ca3628da86d07e5ebf1c5730c5dc06","permalink":"/post/incremental-learning/","publishdate":"2019-12-11T00:00:00+01:00","relpermalink":"/post/incremental-learning/","section":"post","summary":"The hallmark of human intelligence is the capacity to learn continuously. Modern algorithms unfortunately lack this ability; the field of incremental learning is then trying to make our algorithms learn a continuous succession of tasks without forgetting.","tags":[],"title":"Learning Deep Neural Networks incrementally forever","type":"post"},{"authors":[],"categories":null,"content":" Those notes are based on the research paper \u0026ldquo;On Calibration of Modern Neural Networks\u0026rdquo; by (Guo et al, 2017.).\nHow To Be Confident In Your Neural Network Confidence? Very large and deep models, as ResNet, are far more accurate than their older counterparts, as LeNet, on computer vision datasets such as CIFAR100. However while they are better at classifying images, we are less confident in their own confidence!\nMost neural networks for classification uses as last activation a softmax: it produces a distribution of probabilities for each target (cat, dog, boat, etc.). These probabilities sum to one. We may expect that if for a given image, our model associate a score of 0.8 to the target ‘boat’, our model is confident at 80% that this is the right target.\nOver 100 images that were detected as boat, we can expect approximately that 80 images are indeed real boats, while the 20 remaining were false positives.\nIt was true for shallow model as LeNet but as newer models gained in accuracy their confidences became decorrelated from the “real confidence”.\nThis does not work anymore for deep neural networks:\n Figure 1: Miscalibration in modern neural network [source]   As you can see, older networks as LeNet had a low accuracy (55%) but their confidence was actually in line with the accuracy! Modern networks as ResNet have a higher accuracy (69%) but as showed in figure 1, they are over-confident.\nThis discrepancy between the model confidence and the actual accuracy is called miscalibration.\nWhy It Is Important Outside of toy datasets used in the academy, it can be useful to know how much confident our model is.\nImagine we have a model predicting frauds. We want to flag some transaction as suspicious based on the model confidence that it is a fraud. We could definitely compute an optimal threshold on the validation set, and then every confidence above this threshold would be flagged as a fraud. However this computed threshold could be 0.2 or 0.9 but would probably make much sense to a human.\nA model without miscalibration would help the users to interpret better the predictions.\nWhy It Happens The authors explores empirically what are the causes of this miscalibration in modern networks.\nThey measure the miscalibration with the Expected Calibration Error (ECE): the average difference between the confidence and the accuracy. This metric should be minimized.\nHigher Capacity \u0026amp; Cross-Entropy The most interpretable cause of the miscalibration is the increase of capacity and the cross-entropy loss.\nModel capacity can be seen as a measurement of how much a model can memorize. With an infinite capacity, the model could simply learn by heart the whole training dataset. A trade-off has to be made between a low and high capacity. If it is too low the model wouldn’t be able to learn essential features of your data. If it is too high, the model will learn too much and overfit instead of generalize. Indeed comprehension is compression: by leaving few enough capacity the model has to pick up the most representative features (pretty much in the same way PCA works) and will then generalize better (but too few capacity \u0026amp; no learning will happen!).\nThe new architectures such as ResNet have way more capacity than the older LeNet (25M parameters for the former and 20k for the latter). This high capacity led to better accuracy: the training set can almost be learned by heart.\nIn addition the models optimizes the cross-entropy loss that force them to be right AND to be very confident. The higher capacity helped to lower the cross-entropy loss and thus encourages deep neural networks to be over-confident. As you’ve seen on figure 1, the new models are now over-confident.\n Figure 2: More capacity (in depth or width) increases the miscalibration. [source]   The Mysterious Batch Normalization Batch Normalization normalizes the tensors in a network. It greatly improves the training convergence \u0026amp; the final performance. Why exactly it works that well is still a bit undefined (see more).\nThe authors remark empirically that using Batch Normalization increased the miscalibration but could not find an exact reason why.\n Figure 3: Batch Normalization increases the miscalibration. [source]   Could the help given by this method in training facilitate the over-confidence?\nRegularization The weight decay is an additional loss that penalizes the L2 norm of the weights. The larger the weights, the bigger the norm and thus the loss. By constraining the weights magnitude, it avoid the model finding extreme weight values that could make it overfit.\nThe authors found that increasing the regularization decreases the model accuracy as expected. However it also decreased the miscalibration! The answer is then again because regularization avoid overfitting \u0026amp; thus over-confidence.\n Figure 4: More regularization decreases the miscalibration. [source]   How To Fix Miscalibration This article\u0026rsquo;s title, \u0026ldquo;How To Be Confident In Your Neural Network Confidence\u0026rdquo;, led you to believe that you would discover how to reduce miscalibration.\nYou\u0026rsquo;re not going to reduce the capacity, remove Batch Normalization, and increase the regularization: you\u0026rsquo;ll hurt too much your precious accuracy.\nFortunately there are post-processing solutions. The authors describe several but the most effective one is also the simplest: Temperature Scaling.\nInstead of computing the softmax like this:\n$$\\text{softmax}(x)_i = \\frac{e^{y_i}}{\\Sigma_j^N e^{y_j}}$$\nAll the logits (values just before the final activation, here softmax) are divided by the same value called temperature:\n$$\\text{softmax}(x)_i = \\frac{e^{\\frac{y_i}{T}}}{\\Sigma_j^N e^{\\frac{y_j}{T}}}$$\nSimilar to (Hinton et al, 2015.), this temperature softens the probabilities.\nExtreme probabilities (high confidence) are more decreased than smaller probabilities (low confidence). The authors find the optimal temperature by minimizing the Expected Calibration Error on the validation set.\nThe miscalibration is almost entirely corrected:\n Figure 5: Temperature Scaling fixes the miscalibration. [source]   Another cool feature of Temperature Scaling: because all logits are divided by the same value, and that softmax is a monotone function, the accuracy remains unchanged!\n","date":1559167200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559167200,"objectID":"5b589cb7dae5a1c7637219549e9045c8","permalink":"/post/miscalibration/","publishdate":"2019-05-30T00:00:00+02:00","relpermalink":"/post/miscalibration/","section":"post","summary":"Why deep neural networks confidence are flawed and how to fix it.","tags":[],"title":"How To Be Confident In Your Neural Network Confidence","type":"post"},{"authors":[],"categories":null,"content":" Deep Neural Networks (DNNs) are notorious for requiring less feature engineering than Machine Learning algorithms. For example convolutional networks learn by themselves the right convolution kernels to apply on an image. No need of carefully handcrafted kernels.\nHowever a common point to all kinds of neural networks is the need of normalization. Normalizing is often done on the input, but it can also take place inside the network. In this article I\u0026rsquo;ll try to describe what the literature is saying about this.\nThis article is not exhaustive but it tries to cover the major algorithms. If you feel I missed something important, tell me!\nNormalizing the input It is extremely common to normalize the input (lecun-98b), especially for computer vision tasks. Three normalization schemes are often seen:\n Normalizing the pixel values between 0 and 1:  img /= 255.  Normalizing the pixel values between -1 and 1 (as Tensorflow does):  img /= 127.5 img -= 1.  Normalizing according to the dataset mean \u0026amp; standard deviation (as Torch does):  img /= 255. mean = [0.485, 0.456, 0.406] # Here it\u0026#39;s ImageNet statistics std = [0.229, 0.224, 0.225] for i in range(3): # Considering an ordering NCHW (batch, channel, height, width) img[i, :, :] -= mean[i] img[i, :, :] /= std[i] Why is it recommended? Let\u0026rsquo;s take a neuron, where:\n$$y = w \\cdot x$$\nThe partial derivative of $y$ for $w$ that we use during backpropagation is:\n$$\\frac{\\partial y}{\\partial w} = X^T$$\nThe scale of the data has an effect on the magnitude of the gradient for the weights. If the gradient is big, you should reduce the learning rate. However you usually have different gradient magnitudes in a same batch. Normalizing the image to smaller pixel values is a cheap price to pay while making easier to tune an optimal learning rate for input images.\n1. Batch Normalization We\u0026rsquo;ve seen previously how to normalize the input, now let\u0026rsquo;s see a normalization inside the network.\n(Ioffe \u0026amp; Szegedy, 2015) declared that DNN training was suffering from the internal covariate shift.\nThe authors describe it as:\n [\u0026hellip;] the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change.\n Their answer to this problem was to apply to the pre-activation a Batch Normalization (BN):\n$$BN(x) = \\gamma \\frac{x - \\mu_B}{\\sigma_B} + \\beta$$\n$\\mu_B$ and $\\sigma_B$ are the mean and the standard deviation of the batch. $\\gamma$ and $\\beta$ are learned parameters.\nThe batch statistics are computed for a whole channel:\n Statistics are computed for a whole batch, channel per channel.   $\\gamma$ and $\\beta$ are essential because they enable the BN to represent the identity transform if needed. If it couldn\u0026rsquo;t, the resulting BN\u0026rsquo;s transformation (with a mean of 0 and a variance of 1) fed to a sigmoid non-linearity would be constrained to its linear regime.\nWhile during training the mean and standard deviation are computed on the batch, during test time BN uses the whole dataset statistics using a moving average/std.\nBatch Normalization has showed a considerable training acceleration to existing architectures and is now an almost de facto layer. It has however for weakness to use the batch statistics at training time: With small batches or with a dataset non i.i.d it shows weak performance. In addition to that, the difference between training and test time of the mean and the std can be important, this can lead to a difference of performance between the two modes.\n1.1. Batch ReNormalization (Ioffe, 2017)\u0026rsquo;s Batch Renormalization (BR) introduces an improvement over Batch Normalization.\nBN uses the statistics ($\\mu_B$ \u0026amp; $\\sigma_B$) of the batch. BR introduces two new parameters $r$ \u0026amp; $d$ aiming to constrain the mean and std of BN, reducing the extreme difference when the batch size is small.\nIdeally the normalization should be done with the instance\u0026rsquo;s statistic:\n$$\\hat{x} = \\frac{x - \\mu}{\\sigma}$$\nBy choosing $r = \\frac{\\sigma_B}{\\sigma}$ and $d = \\frac{\\mu_B - \\mu}{\\sigma}$:\n$$\\hat{x} = \\frac{x - \\mu}{\\sigma} = \\frac{x - \\mu_B}{\\sigma_B} \\cdot r + d$$\nThe authors advise to constrain the maximum absolute values of $r$ and $d$. At first to 1 and 0, behaving like BN, then to relax gradually those bounds.\n1.2. Internal Covariate Shift? Ioffe \u0026amp; Szegedy argued that the changing distribution of the pre-activation hurt the training. While Batch Norm is widely used in SotA research, there is still controversy (Ali Rahami\u0026rsquo;s Test of Time) about what this algorithm is solving.\n(Santurkar et al, 2018) refuted the Internal Covariate Shift influence. To do so, they compared three models, one baseline, one with BN, and one with random noise added after the normalization.\nBecause of the random noise, the activation\u0026rsquo;s input is not normalized anymore and its distribution change at every time test.\nAs you can see on the following figure, they found that the random shift of distribution didn\u0026rsquo;t produce extremely different results:\n Comparison between standard net, net with BN, and net with noisy BN.   On the other hand they found that the Batch Normalization improved the Lipschitzness of the loss function. In simpler term, the loss is smoother, and thus its gradient as well.\n Figure 3: Loss with and without Batch Normalization.   According to the authors:\n Improved Lipschitzness of the gradients gives us confidence that when we take a larger step in a direction of a computed gradient, this gradient direction remains a fairly accurate estimate of the actual gradient direction after taking that step. It thus enables any (gradient–based) training algorithm to take larger steps without the danger of running into a sudden change of the loss landscape such as flat region (corresponding to vanishing gradient) or sharp local minimum (causing exploding gradients).\n The authors also found that replacing BN by a $l_1$, $l_2$, or $l_{\\infty}$ lead to similar results.\n2. Computing the mean and variance differently Algorithms similar to Batch Norm have been developed where the mean \u0026amp; variance are computed differently.\n source    2.1. Layer Normalization (Ba et al, 2016)\u0026rsquo;s layer norm (LN) normalizes each image of a batch independently using all the channels. The goal is have constant performance with a large batch or a single image. It\u0026rsquo;s used in recurrent neural networks where the number of time steps can differ between tasks.\nWhile all time steps share the same weights, each should have its own statistic. BN needs previously computed batch statistics, which would be impossible if there are more time steps at test time than training time. LN is time steps independent by simply computing the statistics on the incoming input.\n2.2. Instance Normalization (Ulyanov et al, 2016)\u0026rsquo;s instance norm (IN) normalizes each channel of each batch\u0026rsquo;s image independently. The goal is to normalize the constrast of the content image. According to the authors, only the style image contrast should matter.\n2.3. Group Normalization According to (Wu and He, 2018), convolution filters tend to group in related tasks (frequency, shapes, illumination, textures).\nThey normalize each image in a batch independently so the model is batch size independent. Moreover they normalize the channels per group arbitrarily defined (usually 32 channels per group). All filters of a same group should specialize in the same task.\n3. Normalization on the network Previously shown methods normalized the inputs, there are methods were the normalization happen in the network rather than on the data.\n3.1. Weight Normalization (Salimans and Kingma, 2016) found that decoupling the length of the weight vectors from their direction accelerated the training.\nA fully connected layer does the following operation:\n$$y = \\phi(W \\cdot x + b)$$\nIn weight normalization, the weight vectors is expressed the following way:\n$$W = \\frac{g}{\\Vert V \\Vert}V$$\n$g$ and $V$ being respectively a learnable scalar and a learnable matrix.\n3.2. Cosine Normalization (Luo et al, 2017) normalizes both the weights and the input by replacing the classic dot product by a cosine similarity:\n$$y = \\phi(\\frac{W \\cdot X}{\\Vert W \\Vert \\Vert X \\Vert})$$\n4. Conclusion Batch normalization (BN) is still the most represented method among new architectures despite its defect: the dependence on the batch size. Batch renormalization (BR) fixes this problem by adding two new parameters to approximate instance statistics instead of batch statistics.\nLayer norm (LN), instance norm (IN), and group norm (GN), are similar to BN. Their difference lie in the way statistics are computed.\nLN was conceived for RNNs, IN for style transfer, and GN for CNNs.\nFinally weigh norm and cosine norm normalize the network\u0026rsquo;s weight instead of simply the input data.\n","date":1533852000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533852000,"objectID":"f094b978dc490feab2c019f260c36931","permalink":"/post/normalization/","publishdate":"2018-08-10T00:00:00+02:00","relpermalink":"/post/normalization/","section":"post","summary":"Overview of the different normalization methods that exist for neural networks.","tags":[],"title":"Normalization in Deep Learning","type":"post"},{"authors":null,"categories":null,"content":" TP Deep Learning RDFIA RDFIA / Master DAC \u0026amp; IMA / Sorbonne\nLe cours est organisé par le professeur Matthieu Cord. Vos assistants de TPs auquels vous devrez envoyer vos travaux sont Yifu Chen (yifu.chen@lip6.fr) et moi-même Arthur Douillard (arthur.douillard@lip6.fr).\nPour simplifier notre tâche vous êtes priés de nous adresser les mails avec pour objet [RDFIA][TP-\u0026lt;numero\u0026gt;].\nRappels Les TPs seront en Python3 et plusieurs bibliothèques seront utilisées. Voici quelques liens pour rappel, ou pour vous familiariser en avance:\n Rappel de Python Rappel de Numpy Introduction de Scikit-Learn. L\u0026rsquo;api est très similaire quelque soit l\u0026rsquo;algorithme (init / fit / predict) Introduction de Pytorch  Les cours seront ajoutés au fur et à mesure.\nTP 1 - 2 : SIFT / Bag of words 18 \u0026amp; 25 Septembre 2019\n Énoncé: TP1-2.pdf Code et data: TP1-2.zip  Pour ceux n\u0026rsquo;ayant pas réussi à calculer tous les descriptors SIFT du dataset fourni: voici un zip les contenant: sift.zip\nVoici le genre de résultat que vous aurez pu obtenir:\nTP 3 : SVM 2 Octobre 2019\n Énoncé: TP3.pdf Bow 15_scenes_Xy.npz  TP 4-5 : Introduction aux réseaux de neurones 9 \u0026amp; 16 Octobre 2019\n Énoncé: TP4-5.pdf Code: TP4-5.zip\n Math formules: TP4-5_formula.pdf\n  TP 6-7 : Réseaux convolutionnels pour l\u0026rsquo;image 23 \u0026amp; 30 Octobre 2019\n Énoncé: TP6-7.pdf Code: TP6-7.zip  TP 8: Transfer Learning par extraction de features dans un CNN 27 Novembre 2019\n Énoncé: TP8.pdf Code: TP8.zip  TP 9: Visualisation des réseaux de neurones 4 Décembre 2019\n Énoncé: TP9.pdf Code: TP9.zip  TP 10-11: Generative Adversarial Networks 11 Décembre 2019\n Énoncé: TP10-11.pdf Code: TP10-11.zip  Mises à jour: 2019-09-25, 14:16: Ajout d\u0026rsquo;un zip sift + deux images résultats\n2019-10-02, 11:15: Ajout du TP 3 + BoW data.\n2019-10-09, 11:07: Ajout du TP 4-5.\n2019-10-16, 12:52: Ajout des formules forward / backward.\n2019-10-23, 12:42: Ajout du TP 6-7.\n2019-10-26, 17:15: Modification de la date du TP 7 + report de la date de rendu.\n2019-11-27, 13:27: Ajout du TP 8.\n2019-12-04, 12:50: Ajout du TP 9, correction de typo + env var pour le TP 8.\n2019-12-11, 13:15: Ajout du TP 10-11.\n","date":1530136800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530136800,"objectID":"cdbcbe0f0a50ce2bfd03c60c664b3e88","permalink":"/rdfia_2019/","publishdate":"2018-06-28T00:00:00+02:00","relpermalink":"/rdfia_2019/","section":"","summary":"TP Deep Learning RDFIA RDFIA / Master DAC \u0026amp; IMA / Sorbonne\nLe cours est organisé par le professeur Matthieu Cord. Vos assistants de TPs auquels vous devrez envoyer vos travaux sont Yifu Chen (yifu.chen@lip6.fr) et moi-même Arthur Douillard (arthur.douillard@lip6.fr).\nPour simplifier notre tâche vous êtes priés de nous adresser les mails avec pour objet [RDFIA][TP-\u0026lt;numero\u0026gt;].\nRappels Les TPs seront en Python3 et plusieurs bibliothèques seront utilisées. Voici quelques liens pour rappel, ou pour vous familiariser en avance:","tags":null,"title":"TP Deep Learning RDFIA","type":"mylayouts"},{"authors":[],"categories":null,"content":" Imagine you’re in a landlocked country, and an infection has spread. The government has fallen, and rebels are roaming the country. If you’re the armed forces in this scenario, how do you make decisions in this environment? How can you fully understand the situation at hand?\nA few weeks ago, NATO organized an innovation challenge that posed these questions. We decided to take on the challenge with the goal of finding innovative solutions in the areas of data filtering/fusing, visualization, and predictive analytics.\nFor those who don’t know, NATO is an intergovernmental military alliance between 29 North American and European countries. It constitutes a system of collective defense whereby its independent member states agree to mutual defense in response to an attack by any external party.\nNATO did not provide any data for the challenge, so we had to find it ourselves. Ultimately, the solution we came up with used a variety of different techniques including computer vision on aerial imagery, natural language processing on press \u0026amp; social media, geo data processing, and — of course — fancy graphs.\nIn this post, we will focus on the most technical part: object detection for aerial imagery, walking through what kind of data we used, which architecture was employed, and how the solution works, and finally our results. If you’re interested in a higher-level look at the project, that’s over here.\nThis challenge was done while I was an intern at Dataiku. My team was composed of an commander, a salesman, and myself as the lead/sole scientist.\n1. The dataset For the object detection portion of the project, we used the Cars Overhead With Context (COWC) dataset, which is provided by the Lawrence Livermore National Laboratory. It features aerial imagery taken in six distinct locations:\n Toronto, Canada Selwyn, New Zealand Potsdam and Vaihingen*, Germany Columbus (Ohio)* and Utah, USA  * We ultimately did not use the Columbus and Vaihingen data because the imagery was in grayscale.\nThis dataset offers large imagery (up to 4 square kilometers) with good resolution (15cm per pixel) with the center localization of every car. As suggested in this Medium post, we assumed that cars have a mean size of 3 meters. We created boxes centered around each car center to achieve our ultimate goal of predicting box (i.e., car) locations in unseen images.\n Figure 1: An example image from the COWC dataset   2. The architecture To detect cars in these large aerial images, we used the RetinaNet (Lin et al, 2017) architecture. Published in 2017 by Facebook FAIR, this paper won the Best Student Paper of ICCV 2017.\nObject detection architectures are split in two categories: single-stage and two-stage.\nTwo-stage architectures first categorize potential objects in two classes: foreground or background. Then all foreground’s potential objects are classified in more fine-grained classes: cats, dogs, cars, etc. This two-stage method is slow but also, and of course, produces the best accuracy. The most famous two-stage architecture is Faster-RCNN (Ren et al, 2015).\nOn the other hand, single-stage architectures don’t have this pre-selection step of potential foreground objects. They are usually less accurate, but they are also faster. RetinaNet’s single-stage architecture is an exception: it reaches two-stage performance while having single-stage speed!\nOn the figure 2 below, you can see a comparison of object detection architectures.\n Figure 2: Performance of object detection algorithms   RetinaNet is made of four components. We’ll try to describe how the data is transformed through every step.\n Figure 3: The RetinaNet architecture   2.1. Convolutional Network First of all there is a ResNet-50 (He et al., 2015). As every convolutional neural network (CNN), it takes an image as input and processes it through convolution kernels. Each kernel’s output is a feature map — the first feature maps capture high-level features (such as a line or a color). The further we go down in the network, the smaller the feature maps become because of the pooling layers. While they are smaller, they also represent more fined-grained information (such as an eye, a dog ear, etc.). The input image has three channels (red, blue, green), but every subsequent feature map has dozens of channels! Each of them represents a different kind of feature that it captured.\nA common classifier takes the ResNet’s last feature maps (of shape (7, 7, 2048)), applies an average pooling on each channel (resulting in (1, 1, 2048)), and feeds it to a fully connected layer with a softmax.\n2.2. Feature Pyramid Network Instead of adding a classifier after ResNet, RetinaNet adds a Feature Pyramid Network (FPN) (Lin et al., 2016). By picking feature maps at different layers from the ResNet, it provides rich and multi-scale features.\n Figure 4: The lateral connection between the backbone and the FPN   However, ResNet’s first feature maps may be too crude to extract any useful information. As you can see in figure 4, the smaller and more precise feature maps are combined with the bigger feature maps. We first upsample the smaller ones and then sum it with the bigger ones. Several upsampling methods exist; here, the upsampling is done with the nearest neighbor method.\nEach level of the FPN encodes a different kind of information at a different scale. Thus, each of them should participate in the object detection task. The FPN takes as input the output of the third (512 channels), fourth (1024 channels), and fifth (2048 channels) blocks of ResNet. The third is half the size of the fourth, and the fourth is half of the fifth.\nWe apply pointwise convolution (convolution with a 1x1 kernel) to uniformize the number of channels of each level to 256. Then we upsampled the smaller levels by a factor of two to match the dimension of the bigger levels.\n2.3. Anchors At each FPN level, anchors are moved around the FPN’s feature maps. An anchor is a rectangle with different sizes and ratios, like this:\n Figure 5: A sample of anchors of different sizes and ratios   These anchors are the base position of the potential objects. Five sizes and three ratios exist, thus there are 15 unique anchors. These anchors are also scaled according to the dimension of the FPN levels. These unique anchors are duplicated on all the possible positions in the feature maps. It results in $K$ total anchors.\nLet’s put aside those anchors for the moment.\n2.4. Regression \u0026amp; classification Each FPN’s level is fed to two Fully Convolutional Networks (FCN), which are neural networks made only of convolutions and pooling. To fully exploit the fact that every FPN’s level holds different kind of information, the two FCNs are shared among all levels! Convolution layers are independent of the input size; only their kernel size matter. Thus while each FPN’s feature maps have different sizes, they can be all fed to the same FCNs.\nThe first FCN is the regression branch. It predicts $K x 4$ (x1, y1, x2, y2 for each anchor) values. Those values are deltas that slightly modify the original anchors so they fit the potential objects better. All the potential objects will now have coordinates of the type:\n(x1 + dx1, y1 + dy1, x2 + dx2, y2 + dy2)  With x? and y?, the fixed coordinates of the anchors, and dx?, dy?, the deltas produced by the regression branch.\nWe now have the final coordinates for all objects — that is, all potential objects. They are not yet classified as background or car, truck, etc.\nThe second FCN is the classification branch. It is a multi-label problem where the classifier predicts $K x N$ ($N$ being the number of classes) potential objects with sigmoid.\n2.5. Removing duplicates At this point we have $K x 4$ coordinates and $K x N$ class scores. We now have a problem: it is common to detect, for the same class, several boxes for a same object!\n Figure 6: Several boxes have been detected for a single car.   Therefore, for each class (even if it’s not the highest scoring class) we apply a Non-max suppression. Tensorflow provides a function to do it:\ntf.image.non_max_suppression(boxes, scores, max_output_size, iou_threshold) The main gist of this method is that it will remove overlapping boxes (such as in Figure 6) to keep only one. It also using the scores to keep the most probable box.\nA general comment on the input parameter of the Tensorflow method above: The max_output_size corresponds to the maximum number of boxes we want at the end — let’s say 300. The iou_threshold is a float between 0 and 1, describing the maximum ratio of overlapping that is accepted.\n Figure 7: Figure 6 after the non-max-suppression has been applied.   2.6. Keeping the most probable class Duplicate boxes for the same class at the same place are now removed. For each of the remaining boxes, we are keeping only the highest-scoring class (car, truck, etc.). If none of the classes have a score above a fixed threshold (we used $0.4$), it’s considered to be part of the background.\n2.7. The Focal Loss All this may sound complicated, but it’s nothing new — it’s not enough to have good accuracy. The real improvement from RetinaNet is its loss: the Focal Loss. Single-stage architectures that don’t have potential objects pre-selection are overwhelmed with the high frequency of background objects. The Focal Loss deals with it by according a low weight to well-classified examples, usually the background.\n Figure 8: We define Pt, the confidence to be right   In Figure 8, we define $p_t$, the confidence to be right in a binary classification.\n Figure 9: The Focal Loss   In Figure 9, we module the cross entropy loss $-\\log(p_t)$ by a factor $(1 — p_t)^\\gamma$. Here, $\\gamma$ is a modulating factor oscillating between 0 and 5. The well-classified examples have a high $p_t$ , and thus a low factor. Therefore, the loss for well-classified examples is low and forces the model learn on harder examples. You can see in Figure 10 how much the loss is affected.\n Figure 10: The focal loss under various modulating factors   3. Implementation We used the excellent Keras implementation of RetinaNet by Fizyr. We also wrote a new generator, taking Pandas’ DataFrames instead of CSV files.\nclass DfGenerator(CSVGenerator): def __init__(self, df, class_mapping, cols, base_dir=\u0026#39;\u0026#39;, **kwargs): \u0026#34;\u0026#34;\u0026#34;Custom generator intended to work with in-memory Pandas\u0026#39; dataframe. Arguments: df: Pandas DataFrame containing paths, labels, and bounding boxes. class_mapping: Dict mapping label_str to id_int. cols: Dict Mapping \u0026#39;col_{filename/label/x1/y1/x2/y2} to corresponding df col. \u0026#34;\u0026#34;\u0026#34; self.base_dir = base_dir self.cols = cols self.classes = class_mapping self.labels = {v: k for k, v in self.classes.items()} self.image_data = self._read_data(df) self.image_names = list(self.image_data.keys()) Generator.__init__(self, **kwargs) def _read_classes(self, df): return {row[0]: row[1] for _, row in df.iterrows()} def __len__(self): return len(self.image_names) def _read_data(self, df): data = {} for _, row in df.iterrows(): img_file, class_name = row[self.cols[\u0026#39;col_filename\u0026#39;]], row[self.cols[\u0026#39;col_label\u0026#39;]] x1, y1 = row[self.cols[\u0026#39;col_x1\u0026#39;]], row[self.cols[\u0026#39;col_y1\u0026#39;]] x2, y2 = row[self.cols[\u0026#39;col_x2\u0026#39;]], row[self.cols[\u0026#39;col_y2\u0026#39;]] if img_file not in data: data[img_file] = [] # Image without annotations if not isinstance(class_name, str) and np.isnan(class_name): continue data[img_file].append({ \u0026#39;x1\u0026#39;: int(x1), \u0026#39;x2\u0026#39;: int(x2), \u0026#39;y1\u0026#39;: int(y1), \u0026#39;y2\u0026#39;: int(y2), \u0026#39;class\u0026#39;: class_name }) return data As you can see, images without annotations are kept in the training phase. They still help the training of our algorithm, as it forces the algorithm to not see cars everywhere (even where there aren’t any).\nWe used a pre-trained RetinaNet on COCO and then fine-tuned it for the COWC dataset. Only the two FCNs are retrained for this new task, while the ResNet backbone and the FPN are frozen.\nYou can see in the code block below how to load the RetinaNet and compile it. Note that it is important to add skip_mismatch=True when loading the weights! The weights were created on COCO with 80 classes, but in our case we only have 1 class, thus the number of anchors is not the same.\ndef load_retinanet(weights, n_classes, freeze=True): modifier = freeze_model if freeze else None model = resnet50_retinanet(num_classes=num_classes, modifier=modifier) model.load_weights(weights, by_name=True, skip_mismatch=True) return model def compile(model): model.compile( loss={ \u0026#39;regression\u0026#39; : keras_retinanet.losses.smooth_l1(), \u0026#39;classification\u0026#39;: keras_retinanet.losses.focal() }, optimizer=optimizers.adam(lr=configs[\u0026#39;lr\u0026#39;], clipnorm=0.001) ) def train(model, train_gen, val_gen, callbacks, n_epochs=20): \u0026#34;\u0026#34;\u0026#34;train_gen and val_gen are instances of DfGenerator.\u0026#34;\u0026#34;\u0026#34; model.fit_generator( train_gen, steps_per_epoch=len(train_gen), validation_data=val_gen, validation_steps=len(val_gen), callbacks=callbacks, epochs=n_epochs, verbose=2 ) There is something we still need to deal with, which is the massive weight of each image. Images from the COWC dataset are up to 4 square kilometers, or 13k pixel wide and high. Those big images weigh 300mb. It is impracticable to feed such large images to our RetinaNet. Therefore, we cut the images in patches of 1000x1000 pixels (or 150x150 meters).\nHowever, it would be stupid to miss cars because they’d been cut between two patches. So to avoid this problem, we made a sliding window of 1000x1000 pixels that moves by steps of 800 pixels. That way, there is a 200-pixel-wide overlap between two adjacent patches.\nThis leads to another problem: we may detect cars twice. To remove duplicates, we applied non-max suppression when binding together the small patches. Indeed, that means we have a non-max suppression twice: after the RetinaNet and when binding together the small patches. For the second non-max suppression, we used a Numpy version of the algorithm. You can either use a fast \u0026amp; vectorized version by PyImageSearch, or the following naive version:\ndef jaccard(a, b): \u0026#34;\u0026#34;\u0026#34;Compute the jaccard score between box a and box b.\u0026#34;\u0026#34;\u0026#34; side1 = max(0, min(a[2], b[2]) - max(a[0], b[0])) side2 = max(0, min(a[3], b[3]) - max(a[1], b[1])) inter = side1 * side2 area_a = (a[2] - a[0]) * (a[3] - a[1]) area_b = (b[2] - b[0]) * (b[3] - b[1]) union = area_a + area_b - inter return inter / union def naive_nms(boxes, scores, threshold=0.4): scores_idx = scores.argsort()[::-1] # Keep highest scores first boxes = boxes[scores_idx] indices_to_skip = set() for i in range(boxes.shape[0]): for j in range(boxes.shape[0]): if i == j or j in indices_to_skip: continue if jaccard(boxes[i], boxes[j]) \u0026gt; threshold: indices_to_skip.add(j) mask = np.ones(boxes.shape[0], np.bool) mask[np.array(list(indices_to_skip))] = 0 return boxes[mask] When dealing with aerial imagery, we can use a lot of data augmentation. First of all, we can flip the horizontal axis and the vertical axis. We can also rotate the image by any angle. If the imagery’s scale is not uniform (the distance drone-to-ground may not be constant), it is also useful to randomly scale down and up the pictures.\n4. Results You can see on Figures 11 and 12 below how our RetinaNet behaves on this unseen image of Salt Lake City.\n Figure 11: 13,000 detected cars in a 4 square kilometer area of Salt Lake City    Figure 12: A zoom in of Figure 11   5. Are we good? How can we evaluate our performance?\nAccuracy is not enough; we need to see how many false positives and false negatives we get. If we detect cars everywhere, we’d have a lot of false positive, but if we miss most of the cars, that’s a lot of false negative.\nThe recall measures the former while the precision measures the latter. Finally, the f1-score is a combination of those two metrics.\ndef compute_metrics(true_pos, false_pos, false_neg): \u0026#34;\u0026#34;\u0026#34;Compute the precision, recall, and f1 score.\u0026#34;\u0026#34;\u0026#34; precision = true_pos / (true_pos + false_pos) recall = true_pos / (true_pos + false_neg) if precision == 0 or recall == 0: return precision, recall, 0 f1 = 2 / (1 / precision + 1 / recall) return precision, recall, f1 However, we are not expecting our RetinaNet to detect the cars at the exact right pixels. Therefore, we are computing the Jaccard Index of the detected cars and the ground-truth cars, and if it is more than a chosen threshold, we consider that the car was rightfully detected. Note that the Jaccard index is often also (blandly) called Intersection-over-Union (IoU):\ndef jaccard(box_a, box_b): \u0026#34;\u0026#34;\u0026#34;box_X is a tuple of the shape (x1, y1, x2, y2).\u0026#34;\u0026#34;\u0026#34; side1 = max(0, min(a[2], b[2]) - max(a[0], b[0])) side2 = max(0, min(a[3], b[3]) - max(a[1], b[1])) inter = side1 * side2 area_a = (a[2] - a[0]) * (a[3] - a[1]) area_b = (b[2] - b[0]) * (b[3] - b[1]) union = area_a + area_b - inter return inter / union def is_valid(box_pred, box_true, threshold=0.3): return jaccard(box_red, box_true) \u0026gt;= threshold  Figure 13: True Positive (green), False Positive (yellow), and False Negative (red)   You can see a sample on Figure 13 where true positives, false positives, and false negatives have been plotted.\nNote that among the four false positives, two of them are garbage bins, one is a duplicate, and one is actually\u0026hellip; a car! Indeed, as in every dataset, there may be some errors in the ground-truth annotations.\nOn Figure 12, the f1-score is $0.91$. Usually in more urban environments the f1-score is around $0.95$. The main mistake our model makes is considering ventilation shafts on tops of buildings to be cars. To the model’s defense, without knowledge of building, it’s quite hard to see that.\n6. Conclusion For the NATO challenge, we didn’t only use car detection from aerial imagery, but it was the main technical part of the project.\nOh\u0026hellip; Did we forget to tell you the challenge results?\nThree prizes were awarded: The NATO prize (with a trip to Norfolk), the France prize (with $25k), and the Germany prize (with a trip to Berlin).\nWe won both the NATO and France prize!\n Figure 14: General Maurice, Supreme Commander Mercier, and our team   Thanks to Hicham El Boukkouri \u0026amp; Léo Dreyfus-Schmidt for their review of this blog post.\nNote that this post was at first published on Dataiku\u0026rsquo;s technical blog on Medium.\n","date":1529618400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1529618400,"objectID":"eb7349e7afb7086dc0b7ac6c38abb6f8","permalink":"/post/nato-challenge/","publishdate":"2018-06-22T00:00:00+02:00","relpermalink":"/post/nato-challenge/","section":"post","summary":"Imagine you’re in a landlocked country, and an infection has spread. The gouvernement has fallen, and rebels are roaming the country. If you’re the armed forces in this scenario, how do you make decisions in this environment? How can you fully understand the situation at hand?","tags":[],"title":"Detecting cars from aerial imagery for the NATO Innovation Challenge","type":"post"},{"authors":[],"categories":null,"content":"PhD Grind (pdf url) is not strictly speaking a book, but rather a pdf memoir. It is the story of a PhD, during its six long years, in the USA in Computer Science (Model checking \u0026amp; HCI).\nI found this book fantastic. As a first-year PhD, it really talked to me. I was interested to discover the PhD system in the USA, which sounds quite different from what I know in France.\nI also very much enjoyed the unfiltered speech of a student who had doubts, failures, and success (spoiler: there is a happy ending.)\nIf I find motivation, and time, I would like to write such memoir about my own thesis.\n","date":1526335200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526335200,"objectID":"83293c134ec621bf09232984a7c08cf2","permalink":"/books/phd-grind/","publishdate":"2018-05-15T00:00:00+02:00","relpermalink":"/books/phd-grind/","section":"books","summary":"PhD Grind (pdf url) is not strictly speaking a book, but rather a pdf memoir. It is the story of a PhD, during its six long years, in the USA in Computer Science (Model checking \u0026amp; HCI).\nI found this book fantastic. As a first-year PhD, it really talked to me. I was interested to discover the PhD system in the USA, which sounds quite different from what I know in France.","tags":[],"title":"PhD Grind","type":"books"},{"authors":[],"categories":null,"content":" Most CNN architectures have been developed to attain the best accuracy on ImageNet. Computing power is not limited for this competition, why bother?\nHowever you may want to run your model on an old laptop, maybe without GPU, or even on your mobile phone. Let’s see three CNN architectures that are efficient while sacrificing little accuracy performance.\n1. MobileNet Arxiv link: (Howard et al, 2017)\nMobileNet uses depthwise separable convolutions. This convolution block was at first introduced by Xception (Chollet, 2016). A depthwise separable convolution is made of two operations: a depthwise convolution and a pointwise convolution.\nA standard convolution works on the spatial dimension of the feature maps and on the input and output channels. It has a computational cost of $D_f^2 * M * N * D_k^2$; with $D_f$ the dimension of the input feature maps, $M$ and $N$ the number of input and output channels, and $D_k$ the kernel size.\nA depthwise convolution maps a single convolution on each input channel separately. Therefore its number of output channels is the same of the number of input channels. Its computational cost is $D_f^2 * M * D_k^2$.\nThe last operation is a pointwise convolution. It is a convolution with a kernel size of 1x1 that simply combines the features created by the depthwise convolution. Its computational cost is $M * N * D_f^2$.\nThe computational cost of the depthwise separable convolution is the sum of the costs of the depthwise and pointwise operations. Compared to a standard convolution it offers a computation reduction of $\\frac{1}{N} + \\frac{1}{D_k^2}$. With a kernel size of 3x3, it results in 8 times less operations!\nMobileNet also provides two parameters allowing to reduce further more its number of operations:\nThe width multiplier (between 0 and 1) thins the number of channels. At each layer instead of producing $N$ channels, it will produce $\\alpha * N$. This multiplier can be used to handle a trade-off between the desired latency and the performance.\nAnother multiplier exists: the resolution multiplier. It scales the input size of the image, between 224 to 128. Because the MobileNet uses a global average pooling instead of a flatten, you can train your MobileNet on 224x224 images, then use it on 128x128 images! Indeed with a global pooling, the fully connected classifier at the end of the network depends only the number of channels not the feature maps spatial dimension.\n2. ShuffleNet Arxiv link: (Zhang et al, 2017)\nShuffleNet introduces the three variants of the Shuffle unit. It is composed of group convolutions and channel shuffles.\nA group convolution is simply several convolutions, each taking a portion of the input channels. In the following image you can see a group convolution, with 3 groups, each taking one of the 3 input channels.\nIt was at first introduced by AlexNet (Krizhevsky et al, 2012) to split a network into two GPUs.\nIt greatly diminishes the computational cost. Let us take a practicable example: If there are 4 input channels, and 8 output channels and we choose to have two groups, each taking 2 input channels and 4 output channels.\nWith one group the computational cost would be $D_f^2 * D_k^2 * 4 * 8$, while with two groups the cost is $(D_f^2 * D_k^2 * 2 * 4) * 2$ or $D_f^2 * D_k^2 * 4 * 4$. Half as many operations! The authors reached best results with 8 groups, thus the reduction is even more important.\nFinally the authors add a channel shuffle that randomly mix the output channels of the group convolution. The trick to produce this randomness can be seen here.\n3. EffNet Arxiv link: (Freeman et al, 2018)\nEffNet uses spatial separable convolutions. It is similar to MobileNet\u0026rsquo;s depthwise separable convolutions.\nThe separable depthwise convolution is the rectangle colored in blue for EffNet block. It is made of depthwise convolution with a line kernel (1x3), followed by a separable pooling, and finished by a depthwise convolution with a column kernel (3x1)\nLet\u0026rsquo;s see the computational gain. A normal depthwise with a 3x3 kernel would have a cost of $3^2 * D_f^2 * M$. The first depthwise with a 1x3 kernel has a computational cost of $3 * D_f^2 * M$. The separable pooling halves the feature maps height and has a marginal cost. The second depthwise, with a 3x1 kernel, has then a cost of $3 * \\frac{D_f^2}{2} * M$. Thus the whole cost is $1.5 * (3 * D_f^2 * M)$. Half less than the normal depthwise!\nAnother optimization done by EffNet over MobileNet and ShuffleNet, is the absence of \u0026ldquo;normal convolution\u0026rdquo; at the beginning:\nTo quote the authors (emphasis mine):\n Both MobileNet and ShuffleNet avoided replacing the first layer with the claim that this layer is already rather cheap to begin with. We respectfully disagree with this claim and believe that every optimisation counts. After having optimised the rest of the layers in the network, the first layer becomes proportionally larger. In our experiments, replacing the first layer with our EffNet block saves ∼ 30% of the computations for the respective layer.\n 4. Conclusion MobileNet, ShuffleNet, and EffNet are CNN architectures conceived to optimize the number of operations. Each replaced the standard convolution with their own version.\nMobileNet (github) depthwise separable convolution uses a depthwise convolution followed by a pointwise convolution. In a addition it introduces two hyperparameters: the width multiplier that thins the number of channels, and the resolution multiplier that reduces the feature maps spatial dimensions.\nShuffleNet (github) uses pointwise convolution in groups. In order to combine the features produced by each group, a shuffle layer is also introduced.\nFinally EffNet (github) uses spatial separable convolution, which is simply a depthwise convolution splitted along spatial axis with a separable pooling between them.\nThis article was at first published in Towards Data Science and has also been translated in Chinese!\n","date":1526162400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526162400,"objectID":"68be5d402127d1ae61e2ed019aa46833","permalink":"/post/3-small-but-powerful-cnn/","publishdate":"2018-05-13T00:00:00+02:00","relpermalink":"/post/3-small-but-powerful-cnn/","section":"post","summary":"Overview of three CNN that have a low computational cost","tags":[],"title":"3 Small but Powerful Convolutional Neural Networks","type":"post"},{"authors":[],"categories":null,"content":"Ball Lightning is an early book of Cixin Liu, the Three-Body Problem trilogy author. While not as good as its trilogy, the book was interesting to read because of its theme: Science.\nThe protagonists try to understand the ball lightning (a real phenomenon). The whole book is structured around their experiments, their thoughts, and failures. The end of the book goes a bit far, and is clearly \u0026ldquo;science-fiction\u0026rdquo;, but still in a \u0026ldquo;hard\u0026rdquo;, plausible way. This won\u0026rsquo;t surprise those who read the trilogy.\n","date":1526162400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526162400,"objectID":"202f35eb5e6109adaf90fadfa7e6966a","permalink":"/books/ball-lightning/","publishdate":"2018-05-13T00:00:00+02:00","relpermalink":"/books/ball-lightning/","section":"books","summary":"Ball Lightning is an early book of Cixin Liu, the Three-Body Problem trilogy author. While not as good as its trilogy, the book was interesting to read because of its theme: Science.\nThe protagonists try to understand the ball lightning (a real phenomenon). The whole book is structured around their experiments, their thoughts, and failures. The end of the book goes a bit far, and is clearly \u0026ldquo;science-fiction\u0026rdquo;, but still in a \u0026ldquo;hard\u0026rdquo;, plausible way.","tags":[],"title":"Ball Lightning","type":"books"},{"authors":[],"categories":null,"content":"This is the story of a father and his son. Coming from Beijing, they come to London with different goals. The son hopes to learn and to held a business, the father just follow the stream.\nThe story itself isn\u0026rsquo;t very interesting. What I liked in this book was the description of racism and inter-generational gap.\nEnglish men are deeply racist against Chinese men, but the latter \u0026mdash;especially the old generation\u0026ndash; also hold preconceived ideas on the western culture.\n","date":1526162400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526162400,"objectID":"3de4da1503a1acd8f380ccd5129789e3","permalink":"/books/mr-ma-and-son/","publishdate":"2018-05-13T00:00:00+02:00","relpermalink":"/books/mr-ma-and-son/","section":"books","summary":"This is the story of a father and his son. Coming from Beijing, they come to London with different goals. The son hopes to learn and to held a business, the father just follow the stream.\nThe story itself isn\u0026rsquo;t very interesting. What I liked in this book was the description of racism and inter-generational gap.\nEnglish men are deeply racist against Chinese men, but the latter \u0026mdash;especially the old generation\u0026ndash; also hold preconceived ideas on the western culture.","tags":[],"title":"Mr Ma and Son","type":"books"},{"authors":[],"categories":null,"content":"https://www.gatesnotes.com/Books/The-Ride-of-a-Lifetime\n","date":1526162400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526162400,"objectID":"590e45269e790dc72fe0e00a4150c24a","permalink":"/books/ride-of-a-lifetime/","publishdate":"2018-05-13T00:00:00+02:00","relpermalink":"/books/ride-of-a-lifetime/","section":"books","summary":"https://www.gatesnotes.com/Books/The-Ride-of-a-Lifetime","tags":[],"title":"The Ride of a Lifetime","type":"books"},{"authors":[],"categories":null,"content":"Except the Sun Tzu\u0026rsquo;s Art of War, the Three-Body Problem first tome was my first Chinese book. The first book set the tone, starting in communist-era in the 70s.\nWhen I introduce this trilogy to friends, I often say \u0026mdash;no spoilers:\n Several of world best theoretical physicists suicide. They all leave a note, saying in substance: \u0026ldquo;Physic doesn\u0026rsquo;t exist\u0026rdquo;.\n You don\u0026rsquo;t need to know more to go buy this book. It\u0026rsquo;s hard-science-fiction at its best.\nWARNING, SPOILERS-AHEAD:\nThe first half of the first book (The Three-Body Problem) is very good. The initial problem, physic doesn\u0026rsquo;t exist, is both original and extremely exciting. I dislike the second half, especially the explanation about sophons and how they would communicate by quantic entanglement (which doesn\u0026rsquo;t work like this).\nThe second book was particularly surprising. I really like the Wallfacers concepts: four men whose objective is to find a way to defeat high-tech aliens in 300 years. And they have to keep their plans secret to avoid leaking info through the sophons. The jump in the future was similar to the movie the 5th element where everything just seems crazy. And finally come the key idea of this book, and trilogy: the dark forest hypothesis. It\u0026rsquo;s both terrifying and plausible.\nThe third book is at first nice, a lot of tech progress is done. However its very end seems a bit far fetched, closer to fantasy than hard-sf. But I guess when talking about eons in the future, anything is possible.\n","date":1526162400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526162400,"objectID":"1c93f05b818253d22a35052bf95641b8","permalink":"/books/three-body/","publishdate":"2018-05-13T00:00:00+02:00","relpermalink":"/books/three-body/","section":"books","summary":"Except the Sun Tzu\u0026rsquo;s Art of War, the Three-Body Problem first tome was my first Chinese book. The first book set the tone, starting in communist-era in the 70s.\nWhen I introduce this trilogy to friends, I often say \u0026mdash;no spoilers:\n Several of world best theoretical physicists suicide. They all leave a note, saying in substance: \u0026ldquo;Physic doesn\u0026rsquo;t exist\u0026rdquo;.\n You don\u0026rsquo;t need to know more to go buy this book.","tags":[],"title":"The Three-Body Problem trilogy","type":"books"},{"authors":[],"categories":null,"content":"Very Short Introduction (VSI) is an Oxford collection about various subjects, ranging from humanities (arts, religion, politics, etc.) to very technical field (neuroscience, physics, etc.). With a small format (~100 pages), they aim to be a good introduction.\nVSI: Food fulfills perfectly this mission. The book is simple enough for a neophyte in food to understand, and still manages to cover a wide breadth of topics, all centered around food. It\u0026rsquo;s far easier than its cousin VSI: Nutrition.\n","date":1526162400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1526162400,"objectID":"53a249e44f36ac9b849e1d545ea8d002","permalink":"/books/very-short-intro-food/","publishdate":"2018-05-13T00:00:00+02:00","relpermalink":"/books/very-short-intro-food/","section":"books","summary":"Very Short Introduction (VSI) is an Oxford collection about various subjects, ranging from humanities (arts, religion, politics, etc.) to very technical field (neuroscience, physics, etc.). With a small format (~100 pages), they aim to be a good introduction.\nVSI: Food fulfills perfectly this mission. The book is simple enough for a neophyte in food to understand, and still manages to cover a wide breadth of topics, all centered around food. It\u0026rsquo;s far easier than its cousin VSI: Nutrition.","tags":[],"title":"Very Short Introduction: Food","type":"books"},{"authors":[],"categories":null,"content":" This article contains note of the research paper:\n Densely Connected Convolutional Networks by Cornell Uni, Tsinghua Uni, and Facebook Research.  This paper was awarded the CVPR 2017 Best Paper Award.\nIntroduction DenseNet is a new CNN architecture that reached State-Of-The-Art (SOTA) results on classification datasets (CIFAR, SVHN, ImageNet) using less parameters.\nThanks to its new use of residual it can be deeper than the usual networks and still be easy to optimize.\nGeneral Architecture DenseNet is composed of Dense blocks. In those blocks, the layers are densely connected together: Each layer receive in input all previous layers output feature maps.\nThis extreme use of residual creates a deep supervision because each layer receive more supervision from the loss function thanks to the shorter connections.\n1. Dense block A dense block is a group of layers connected to all their previous layers. A single layer looks like this:\n Batch Normalization ReLU activation 3x3 Convolution  The authors found that the pre-activation mode (BN and ReLU before the Conv) was more efficient than the usual post-activation mode.\nNote that the authors recommend a zero padding before the convolution in order to have a fixed size.\n2. Transition layer Instead of summing the residual like in ResNet, DenseNet concatenates all the feature maps.\nIt would be impracticable to concatenate feature maps of different sizes (although some resizing may work). Thus in each dense block, the feature maps of each layer has the same size.\nHowever down-sampling is essential to CNN. Transition layers between two dense blocks assure this role.\nA transition layer is made of:\n Batch Normalization 1x1 Convolution Average pooling  Growth rate Concatenating residuals instead of summing them has a downside when the model is very deep: It generates a lot of input channels!\nYou may now wonder how could I say in the introduction that DenseNet has less parameters than an usual SotA networks. There are two reasons:\nFirst of all a DenseNet\u0026rsquo;s convolution generates a low number of feature maps. The authors recommend 32 for optimal performance but shows SotA results with only 12 output channels!\nThe number of output feature maps of a layer is defined as the growth rate.\nDenseNet has lower need of wide layers because as layers are densely connected there is little redundancy in the learned features. All layers of a same dense block share a collective knowledge.\n The growth rate regulates how much new information each layer contributes to the global state.\n Bottleneck The second reason DenseNet has few parameters despite concatenating many residuals together is that each 3x3 convolution can be upgraded with a bottleneck.\nA layer of a dense block with a bottleneck will be:\n Batch Normalization ReLU activation 1x1 Convolution bottleneck producing: $\\text{grow rate} * 4$ feature maps.\n Batch Normalization\n ReLU activation\n 3x3 Convolution\n  With a growth rate of 32, the tenth layer would have in input 288 feature maps! Thanks to the bottleneck at most 128 feature maps would be fed to a layer. This helps the network have hundred, if not thousand, layers.\nCompression The authors further improves the compactness of the model with a compression. This compression happens in the transition layer.\nNormally the transition layer\u0026rsquo;s convolution does not change the number of feature maps. In the case of the compression, its number of output feature maps is $\\theta * m$. With $m$ the number of input feature maps and $\\theta$ a compression factor between 0 and 1.\nNote that the compression factor $\\theta$ has the same role as the parameter $\\alpha$ in MobileNet.\nConclusion The final architecture of DenseNet is the following:\nTo summarize, the DenseNet architecture uses the residual mechanism to its maximum by making every layer (of a same dense block) connect to their subsequent layers.\nThis model\u0026rsquo;s compactness makes the learned features non-redundant as they are all shared through a common knowledge.\nIt is also far more easy to train deep network with the dense connections because of an implicit deep supervision where the gradient is flowing back more easily thanks to the short connections.\n","date":1525644000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525644000,"objectID":"0acb7ec417012047b05f0d6569fcf37f","permalink":"/post/densenet/","publishdate":"2018-05-07T00:00:00+02:00","relpermalink":"/post/densenet/","section":"post","summary":"All about the architecture DenseNet.","tags":[],"title":"Densely Connected Convolutional Networks","type":"post"},{"authors":[],"categories":null,"content":" This post contains the notes taken from the following paper:\n Deep Learning Scaling Is Predictable, Empirically by Baidu Research.  The last years in Deep Learning have seen a rush to gigantism:\n Models are becoming deeper and deeper from the 8 layers of AlexNet to the 1001-layer ResNet. Training on large dataset is way quicker, ImageNet can now (with enough computing power) been trained in less than 20 minutes. Dataset size are increasing each year.  As this paper rightly declare in its introduction:\n The Deep Learning (DL) community has created impactful advances across diverse application domains by following a straightforward recipe: search for improved model architectures, create large training data sets, and scale computation.\n However it also notes that new models and hyperparameters configuration are often depend on epiphany and serendipity.\nIn order to harness the power of big data (more data, more computation power, etc.) models should not be designed to reduce error rate of an epsilon on Imagenet but be designed to be better with more data.\nBaidu Research introduce a power-law expononent, that measure the steepness of the learning curve:\n$$\\epsilon(m) \\propto \\alpha m^{\\beta_g}$$\nWhere $\\epsilon(m)$ is the generalization error on the number of train samples $m$; $\\alpha$ a constant related to the problem; and $\\beta_g$ the steepness of the learning curve.\n$\\beta_g$ is said to settle between -0.07 and -0.35.\nThe Methodology Baidu tested four domains: machine translation, language modeling, image classification, and speech recognition.\nFor each domain, a variety of architectures, optimizers, and hyperparameters is tested. To see how models scale with dataset size, Baidu trained models on samples ranging from 0.1% of the original data to the whole data (minus the validation set).\nThe paper\u0026rsquo;s authors try to find the smallest model that is able to overfit each sample.\nBaidu also removed any regularizations, like weight decay, that might reduce the model\u0026rsquo;s effective capacity.\nResults In all domain, they found that the model size growth with dataset size sublinearly.\n   Domain Learning Curve Steepness $\\beta_g$     Machine Translation -0.128   Language Modeling [-0.09, -0.06]   Image (top-1) -0.309   Image (top-5) -0.488   Speech -0.299    The first thing that we can conclude from these numbers is that text based problems (translation and language modeling) scale badly faced to image problems.\nIt is worth noting that (current) models seem to scale better depending on the data dimension: Image and speech are of a higher dimensionality than text.\nYou may also wonder why image has two entries in the table. One for top-1 generalization error, and one for top-5. This is one of the most interesting finding of this paper. Current models of image classification improve their top-5 faster than top-1 as data size increases! I wonder the reason why.\nImplications The authors separate the generalization error per data size in three areas:\n The small data region, where models given so few data can only make random guessing. The power law region, where models follow the power law. However the learning curve steepness may be improved. The irreductible error, a combination of the Bayes error (on which the model cannot be improved) and the dataset defects that may impair generalization.  The authors also underline major implications of the power law:\nGiven the power law, researchers can train their new architecture on a small dataset, and have a good estimation of how it would scale on a bigger dataset. It may also give a reasonable estimation of the hardware and time requirements to reach a chosen generalization error.\nInstead of simply trying to improve a model\u0026rsquo;s accuracy, the authors suggest that beating the power law should be the end goal. Dataset size is going to grow each year, a scalable model would thrive in this situation. The authors advise methods that may help to extract more info on less data:\n We suggest that future work more deeply analyze learning curves when using data handling techniques, such as data filtering/augmentation, few-shot learning, experience replay, and generative adversarial networks.\n Baidu also recommends to search how to push the boundaries of the irreductible error. To do that we should be able to distinguish between what contributes to the bayes error, and what\u0026rsquo;s not.\nSummary Baidu Research showed that models follow a power law curve. They empirically determined the power law exponent, or steepness of the learning curve, for machine translation, language modeling, image classification, and speech recognition.\nThis power law express how much a model can improve given more data. Models for text problems are currently the less scalable.\n","date":1524175200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524175200,"objectID":"f43a3703e323019f0569a6ece24bbe1e","permalink":"/post/deep-learning-scaling/","publishdate":"2018-04-20T00:00:00+02:00","relpermalink":"/post/deep-learning-scaling/","section":"post","summary":"How much a Deep Learning model would improve if the dataset size increase?","tags":[],"title":"Deep Learning Scaling Is Predictable, Empirically","type":"post"},{"authors":[],"categories":null,"content":" This post contains the notes taken from the following paper:\n Fast-RCNN by R. Girshick.\n Faster-RCNN by Microsoft Research.\n  Ross Girshick is an influential researcher on object detection: he has worked on RCNN, Fast{er}-RCNN, Yolo, RetinaNet\u0026hellip;\nFast-RCNN and Faster-RCNN are both incremental improvements on the original RCNN.\nLet\u0026rsquo;s see what were those improvements:\nFast-RCNN In Fast-RCNN, Girshick ditched the SVM used previously. It resulted in a 10x inference speed improvement, and a better accuracy.\nGirshick replaced the SVM by a Region of Interest (RoI) pooling. RoIs are still produced by the selective search, and they are used to select a subset of the feature map produced from the whole image:\nAt the end of the CNN, without top, a feature map is generated by filter.\nLet\u0026rsquo;s consider as example an input image of size 10x10; At the end of the CNN, the feature map has a size of 5x5. If the selective search proposes a box between (top-left and bottom-right) (0, 2) and (6, 8) then we extract a similar box from the feature map. However this box is proportionally scaled down:\nObjects have different sizes, and so are the boxes extracted from the feature maps. To normalize their size a max pooling is done. Note that it does not really matter if the height or width of the extracted box is not even:\nThose extracted fixed-size feature maps (one per filter per object) are then fed to fully connected layers. At some point, the network split into two sub-networks. One is designed to classify the class with a softmax activation. The other is a regressor with 4 values: The coordinates of the top-left point of the box and its width \u0026amp; height.\nNote that if you want to train your RCNN to detect $K$ classes, the sub-network detecting the box\u0026rsquo;s class will choose between $K + 1$ classes. The extra class is the ubiquitous background. The bounding-box regressor\u0026rsquo;s loss won\u0026rsquo;t be taken in account if a background is detected.\nFaster-RCNN The main contribution of Fast-RCNN was the RoI pooling followed by a two-headed fully connected network. Faster-RCNN eliminated another speed bottleneck: The generation of the region proposals by selective search:\n Fast R-CNN, achieves near real-time rates using very deep networks, when ignoring the time spent on region proposals. Now, proposals are the test-time computational bottleneck in state-of-the-art detection systems.\n The authors introduced the Region Proposal Network (RPN) to fix this problem.\nRegion Proposal Network RPN generates region proposals that are given to the classifier which is Fast-RCNN.\nFirst of all the feature maps are reduced to intermediate layers of smaller size. The authors used a layer of dimension 512 when the feature maps were originating from VGG16.\nThen the RPN uses a sliding window, moving all around the intermediate layers. At each location, anchors are used. An anchor is simply a box of a pre-defined size and shape. 9 different anchors exist: there are 3 different scales and 3 different ratios.\nThe first two scales, and the three possible ratios.\nFor each possible anchor a mini-network is used for two tasks:\n classify whether the location is a background or an actual object. Predict the exact bounding-box coordinates and width.  With $k = 9$, the number of anchor.\nFinally these object proposals are fed to the same top as Fast-RCNN.\nHow to Train In addition of the RPN, I\u0026rsquo;ve really found interesting how the authors used tricks to train their model.\nA big problem of object detection model is that most of the proposal are coming from background (RetinaNet solves this problem elegantly). The authors sample 256 proposals for an image where background and non-background proposals are in equal quantity. The loss function is computed on this sampling.\nThe features of Fast-RCNN and the RPN are shared. To take advantage of this, the authors tried four training strategies:\n Alternate sharing: Similar to some matrix decomposition methods, the authors train RPN, then Fast-RCN, and so on. Each network is trained a bit alternatively.\n Approximate joint training: This strategy consider the two networks as a single unified one. The back-propagation uses both the Fast-RCNN loss and the RPN loss. However the regression of bounding-box coordinates in RPN is considered as pre-computed, and thus its derivative is ignored.\n Non-approximate joint training: This solution was not used as more difficult to implement. The RoI pooling is made differentiable w.r.t the box coordinates using a RoI warping layer.\n 4-Step Alternating training: The strategy chosen takes 4 steps: In the first of one the RPN is trained. In the second, Fast-RCNN is trained using pre-computed RPN proposals. For the third step, the trained Fast-RCNN is used to initialize a new RPN where only RPN\u0026rsquo;s layers are fine-tuned. Finally in the fourth step RPN\u0026rsquo;s layers are frozen and only Fast-RCNN is fine-tuned.\n  Summary These two papers are incremental improvements of RCNN. They introduce RoI pooling and Region Proposal Network.\nRoI pooling concept is also used in other models. FashionNet, a model to predict clothes\u0026rsquo; attributes uses a concept of landmark pooling to force model\u0026rsquo;s attention on a particular cloth\u0026rsquo;s trait.\nRegion Proposal Network is now used in most object detection models, like the Feature Pyramid Network.\n","date":1522015200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1522015200,"objectID":"b5e738e0afbd9d7feca0f89b9bd56e24","permalink":"/post/faster-rcnn/","publishdate":"2018-03-26T00:00:00+02:00","relpermalink":"/post/faster-rcnn/","section":"post","summary":"Fast-RCNN is an Object Detection algorithm using Selective Search and CNN.","tags":[],"title":"Fast and Faster Region-based Convolutional Network","type":"post"},{"authors":[],"categories":null,"content":" This post contains the notes taken from reading of the following paper:\n Selective Search for Object Recognition.  This paper, published in 2012, describes an algorithm generating multiple possible object locations that will later be used by object recognition models. Fast-RCNN uses the Selective Search in its object proposal module.\nMotivations The authors divide the domain of object recognition in three categories:\n Exhaustive Search Segmentation Other sampling strategies (using Bag-of-Words, Hough Transform, etc.)  An Exhaustive Search tries to find bounding boxes for every objects in an image. Searching at every position and scale is unpracticable. Some use instead a windows of different ratio and make them slide around the image. More sophisticated methods also exists (1, 2).\nSegmentation colors each pixel to a given class, creating objects with non-rigid shapes. Segmentation methods usually rely on a single strong algorithm to identify pixels\u0026rsquo; regions.\nSelective Search uses the best of both worlds: Segmentation improve the sampling process of different boxes. This reduces considerably the search space. To improve the algorithm\u0026rsquo;s robustness (to scale, lightning, textures\u0026hellip;) a variety of strategies are used during the bottom-up boxes\u0026rsquo; merging.\nSelective Search produces boxes that are good proposals for objects, it handles well different image conditions, but more important it is fast enough to be used in a prediction pipeline (like Fast-RCNN) to do real-time object detection.\nThe Algorithm At first the authors produce a sampling a bounding boxes based on regions\u0026rsquo; segmentation produced by the Efficient Graph-Based Segmentation.\nStarting from these initial boxes, the authors use a bottom-up merging based on similarity: Boxes, small at first, are merged with their most similar neighbour box. The history of all seen boxes at the different steps of the algorithm is kept.\nBy keeping all existing boxes, the search can capture all scales which is important in hierarchical image: Imagine a pilot in a plane: the pilot\u0026rsquo;s box in comprised in the bigger plane\u0026rsquo;s box.\nPlenty of boxes are created, the last box is the entire image! However some may be more probable object location. The authors sort the boxes by creation time, the most recent first. To avoid privileging too much large boxes, the box\u0026rsquo;s index (1 being the most recent box) is multiplied by a random number between 0 and 1.\nThe model using the selective search can now make a trade-off between having all location proposals and getting only the k-first most probable.\nDiversification Strategies The authors use three strategies to improve the search\u0026rsquo;s robustness:\n1. Different color spaces In order to handle different lightning, the authors apply their algorithm to the same image transposed in different color spaces.\nThe most known color space is RGB, where a pixel has values of red, blue, and green. Among the other used color spaces there are:\n Grayscale: Where a pixel has for single value its intensity. Lab: With one value for lightness, one for green-red, and one for blue-yellow. HSV: With hue, saturation, and a value etc.  2. Different Starting Regions As said before, the algorithm produces its initial boxes from the regions generated by the efficient graph-based segmentation. This segmentation has a parameter $k$ that affects the size of the regions.\nDifferent starting regions from the segmentation affect deeply the selective search.\n3. Different Similarity Measures The most interesting part of this algorithm is the different metrics used to assess similarity between boxes.\nFour similarity measures are defined: Color, texture, size, fitness. These metrics are based on features computed with the pixels\u0026rsquo; values. It would be slow to re-compute these features each time boxes are merged. The authors designed these features so that they could be merged and propagated to the new box without re-computing everything.\nThese similarities are added together producing a final similarity measure.\n3.1. Color Similarity Each box has a color histogram of 25 bins. The similarity of two boxes is the histogram intersection:\n$$s_{color}(r_i, r_j) = \\sum_{k=1}^n min(c^k_i, c^k_j)$$\nWith $r_x$ being a region, and $c^k$ a bin of the histogram.\nIt is simply the number of common pixel values:\nTo propagate the histogram to the box created by a merge of two smaller boxes, the authors average the two histograms with a size\u0026rsquo;s weight:\n$$C_t = \\frac{size(r_i) * C_i + size(r_j) * C_J}{size(r_i) + size(r_j)}$$\n3.2. Texture Similarity Textures matter a lot, otherwise how to make a difference between a cameleon and the material it sits on?\nThe authors create a texture histogram with SIFT. From this histogram, they use the same formulas for both histogram intersection and hierarchy propagation.\n3.3. Size Similarity The size similarity has been created in order to avoid an imbalance between the boxes\u0026rsquo; size. Where one growing big box would forbid intermediary boxes to form.\n$$s_{size}(r_i, r_j) = 1 - \\frac{size(r_i) + size(r_j)}{size(image)}$$\nThe propagation of this feature is simply the sum of the two sizes.\n3.4. Fitness Feature The initial boxes created from the segmentation may overlap. Two overlapping boxes should be merged early, to do this a fitness feature is used:\n$$s_{\\text{fitness}}(r_i, r_j) = 1 - \\frac{size(BB_{ij} - size(r_i) - size(r_j))}{size(\\text{image})}$$\nThe box $BB_{ij}$ is a bounding box that contains both $r_i$ and $r_j$. The feature $s_{\\text{fitness}}$ is proportional to the fraction covered by $r_i$ and $r_j$ in the bounding box $BB_{ij}$.\nExploiting The Search Creating bounding boxes is interesting. However the end goal here is to use the selective search in a object recognition model.\nThe authors use a SVM with an histogram intersection kernel. SVMs are binary classifiers (but can be expanded to multi-classification with One-Against-Rest, One-Against-All schemes). The positive bounding boxes are ground-truth objects. The negative bounding boxes are boxes generated by the selective search that have an overlap of 20% to 50% with a positive box. This force the SVMs to train on particularly difficult boxes.\nHowever SVMs are slow to train with large amount of data. I will publish another article on Fast-RCNN, a model that use Convolutional Neural Networks on top of the Selective Search to do object recognition.\n","date":1520550000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520550000,"objectID":"e85344f081b4e08a9ff68a886ac972a5","permalink":"/post/selective-search/","publishdate":"2018-03-09T00:00:00+01:00","relpermalink":"/post/selective-search/","section":"post","summary":"The Robust Object Proposal used by Fast-RCNN.","tags":[],"title":"Selective Search for Object Recognition","type":"post"},{"authors":[],"categories":null,"content":" This post contains the notes taken from reading of the following paper:\n Efficient Graph-Based Segmentation by Pedro Felzenszwalb and Daniel Huttenlocher.  I was also helped by the slides of Stanford\u0026rsquo;s CS231b.\nFast-RCNN was the state-of-the-art algorithm for object detection in 2015; its object proposal used Selective Search that itself used Efficient Graph-Based Segmentation.\nThe reason this segmentation was still useful almost 10 years later is because the algorithm is fast, while remaining efficient. Its goal is to segment the objects in an image.\nA Graph-Based Algorithm The algorithm sees an image as a graph, and every pixels as vertices. Making of good segmentation for an image is thus equivalent to finding communities in a graph.\nWhat separates two communities of pixels is a boundary based on where similarity ends and dissimilarity begins. A segmentation too fine would result in communities separated without real boundary between them; in a segmentation too coarse communities should be splitted.\nThe authors of the papers argue that their algorithm always find the right segmentation, neither too fine nor too coarse.\nPredicate Of A Boundary The authors define their algorithm with a predicate $D$ that measures dissimilarity: That predicate takes two components and returns true if a boundary exists between them. A component is a segmentation of one or more vertice.\nWith $C1$ and $C2$ two components:\n$$ D(C1, C2) = \\begin{cases} true \u0026amp; \\text{if } \\text{Dif}(C1, C2) \u0026gt; \\text{MInt}(C1, C2)\\newline false \u0026amp; \\text{otherwise} \\end{cases} $$\nWith:\n$$\\text{Dif}(C1, C2) = \\min_{\\substack{v_i \\in C1, v_j \\in C2 \\newline (v_i, v_j) \\in E_{ij}}} w(v_i, v_j)$$\nThe function $Dif(C1, C2)$ returns the minimum weight $w(.)$ edge that connects a vertice $v_i$ to $v_j$, each of them being in two different components. $E_{ij}$ is the set of edges connecting two vertices between components $C1$ and $C2$. This function $Dif$ measures the difference between two components.\nAnd with:\n$$\\text{MInt}(C1, C2) = min (\\text{Int}(C1) + \\tau(C1), \\text{Int}(C2) + \\tau(C2))$$\n$$\\tau(C) = \\frac{k}{|C|}$$\n$$\\text{Int}(C) = \\max_{\\substack{e \\in \\text{MST}(C, E)}} w(e)$$\nThe function $\\text{Int}(C)$ returns the edge with maximum weight that connects two vertices in the Minimum Spanning Tree (MST) of a same component. Looking only in the MST reduces considerably the number of possible edges to consider: A spanning tree has $n - 1$ edges instead of the $\\frac{n(n - 1)}{2}$ total edges. Moreover, using the minimum spanning tree and not just a common spanning tree allows to have segmentation with high-variability (but still progressive). This function $\\text{Int}$ measures the internal difference of a component. A low $\\text{Int}$ means that the component is homogeneous.\nThe function $\\tau(C)$ is a threshold function, that imposes a stronger evidence of boundary for small components. A large $k$ creates a segmentation with large components. The authors set $k = 300$ for wide images, and $k = 150$ for detailed images.\nFinally $\\text{MInt}(C1, C2)$ is the minimum of internal difference of two components.\nTo summarize the predicate $D$: A large difference between two internally homogeneous components is evidence of a boundary between them. However, if the two components are internally heterogeneous it would be harder to prove a boundary. Therefore details are ignored in high-variability regions but are preserved in low-variability regions:\nNotice how the highly-variable grass is correctly segmented while details like numbers on the back of the first player are preserved.\nDifferent Weight Functions The predicate uses a function $w(v_i, v_j)$ that measures the edge\u0026rsquo;s weight between two vertices $v_i$ and $v_j$.\nThe authors provide two alternatives for this weight function:\nGrid Graph Weight To correctly use this weight function, the authors smooth the image using a Gaussian filter with $\\sigma = 0.8$.\nThe Grid Graph Weight function is:\n$$w(v_j, v_i) = |I(p_i) - I(p_j)|$$\nIt is the intensity\u0026rsquo;s difference of the pixel neighbourhood. Indeed, the authors choose to not only use the pixel intensity, but also its 8 neighbours.\nThe intensity is the pixel-value of the central pixel $p_i$ and its 8 neighbours.\nUsing this weight function, they run the algorithm three times (for red, blue, and green) and choose the intersection of the three segmentations as result.\nNearest Neighbours Graph Weight The second weight function is based on the Approximate Nearest Neighbours Search.\nIt tries to find a good approximation of what could be the closest pixel. The features space is both the spatial coordinates and the pixel\u0026rsquo;s RGB.\nFeatures Space = $(x, y, r, g, b)$.\nThe Actual Algorithm Now that every sub-function of the algorithm has been defined, let\u0026rsquo;s see the actual algorithm:\nFor the Graph $G = (V, E)$ composed of the vertices $V$ and the edges $E$, and a segmentation $S = (C_1, C_2, \u0026hellip;)$:\n Sort E into $\\pi$ = ($o_1$, \u0026hellip;, $o_m$) by increasing edge weight order. Each vertice is alone in its own component. This is the initial segmentation $S^0$. For $q = 1, \u0026hellip;, m$:  Current segmentation is $S^q$ ($v_i$, $v_j$) $= o_q$ If $v_i$ and $v_j$ are not in the same component, and the predicate $D(C_i^{q - 1}, C_j^{q - 1})$ is false then:  Merge $C_i$ and $C_j$ into a single component.   Return $S^m$.  The superscript $q$ in $S^q$ or $C_x^Q$ simply denotes a version of the segmentation or of the component at the instant $q$ of the algorithm.\nBasically what the algorithm is doing is a bottom-up merging of at first individual pixels into larger and larger components. At the end, the segmentation $S^m$ will neither be too fine nor too coarse.\nConclusion As you have seen, the algorithm of this paper is quite simple. What makes it efficient is the chosen metrics and the predicate defined beforehand.\nIf you have read until the bottom of the page, congrats! To thank you, here is some demonstrations by the authors:\n","date":1520377200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520377200,"objectID":"ca514ee2c715c5eca895d998a7466a3a","permalink":"/post/efficient-graph-based-segmentation/","publishdate":"2018-03-07T00:00:00+01:00","relpermalink":"/post/efficient-graph-based-segmentation/","section":"post","summary":"Image segmentation with Graph Theory.","tags":[],"title":"Efficient Graph-Based Segmentation","type":"post"},{"authors":[],"categories":null,"content":" This post contains the notes taken from reading of the following paper:\n A few useful things to know about Machine Learning by Pedro Domingos.  This paper does not introduce any novelties in the field of Machine Learning, nor some kinds of benchmarks, but rather offers a overview of the black art of Machine Learning. Domingos covers a wide area of Machine Learning, but each parts are not explored in depth.\nThe Right Algorithm Domingos splits the problem of choosing the right algorithm in three sub-problems:\n Finding the good representation (hyperplanes, rules, decision trees, etc.) The objective function to optimize (accuracy, likelihood, cross-entropy, etc.) The optimization method (quadratric, beam search, gradient descent, etc.)  The optimal combinations should be taken according to hyperparameters: The accuracy, the training time, the problem type, etc.\nEvaluating The Algorithm Domingos notes that while a high accuracy may seem good, it is not a sufficient indicator. A high score of accuracy on the train data may simply mean that the algorithm has an overfit problem, and thus generalize badly on new unseen data.\nA common pitfall would be to train the algorithm on the train data and tweak the hyperparameters in order to maximize our score on the test data. This may lead to an overfit on also the test data!\nThe generalization problems (how can I estimate my generalization? and how can I improve my generalization) are detailed in the further sections.\nThe Bias-Variance Trade-off When building a model it is interesting to decompose the generalization error into two components: the bias and the variance.\n Bias is a learner\u0026rsquo;s tendency to consistently learn the same wrong thing.\nVariance is the tendency to learn random things irrespective of the real signal.\n This trade-off explains why a powerful learner may not be better than a weak learner. If my powerful learner has a low bias, he is performing well on the train data. However if my powerful learner has also a high variance, it may have learned noise from the train data that would be irrelevant for the test data and behave randomly.\nReducing The Variance There are several ways to reduce the variance:\nTrain, Validation, and Test Before training your model, the data should be split in three parts:\n Train: On which the model will learn. Validation: On which we will optimize model\u0026rsquo;s performance by tweaking the parameters. Test: To test the model, only at the end.  In a certain way, we are overfitting on validation by tweaking the parameters according to the validation\u0026rsquo;s performance. In order to mitigate this we can use the cross-validation:\nCross-Validation We are still training the model on train, and tweaking the parameters in order to optimize validation.\nHowever instead of evaluating a fixed validation set, we are evaluating the average performance of the different folds:\nNote that if there is too many parameters choices, the cross-validation may not be able to avoid overfitting.\nRegularization Another way to way to avoid overfitting is to add regularization. It will force the model to be simpler.\nLet\u0026rsquo;s say the model has a set of weights $W$, an evaluation function $f(X)$ (that depends of the weights), and a loss function $L(X, Y)$.\nWithout regularization the model will try to optimize:\n$$L(X, f(X))$$\nWith a regularization $R(W)$:\n$$L(X, f(X)) + \\lambda R(W)$$\nThe regularization is multiplied by a factor $\\lambda$ that is determined empirically, with cross-validation for example.\nThere are several regularizations possible. The two most common are L1 (also known as LASSO), and L2 (also known as Ridge):\nL1 is the absolute norm:\n$$\\Vert W \\Vert_1 = \\Sigma_{i=1}^n |w_i|$$\nWhile L2 is:\n$$\\Vert W \\Vert_2 = \\Sigma_{i=1}^{n} w_i^2$$\nThe Curse Of Dimensionality In addition of overfitting, a model can also fail to learn high-dimensional data.\nFor example, let\u0026rsquo;s imagine that we want to use a decision tree to learn data which features are binary discrete values. If there are 10 features, it would mean that there is a thousand possible samples. If there are 100 features (which is common), there are a thousand billion of billion of billion possible samples. It is unlearnable, either because the model will never generalize correctly, or the model will take a non-practical amount of time to learn.\nThankfully, the data\u0026rsquo;s features are often not completely independent and many features are just noise. The blessing of non-uniformity as Domingos calls, implies the samples are often spread on a lower-dimensional manifold.\nTo reduce the dimension, i.e. choosing the right features, many algorithms exist: PCA, NMF, LDA, etc.\nThe reduction of dimensionality is an often necessary step before feeding the model with the data.\nFeature Engineering Is The Key Feature Engineering is the action of transforming raw data into something that is more learnable by the model. It is dependant on the data\u0026rsquo;s type, and here lies most of the black art of Machine Learning.\nTwo examples:\nFor text data, several processing are necessary: - tokenization to split the words of the sentence. - lemmatization to get the lemma (loved, loving, lover -\u0026gt; love) - POS-Tagging to get the grammar label of a token (be -\u0026gt; verb, car -\u0026gt; noun)\nFor image data, in the case of object detection we can extract interesting features with the HOG algorithm and feed these features to a SVM to improve significantly the performances.\nWhile feature engineering is major part of Machine Learning, it is less important in Deep Learning: with Convolutional Neural Network (CNN) the model is learning by itself the convolution kernels extracting the interesting features.\nModel Ensembles In order to achieve the best performance we want to decrease both bias and variance. It is often complicated to optimize this trade-off. A great way to achieve this is to combine different models, kind of like a wisdom of the crowd.\nThere are three main categories of ensembles:\nBagging Used in the Random Forest, bagging generates plenty of model. Each has a low bias but a high variance. A voting system is set up between them to choose the output, thus lowering the individual variances.\nBoosting Used in Adaboost or in Gradient Boosting, boosting generates at first a simple weak learner: It should just be a bit better than a random guess. At each iteration of the training, a new weak learner is added to the global learner. The new weak learner focuses on the previously poorly predicted data.\nAt each iteration the bias is reduced as the overall model improves. There is a diminished risk of overfitting with boosting: Because each iteration\u0026rsquo;s learner focuses on poorly predicted data, the risk of over-learning data is small.\nStacking The stacking ensemble is the easiest to understand: Each model is connected to another: The output of one is the input of another.\nData, Data, And Data While Domingos offers us great insights into Machine Learning, and various methods to improve our models, he notes one constant:\n More data beats a cleverer algorithm\n It is often more advisable to focus the efforts on getting as much data as possible, and begin with a simple model, than to expect a complex model to generalize from few data.\nAvailable Data There are plenty of resources available:\n UCL Datasets Kaggle Datasets CIFAR, ImageNet, COCO \u0026hellip;  ","date":1517871600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517871600,"objectID":"9903344a98818c99659063832fe00e48","permalink":"/post/useful-things-to-know-about-ml/","publishdate":"2018-02-06T00:00:00+01:00","relpermalink":"/post/useful-things-to-know-about-ml/","section":"post","summary":"This post contains the notes taken from reading of the following paper:\n A few useful things to know about Machine Learning by Pedro Domingos.  This paper does not introduce any novelties in the field of Machine Learning, nor some kinds of benchmarks, but rather offers a overview of the black art of Machine Learning. Domingos covers a wide area of Machine Learning, but each parts are not explored in depth.","tags":[],"title":"A Few Useful Things To Know About Machine Learning","type":"post"},{"authors":[],"categories":null,"content":" Preambule During my master in Data Science I have read a few papers. While I am a good reader, reading a scientific paper is still a strugle. For the year 2018, and hopefuly the next years, I have decided to read more papers. At least one a week.\nMy favorite method to learn something is to explain it to someone else. That\u0026rsquo;s the Feyman\u0026rsquo;s technique. It may be hard to find a patient listener thus I am making this blog to explain to the potential reader papers I am reading.\nA great and similar example is the blog The Morning Paper that I vivedly recommend.\nHow To Read A Paper The first paper of this blog serie is about the techniques to read a paper.\n How To Read A Paper by S.Keshav.  The author advises a three-pass approach:\nThe First Pass The first pass is about determining what the paper is talking about and whether it is worth it to read it with more attention. A few minutes should suffice.\nThe reader should read the abstract \u0026amp; introduction, sub-introduction of a new section, and the conclusion. A quick pass on the references may also be useful.\nIn order to choose whether to read more of the paper, the reader should check the five Cs:\n To which category the paper belongs? What is the context surrounding the paper? Does the paper seem to be correct? What are the contributions of the paper? Is the paper well written?  The Second Pass The second pass may be enough for most papers.\nThe reader should at first, pay special attention to the figures and illustrations. Then he should read the main gist of the paper while avoiding details such as proofs. And finaly he should jot down the main references.\nThe Third Pass The third and last pass is about re-doing the worker of the researcher: The reader has to examine carefully each proofs, assumptions, and affirmations.\nAt this pass, the reader should also list the strong and the weak points of the paper with greater attention.\n","date":1517871600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517871600,"objectID":"10a02c645b0a2274e460a1e950472e14","permalink":"/post/how-to-read-a-paper/","publishdate":"2018-02-06T00:00:00+01:00","relpermalink":"/post/how-to-read-a-paper/","section":"post","summary":"Summary of a paper about... how to read a paper.","tags":[],"title":"How To Read A Paper","type":"post"}]