---
layout: post
title:  Fast and Faster Region-based Convolutional Network
tags:   [papers, cv, deep_learning, ml]
description: Fast-RCNN is an Object Detection algorithm using Selective Search and CNN
img: /public/assets/fast-rcnn.png
---

This post contains the notes taken from the following paper:

- [Fast-RCNN](https://arxiv.org/abs/1504.08083) by [R. Girshick](https://scholar.google.com/citations?user=W8VIEZgAAAAJ&hl=en).

- [Faster-RCNN](https://arxiv.org/abs/1506.01497) by Microsoft Research.

Ross Girshick is a very influential researcher on object detection:
he has worked on RCNN, Fast{er}-RCNN, Yolo, RetinaNet...

Fast-RCNN and Faster-RCNN are both incremental improvements on the original
[RCNN](https://arxiv.org/abs/1311.2524).

Let's see what were those improvements:

# Fast-RCNN

In Fast-RCNN, Girshick ditched the SVM used [previously](https://arthurdouillard.com/2018/03/09/selective-search/#exploiting-the-search).
It resulted in a 10x inference speed improvement, and a better accuracy.

Girshick replaced the SVM by a **Region of Interest (RoI) pooling**. RoIs are still
produced by the [selective search](https://arthurdouillard.com/2018/03/09/selective-search),
and they are used to select a subset of the feature map produced from the whole image:

![Feature map](/public/assets/feature_map.svg)
*At the end of the CNN, [without top](https://github.com/keras-team/keras/blob/master/keras/applications/vgg16.py#L141),
a feature map is generated by filter.*

Let's consider as example an input image of size 10x10; At the end of the CNN,
the feature map has a size of 5x5. If the selective search proposes a box
between (top-left and bottom-right)  (0, 2) and (6, 8) then we extract a similar box
from the feature map. However this box is proportionaly scaled down:

![RoI Pooling](/public/assets/roi_pooling.svg)

Objects have very different sizes, and so are the boxes extracted from the feature
maps. To normalize their size a max pooling is done. Note that it does not really
matter if the height or width of the extracted box is not even:

![RoI pooling](/public/assets/roi_pooling2.svg)

Those extracted fixed-size feature maps (one per filter per object) are then fed
to several fully connected layers. At some point, the network split into two
sub-networks. One is designed to classify the class with a softmax activation.
The other is a regressor with 4 values: The coordinates of the top-left point
of the box and its width & height.

![Fast-rccn](/public/assets/fast-rcnn2.png)

Note that if you want to train your RCNN to detect $$K$$ classes, the sub-network
detecting the box's class will choose between $$K + 1$$ classes. The extra class
is the ubiquitous background. The bounding-box regressor's loss won't be taken
in account if a background is detected.

# Faster-RCNN

The main contribution of Fast-RCNN was the RoI pooling followed by a two-headed
fully connected network. Faster-RCNN eliminated another speed bottleneck: The
generation of the region proposals by selective search:

> Fast R-CNN, achieves near real-time rates using very deep networks,
> when ignoring the time spent on region proposals. Now, proposals are the
> test-time computational bottleneck in state-of-the-art detection systems.

The authors introduced the **Region Proposal Network** (RPN) to fix this problem.

## Region Proposal Network

![Faster-rccn](/public/assets/faster-rcnn.png)
*RPN generates region proposals that are given to the classifier which is
Fast-RCNN.*


First of all the feature maps are reduced to intermediate layers of smaller size.
The authors used a layer of dimension 512 when the feature maps were originating
from VGG16.

Then the RPN uses a sliding window, moving all around the intermediate layers.
At each location, several **anchors** are used. An anchor is simply a box of a
pre-defined size and shape. 9 different anchors exist: there are 3 different
scales and 3 different ratios.

![Anchors](/public/assets/anchors.svg)
*The first two scales, and the three possible ratios.*

For each possible anchor a mini-network is used for two tasks:

- classify whether the location is a background or an actual object.
- Predict the exact bounding-box coordinates and width.

![RPN](/public/assets/rpn.png)
*With $k = 9$, the number of anchor.$

Finally these object proposals are fed to the same top as Fast-RCNN.

## How to Train

In addition of the RPN, I've really found interesting how the authors used tricks
to train their model.

A big problem of object detection model is that most of the proposal are coming
from background (*[RetinaNet](https://arxiv.org/abs/1708.02002) solves this problem
elegantly*). The authors sample 256 proposals for an image where background and
non-background proposals are in equal quantity. The loss function is computed on
this sampling.

The features of Fast-RCNN and the RPN are shared. To take advantage of this,
the authors tried four training strategies:

- **Alternate sharing**: Similar to some matrix decomposition methods, the authors
train RPN, then Fast-RCN, and so on. Each network is trained a bit alternatively.


- **Approximate joint training**: This strategy consider the two networks as a
single unified one. The backpropagation uses both the Fast-RCNN loss and the RPN
loss. However the regression of bounding-box coordinates in RPN is considered
as pre-computed, and thus its derivative is ignored.

- **Non-approximate joint training**: This solution was not used as more difficult
to implement. The RoI pooling is made differentiable w.r.t the box coordinates using
a [*RoI warping*](https://arxiv.org/abs/1512.04412) layer.

- **4-Step Alternating training**: The strategy choosen takes 4 steps: In the first
of one the RPN is trained. In the second, Fast-RCNN is trained using pre-computed
RPN proposals. For the third step, the trained Fast-RCNN is used to initialize a
new RPN where only RPN's layers are fine-tuned. Finally in the fourth step RPN's
layers are frozen and only Fast-RCNN is fine-tuned.

# Summary

These two papers are incremental improvements of
[RCNN](https://arxiv.org/abs/1311.2524). They introduce RoI pooling and Region
Proposal Network.

RoI pooling concept is now used in many models. [FashioNet](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Liu_DeepFashion_Powering_Robust_CVPR_2016_paper.pdf), a model to predict clothes' attributes uses a concept of *landmark pooling*
to force model's *attention* on a particular clothe's trait.

Region Proposal Network is now used in most object detection models, like
the [Feature Pyramid Network](https://arxiv.org/abs/1612.03144).
